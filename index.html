<!DOCTYPE html>
<html lang="es">
<head>
<!-- metadatos SEO b√°sicos -->
<meta name="description" content="Gu√≠a completa de Numba en espa√±ol: desde @jit b√°sico hasta @overload avanzado. Optimizaci√≥n Python para computaci√≥n num√©rica.">
<meta name="keywords" content="numba, python, optimizaci√≥n, jit, vectorize, paralelismo, numpy">
<meta name="author" content="Tu Nombre">
<meta property="og:title" content="Gu√≠a Completa de Numba">
<meta property="og:description" content="De principiante a experto en optimizaci√≥n num√©rica con Python">
<!-- ;) -->
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Gu√≠a Completa de Numba ‚Äî De Principiante a Avanzado</title>
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --border: #30363d;
    --text: #e6edf3;
    --muted: #8b949e;
    --accent: #58a6ff;
    --accent2: #3fb950;
    --accent3: #d2a8ff;
    --accent4: #f0883e;
    --danger: #f85149;
    --code-bg: #0d1117;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
    background: var(--bg); color: var(--text);
    line-height: 1.7; font-size: 16px;
  }
  .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }

  /* Header */
  .hero {
    text-align: center; padding: 3rem 0 2rem;
    border-bottom: 1px solid var(--border); margin-bottom: 2rem;
  }
  .hero h1 { font-size: 2.4rem; font-weight: 800; margin-bottom: 0.5rem;
    background: linear-gradient(135deg, var(--accent), var(--accent3));
    -webkit-background-clip: text; -webkit-text-fill-color: transparent;
  }
  .hero .subtitle { color: var(--muted); font-size: 1.1rem; }
  .badge { display: inline-block; padding: 3px 10px; border-radius: 20px;
    font-size: 0.75rem; font-weight: 600; margin: 0.5rem 0.25rem;
  }
  .badge.green { background: rgba(63,185,80,0.15); color: var(--accent2); border: 1px solid rgba(63,185,80,0.3); }
  .badge.blue { background: rgba(88,166,255,0.15); color: var(--accent); border: 1px solid rgba(88,166,255,0.3); }
  .badge.purple { background: rgba(210,168,255,0.15); color: var(--accent3); border: 1px solid rgba(210,168,255,0.3); }
  .badge.orange { background: rgba(240,136,62,0.15); color: var(--accent4); border: 1px solid rgba(240,136,62,0.3); }

  /* Navigation */
  .toc { background: var(--surface); border: 1px solid var(--border);
    border-radius: 12px; padding: 1.5rem; margin-bottom: 2.5rem;
  }
  .toc h2 { font-size: 1.1rem; color: var(--accent); margin-bottom: 1rem; }
  .toc-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 0.3rem 2rem; }
  .toc a { color: var(--muted); text-decoration: none; font-size: 0.9rem; padding: 2px 0; display: block; }
  .toc a:hover { color: var(--accent); }
  .toc .module-label { font-size: 0.7rem; color: var(--accent3); text-transform: uppercase;
    letter-spacing: 1px; margin-top: 0.7rem; margin-bottom: 0.2rem; }

  /* Sections */
  .module {
    border: 1px solid var(--border); border-radius: 12px;
    margin-bottom: 2rem; overflow: hidden;
  }
  .module-header {
    background: var(--surface); padding: 1.2rem 1.5rem;
    border-bottom: 1px solid var(--border); cursor: pointer;
    display: flex; align-items: center; justify-content: space-between;
  }
  .module-header:hover { background: #1c2129; }
  .module-header h2 { font-size: 1.3rem; }
  .module-header .num { color: var(--accent); margin-right: 0.75rem; font-weight: 800; }
  .module-body { padding: 1.5rem; display: none; }
  .module.open .module-body { display: block; }
  .module-header .arrow { transition: transform 0.2s; color: var(--muted); font-size: 1.2rem; }
  .module.open .module-header .arrow { transform: rotate(90deg); }

  h3 { color: var(--accent2); font-size: 1.1rem; margin: 1.5rem 0 0.75rem;
    padding-bottom: 0.3rem; border-bottom: 1px solid var(--border);
  }
  h4 { color: var(--accent3); font-size: 0.95rem; margin: 1.2rem 0 0.5rem; }
  p { margin-bottom: 0.75rem; }

  /* Code blocks */
  pre {
    background: var(--code-bg); border: 1px solid var(--border);
    border-radius: 8px; padding: 1rem 1.2rem; overflow-x: auto;
    margin: 0.75rem 0 1rem; font-size: 0.88rem; line-height: 1.6;
  }
  code { font-family: 'SF Mono', 'Fira Code', 'Cascadia Code', Consolas, monospace; }
  p code, li code {
    background: rgba(88,166,255,0.1); padding: 2px 6px; border-radius: 4px;
    font-size: 0.88em; color: var(--accent);
  }

  /* Syntax highlighting */
  .kw { color: #ff7b72; }
  .fn { color: #d2a8ff; }
  .st { color: #a5d6ff; }
  .cm { color: #8b949e; font-style: italic; }
  .num { color: #79c0ff; }
  .dec { color: #f0883e; }
  .op { color: #ff7b72; }
  .typ { color: #ffa657; }

  /* Callouts */
  .tip, .warn, .info, .perf {
    border-radius: 8px; padding: 1rem 1.2rem; margin: 1rem 0;
    border-left: 4px solid;
  }
  .tip { background: rgba(63,185,80,0.08); border-color: var(--accent2); }
  .warn { background: rgba(248,81,73,0.08); border-color: var(--danger); }
  .info { background: rgba(88,166,255,0.08); border-color: var(--accent); }
  .perf { background: rgba(240,136,62,0.08); border-color: var(--accent4); }
  .tip::before { content: "üí° Tip"; font-weight: 700; color: var(--accent2); display: block; margin-bottom: 0.3rem; }
  .warn::before { content: "‚ö†Ô∏è Cuidado"; font-weight: 700; color: var(--danger); display: block; margin-bottom: 0.3rem; }
  .info::before { content: "‚ÑπÔ∏è Info"; font-weight: 700; color: var(--accent); display: block; margin-bottom: 0.3rem; }
  .perf::before { content: "‚ö° Rendimiento"; font-weight: 700; color: var(--accent4); display: block; margin-bottom: 0.3rem; }

  ul, ol { margin: 0.5rem 0 1rem 1.5rem; }
  li { margin-bottom: 0.3rem; }

  /* Comparison tables */
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
  th { background: var(--surface); color: var(--accent); text-align: left; padding: 0.6rem 0.8rem;
    border: 1px solid var(--border); font-weight: 600; }
  td { padding: 0.5rem 0.8rem; border: 1px solid var(--border); }
  tr:nth-child(even) td { background: rgba(255,255,255,0.02); }

  .progress-map {
    display: flex; gap: 0.5rem; flex-wrap: wrap; margin: 1rem 0;
  }
  .progress-step {
    flex: 1; min-width: 120px; text-align: center; padding: 0.8rem 0.5rem;
    border-radius: 8px; border: 1px solid var(--border); background: var(--surface);
    font-size: 0.8rem;
  }
  .progress-step .step-num { font-size: 1.5rem; font-weight: 800; }
  .progress-step.s1 .step-num { color: var(--accent2); }
  .progress-step.s2 .step-num { color: var(--accent); }
  .progress-step.s3 .step-num { color: var(--accent3); }
  .progress-step.s4 .step-num { color: var(--accent4); }
  .progress-step.s5 .step-num { color: var(--danger); }

  @media(max-width:600px) {
    .toc-grid { grid-template-columns: 1fr; }
    .hero h1 { font-size: 1.8rem; }
    .progress-map { flex-direction: column; }
  }
</style>
</head>
<body>
<div class="container">

<div class="hero">
  <h1>Gu√≠a Completa de Numba</h1>
  <p class="subtitle">De principiante a profesional ‚Äî con ejemplos pr√°cticos</p>
  <div>
    <span class="badge green">CPU Optimization</span>
    <span class="badge blue">Paralelismo</span>
    <span class="badge purple">NumPy Integration</span>
    <span class="badge orange">Extensiones</span>
  </div>
</div>

<!-- Mapa de progreso -->
<div class="progress-map">
  <div class="progress-step s1"><div class="step-num">1</div>Fundamentos<br>@jit b√°sico</div>
  <div class="progress-step s2"><div class="step-num">2</div>Ufuncs<br>vectorize</div>
  <div class="progress-step s3"><div class="step-num">3</div>Paralelismo<br>prange</div>
  <div class="progress-step s4"><div class="step-num">4</div>Estructuras<br>jitclass</div>
  <div class="progress-step s5"><div class="step-num">5</div>Extensiones<br>overload</div>
</div>

<!-- TOC -->
<div class="toc">
  <h2>üìö Tabla de Contenidos</h2>
  <div class="toc-grid">
    <div>
      <div class="module-label">Fundamentos</div>
      <a href="#m1">1. ¬øQu√© es Numba y c√≥mo funciona?</a>
      <a href="#m2">2. @jit ‚Äî Tu primer decorador</a>
      <a href="#m3">3. Tipos y Signatures</a>
      <div class="module-label">Intermedio</div>
      <a href="#m4">4. @vectorize y @guvectorize</a>
      <a href="#m5">5. Paralelismo con parallel y prange</a>
    </div>
    <div>
      <div class="module-label">Avanzado</div>
      <a href="#m6">6. @jitclass ‚Äî Clases compiladas</a>
      <a href="#m7">7. @stencil ‚Äî Patrones de c√°lculo</a>
      <a href="#m8">8. Performance Tips profesionales</a>
      <div class="module-label">Experto</div>
      <a href="#m9">9. Extendiendo Numba (@overload, @intrinsic)</a>
      <a href="#m10">10. Cheatsheet y Patrones Comunes</a>
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 1 ==================== -->
<div class="module open" id="m1">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">01</span> ¬øQu√© es Numba y c√≥mo funciona?</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>Numba es un compilador <strong>JIT (Just-In-Time)</strong> para Python que traduce funciones Python a c√≥digo m√°quina nativo usando LLVM. Funciona mejor con c√≥digo num√©rico que usa arrays NumPy y loops.</p>

    <h3>¬øC√≥mo funciona internamente?</h3>
    <p>Cuando decoras una funci√≥n con <code>@jit</code>, Numba hace lo siguiente:</p>
    <ol>
      <li><strong>Analiza el bytecode</strong> de Python de tu funci√≥n</li>
      <li><strong>Infiere los tipos</strong> de todas las variables (en la primera llamada)</li>
      <li><strong>Genera LLVM IR</strong> (representaci√≥n intermedia)</li>
      <li><strong>Compila a c√≥digo m√°quina</strong> nativo optimizado</li>
      <li><strong>Cachea la funci√≥n compilada</strong> para llamadas futuras con los mismos tipos</li>
    </ol>

    <h3>Instalaci√≥n</h3>
<pre><code><span class="cm"># Con conda (recomendado)</span>
$ conda install numba

<span class="cm"># Con pip</span>
$ pip install numba

<span class="cm"># Extras para m√°ximo rendimiento</span>
$ conda install intel-cmplr-lib-rt  <span class="cm"># Intel SVML para funciones matem√°ticas r√°pidas</span>
$ pip install scipy                  <span class="cm"># Para numpy.linalg optimizado</span></code></pre>

    <h3>El ejemplo m√°s simple</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> jit
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="dec">@jit</span>  <span class="cm"># Numba compila esto a c√≥digo m√°quina</span>
<span class="kw">def</span> <span class="fn">suma_cuadrados</span>(arr):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(arr)):
        total += arr[i] ** <span class="num">2</span>
    <span class="kw">return</span> total

<span class="cm"># Primera llamada: compila + ejecuta (m√°s lenta)</span>
arr = np.arange(<span class="num">1_000_000</span>, dtype=np.float64)
resultado = suma_cuadrados(arr)  <span class="cm"># ~compilaci√≥n: 0.5s</span>

<span class="cm"># Segunda llamada: solo ejecuta (rapid√≠sima)</span>
resultado = suma_cuadrados(arr)  <span class="cm"># ~ejecuci√≥n: 0.002s vs ~0.5s en Python puro</span></code></pre>

    <div class="info">
      Numba genera <strong>especializaciones diferentes</strong> seg√∫n los tipos de entrada. Si llamas a la misma funci√≥n con <code>int64</code> y luego con <code>float64</code>, se compilan dos versiones distintas.
    </div>

    <h3>Modos de compilaci√≥n</h3>
    <table>
      <tr><th>Modo</th><th>Descripci√≥n</th><th>Velocidad</th></tr>
      <tr><td><strong>nopython</strong> (default)</td><td>C√≥digo 100% nativo, sin usar el int√©rprete Python</td><td>‚ö°‚ö°‚ö° M√°xima</td></tr>
      <tr><td><strong>object mode</strong></td><td>Fallback que usa objetos Python (casi no acelera)</td><td>‚ö° M√≠nima</td></tr>
    </table>

    <div class="warn">
      Desde Numba 0.59, <code>@jit</code> es equivalente a <code>@jit(nopython=True)</code> y a <code>@njit</code>. El modo object ya no es el fallback por defecto. Si tu c√≥digo no es compatible con nopython mode, Numba lanzar√° un error en lugar de compilar silenciosamente en object mode.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 2 ==================== -->
<div class="module" id="m2">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">02</span> @jit ‚Äî Tu primer decorador</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Compilaci√≥n Lazy vs Eager</h3>
    <h4>Lazy (recomendado para empezar)</h4>
    <p>Numba infiere los tipos autom√°ticamente en la primera llamada:</p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> jit

<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">f</span>(x, y):
    <span class="kw">return</span> x + y

f(<span class="num">1</span>, <span class="num">2</span>)       <span class="cm"># Compila versi√≥n int64</span>
f(<span class="num">1j</span>, <span class="num">2</span>)      <span class="cm"># Compila OTRA versi√≥n para complex128</span>
f(<span class="num">1.0</span>, <span class="num">2.0</span>)  <span class="cm"># Compila OTRA versi√≥n para float64</span></code></pre>

    <h4>Eager (control total de tipos)</h4>
    <p>Defines exactamente qu√© tipos acepta. M√°s r√°pido en la primera llamada (no hay inferencia):</p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> jit, int32, float64

<span class="dec">@jit</span>(int32(int32, int32))  <span class="cm"># retorno(arg1, arg2)</span>
<span class="kw">def</span> <span class="fn">add_int</span>(x, y):
    <span class="kw">return</span> x + y

<span class="cm"># M√∫ltiples signatures</span>
<span class="dec">@jit</span>([int32(int32, int32),
     float64(float64, float64)])
<span class="kw">def</span> <span class="fn">add_multi</span>(x, y):
    <span class="kw">return</span> x + y</code></pre>

    <h3>Opciones clave del decorador</h3>

    <h4><code>cache=True</code> ‚Äî Evita recompilar</h4>
<pre><code><span class="dec">@jit</span>(cache=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">heavy_computation</span>(x):
    <span class="cm"># Se guarda en disco tras la primera compilaci√≥n</span>
    <span class="cm"># Las siguientes ejecuciones del script no recompilan</span>
    <span class="kw">return</span> np.sum(np.sqrt(x))</code></pre>

    <div class="warn">
      Limitaciones del cache: no detecta cambios en funciones importadas de otros m√≥dulos, y las variables globales se tratan como constantes (el cache recuerda el valor al momento de compilar).
    </div>

    <h4><code>nogil=True</code> ‚Äî Libera el GIL</h4>
<pre><code><span class="dec">@jit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_chunk</span>(data):
    <span class="cm"># Se puede ejecutar concurrentemente con otros threads</span>
    <span class="cm"># ¬°Ideal para multithreading real en Python!</span>
    result = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        result += data[i] ** <span class="num">2</span>
    <span class="kw">return</span> result</code></pre>

    <h4><code>fastmath=True</code> ‚Äî Matem√°ticas m√°s r√°pidas</h4>
<pre><code><span class="cm"># Relaja IEEE 754 para mayor velocidad</span>
<span class="dec">@jit</span>(fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">do_sum_fast</span>(A):
    acc = <span class="num">0.</span>
    <span class="kw">for</span> x <span class="kw">in</span> A:
        acc += np.sqrt(x)
    <span class="kw">return</span> acc  <span class="cm"># ~2x m√°s r√°pido que sin fastmath</span>

<span class="cm"># Control granular con flags LLVM espec√≠ficas</span>
<span class="dec">@jit</span>(fastmath={<span class="st">'reassoc'</span>, <span class="st">'nsz'</span>})  <span class="cm"># solo reasociaci√≥n y no-signed-zero</span>
<span class="kw">def</span> <span class="fn">add_assoc</span>(x, y):
    <span class="kw">return</span> (x - y) + y</code></pre>

    <h3>Llamando otras funciones compiladas</h3>
<pre><code><span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">square</span>(x):
    <span class="kw">return</span> x ** <span class="num">2</span>

<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">hypot</span>(x, y):
    <span class="kw">return</span> math.sqrt(square(x) + square(y))  <span class="cm"># LLVM puede inlinear square()</span></code></pre>

    <div class="tip">
      <strong>Siempre</strong> decora con <code>@jit</code> las funciones auxiliares que llamas desde c√≥digo Numba. Si no lo haces, Numba genera c√≥digo mucho m√°s lento al tener que "salir" al int√©rprete Python.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 3 ==================== -->
<div class="module" id="m3">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">03</span> Tipos y Signatures</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <h3>Tipos escalares</h3>
    <table>
      <tr><th>Tipo Numba</th><th>Equivalente</th><th>Uso t√≠pico</th></tr>
      <tr><td><code>int8, int16, int32, int64</code></td><td>Enteros con signo</td><td>√çndices, contadores</td></tr>
      <tr><td><code>uint8, uint16, uint32, uint64</code></td><td>Enteros sin signo</td><td>Datos binarios</td></tr>
      <tr><td><code>float32, float64</code></td><td>Punto flotante</td><td>C√°lculos num√©ricos</td></tr>
      <tr><td><code>complex64, complex128</code></td><td>Complejos</td><td>Se√±ales, FFT</td></tr>
      <tr><td><code>boolean</code></td><td>Booleano</td><td>Flags</td></tr>
      <tr><td><code>intp, uintp</code></td><td>Tama√±o de puntero</td><td>√çndices de arrays</td></tr>
      <tr><td><code>void</code></td><td>None</td><td>Funciones sin retorno</td></tr>
    </table>

    <h3>Tipos de arrays</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> float64, int32

<span class="cm"># Indexas el tipo escalar para crear tipo de array</span>
float64[:]       <span class="cm"># Array 1D de float64</span>
int32[:, :]      <span class="cm"># Array 2D de int32</span>
float64[:, :, :] <span class="cm"># Array 3D de float64</span>

<span class="cm"># En signatures completas</span>
<span class="dec">@jit</span>(float64[:](float64[:], float64[:]))
<span class="kw">def</span> <span class="fn">add_arrays</span>(a, b):
    <span class="kw">return</span> a + b

<span class="cm"># Arrays C-contiguous vs Fortran-contiguous</span>
<span class="kw">from</span> numba <span class="kw">import</span> types
types.Array(types.float64, <span class="num">2</span>, <span class="st">'C'</span>)  <span class="cm"># 2D, C-order (row-major)</span>
types.Array(types.float64, <span class="num">2</span>, <span class="st">'F'</span>)  <span class="cm"># 2D, Fortran-order (column-major)</span>
types.Array(types.float64, <span class="num">2</span>, <span class="st">'A'</span>)  <span class="cm"># 2D, cualquier layout</span></code></pre>

    <h3>Contenedores tipados</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.typed <span class="kw">import</span> Dict, List

<span class="cm"># Listas tipadas (se pueden usar en @jit)</span>
<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">usar_lista</span>():
    lst = List()
    lst.append(<span class="num">1</span>)
    lst.append(<span class="num">2</span>)
    lst.append(<span class="num">3</span>)
    <span class="kw">return</span> lst

<span class="cm"># Diccionarios tipados</span>
<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">usar_dict</span>():
    d = Dict()
    d[<span class="num">1</span>] = <span class="num">10.0</span>
    d[<span class="num">2</span>] = <span class="num">20.0</span>
    <span class="kw">return</span> d

<span class="cm"># Tipos expl√≠citos</span>
types.DictType(types.int64, types.float64)
types.ListType(types.float64)</code></pre>

    <div class="info">
      Las listas y diccionarios de Python est√°ndar <strong>no funcionan</strong> en nopython mode. Debes usar <code>numba.typed.List</code> y <code>numba.typed.Dict</code>.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 4 ==================== -->
<div class="module" id="m4">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">04</span> @vectorize y @guvectorize ‚Äî Ufuncs a medida</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>@vectorize ‚Äî Ufuncs elemento a elemento</h3>
    <p>Escribes la operaci√≥n escalar y Numba genera el loop autom√°ticamente, con todas las features de un ufunc NumPy (broadcasting, reduce, accumulate).</p>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> vectorize, float64, int64

<span class="cm"># Eager: defines los tipos expl√≠citamente</span>
<span class="dec">@vectorize</span>([float64(float64, float64)])
<span class="kw">def</span> <span class="fn">clip_value</span>(x, threshold):
    <span class="kw">if</span> x > threshold:
        <span class="kw">return</span> threshold
    <span class="kw">elif</span> x < -threshold:
        <span class="kw">return</span> -threshold
    <span class="kw">return</span> x

<span class="cm"># Ahora funciona como cualquier ufunc de NumPy</span>
arr = np.random.randn(<span class="num">1000</span>)
result = clip_value(arr, <span class="num">1.5</span>)  <span class="cm"># Broadcasting autom√°tico</span>

<span class="cm"># ¬°Tambi√©n reduce y accumulate!</span>
<span class="dec">@vectorize</span>([float64(float64, float64)])
<span class="kw">def</span> <span class="fn">add</span>(x, y):
    <span class="kw">return</span> x + y

a = np.arange(<span class="num">12</span>).reshape(<span class="num">3</span>, <span class="num">4</span>)
add.reduce(a, axis=<span class="num">0</span>)       <span class="cm"># Suma por columnas</span>
add.accumulate(a, axis=<span class="num">1</span>)   <span class="cm"># Suma acumulada por filas</span></code></pre>

    <h3>Targets: CPU, Parallel, CUDA</h3>
    <table>
      <tr><th>Target</th><th>Cu√°ndo usar</th><th>Overhead</th></tr>
      <tr><td><code>target='cpu'</code></td><td>Datos peque√±os (&lt; 1KB), baja intensidad</td><td>M√≠nimo</td></tr>
      <tr><td><code>target='parallel'</code></td><td>Datos medianos (&lt; 1MB)</td><td>Bajo (threading)</td></tr>
      <tr><td><code>target='cuda'</code></td><td>Datos grandes (&gt; 1MB), alta intensidad</td><td>Alto (transferencia GPU)</td></tr>
    </table>

<pre><code><span class="cm"># Paralelizado autom√°ticamente en todos los cores</span>
<span class="dec">@vectorize</span>([float64(float64, float64)], target=<span class="st">'parallel'</span>)
<span class="kw">def</span> <span class="fn">add_parallel</span>(x, y):
    <span class="kw">return</span> x + y</code></pre>

    <h3>@guvectorize ‚Äî Operaciones sobre sub-arrays</h3>
    <p>Cuando necesitas operar sobre <strong>porciones de arrays</strong>, no solo escalares. Defines un layout simb√≥lico que indica las dimensiones de entrada/salida.</p>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> guvectorize, float64

<span class="cm"># Media m√≥vil: recibe array 1D, devuelve escalar por cada "fila"</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'(n)->()'</span>)
<span class="kw">def</span> <span class="fn">moving_mean</span>(arr, result):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(arr.shape[<span class="num">0</span>]):
        total += arr[i]
    result[<span class="num">0</span>] = total / arr.shape[<span class="num">0</span>]

<span class="cm"># Aplicar sobre una matriz (cada fila es un vector)</span>
data = np.random.rand(<span class="num">100</span>, <span class="num">10</span>)
means = moving_mean(data)  <span class="cm"># ‚Üí array de 100 medias (broadcasting autom√°tico)</span>

<span class="cm"># Ejemplo m√°s completo: suma con peso</span>
<span class="dec">@guvectorize</span>(
    [(float64[:], float64[:], float64[:])],
    <span class="st">'(n),(n)->(n)'</span>  <span class="cm"># dos arrays entrada ‚Üí un array salida, misma forma</span>
)
<span class="kw">def</span> <span class="fn">weighted_add</span>(a, weights, result):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(a.shape[<span class="num">0</span>]):
        result[i] = a[i] * weights[i]</code></pre>

    <div class="tip">
      En <code>@guvectorize</code> los resultados se escriben en el argumento de salida (<code>result</code>), no se retornan. NumPy aloca el array de salida autom√°ticamente.
    </div>

    <h4>Notaci√≥n de layouts</h4>
    <table>
      <tr><th>Layout</th><th>Significado</th></tr>
      <tr><td><code>(n),()->(n)</code></td><td>Array + escalar ‚Üí array</td></tr>
      <tr><td><code>(n)->()</code></td><td>Array ‚Üí escalar (reducci√≥n)</td></tr>
      <tr><td><code>(m,n),(n)->(m)</code></td><td>Matriz √ó vector ‚Üí vector</td></tr>
      <tr><td><code>(n),(n)->(n)</code></td><td>Dos arrays ‚Üí array elemento a elemento</td></tr>
    </table>
  </div>
</div>

<!-- ==================== M√ìDULO 5 ==================== -->
<div class="module" id="m5">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">05</span> Paralelismo con parallel y prange</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Auto-paralelizaci√≥n de operaciones</h3>
    <p>Con <code>parallel=True</code>, Numba identifica operaciones con sem√°ntica paralela y las fusiona en kernels que se ejecutan en m√∫ltiples threads.</p>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> njit
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">ident_parallel</span>(x):
    <span class="kw">return</span> np.cos(x) ** <span class="num">2</span> + np.sin(x) ** <span class="num">2</span>

<span class="cm"># ~5x m√°s r√°pido que NumPy puro</span>
<span class="cm"># ~6x m√°s r√°pido que @njit sin parallel</span></code></pre>

    <h3>Operaciones auto-paralelizables</h3>
    <ul>
      <li>Operaciones aritm√©ticas entre arrays y escalares (<code>+ - * / ** %</code>)</li>
      <li>Operadores de comparaci√≥n (<code>== != < <= > >=</code>)</li>
      <li>Ufuncs de NumPy soportados en nopython mode</li>
      <li>Reducciones: <code>sum, prod, min, max, argmin, argmax, mean, var, std</code></li>
      <li>Creaci√≥n de arrays: <code>zeros, ones, arange, linspace</code></li>
      <li><code>numpy.dot</code> (matrix √ó vector, vector √ó vector)</li>
      <li>Funciones random (<code>rand, randn, normal, uniform</code>, etc.)</li>
    </ul>

    <h3>prange ‚Äî Loops paralelos expl√≠citos</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> njit, prange
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># Reducci√≥n paralela simple</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">parallel_sum</span>(A):
    s = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(A.shape[<span class="num">0</span>]):  <span class="cm"># cada iteraci√≥n en un thread distinto</span>
        s += A[i]
    <span class="kw">return</span> s

<span class="cm"># Reducci√≥n sobre array 2D</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">parallel_product_2d</span>(n):
    shp = (<span class="num">13</span>, <span class="num">17</span>)
    result = <span class="num">2</span> * np.ones(shp, np.int_)
    tmp = <span class="num">2</span> * np.ones_like(result)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        result *= tmp
    <span class="kw">return</span> result</code></pre>

    <div class="perf">
      Las reducciones soportadas por prange son: <code>+=, +, -=, -, *=, *, /=, /, max(), min()</code>. El operador <code>//=</code> NO est√° soportado (depende del orden de aplicaci√≥n).
    </div>

    <h3>Ejemplo real: Regresi√≥n Log√≠stica paralela</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">logistic_regression</span>(Y, X, w, iterations):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(iterations):
        w -= np.dot(
            ((<span class="num">1.0</span> / (<span class="num">1.0</span> + np.exp(-Y * np.dot(X, w))) - <span class="num">1.0</span>) * Y), X
        )
    <span class="kw">return</span> w

<span class="cm"># Numba fusiona autom√°ticamente las operaciones element-wise</span>
<span class="cm"># en un solo kernel paralelo. ¬°Sin cambiar el c√≥digo!</span></code></pre>

    <h3>‚ö†Ô∏è Race Conditions ‚Äî Errores comunes</h3>
<pre><code><span class="cm"># ‚ùå INCORRECTO: race condition en slices</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">prange_wrong</span>(x):
    y = np.zeros(<span class="num">4</span>)
    <span class="kw">for</span> i <span class="kw">in</span> prange(x.shape[<span class="num">0</span>]):
        y[:] += x[i]  <span class="cm"># ¬°M√∫ltiples threads escriben en y al mismo tiempo!</span>
    <span class="kw">return</span> y

<span class="cm"># ‚úÖ CORRECTO: reducci√≥n sobre array completo</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">prange_correct</span>(x):
    y = np.zeros(<span class="num">4</span>)
    <span class="kw">for</span> i <span class="kw">in</span> prange(x.shape[<span class="num">0</span>]):
        y += x[i]  <span class="cm"># Numba detecta la reducci√≥n correctamente</span>
    <span class="kw">return</span> y</code></pre>

    <div class="warn">
      Mutar listas/sets/dicts dentro de un <code>prange</code> <strong>NO es thread-safe</strong>. Nunca hagas <code>z.append(i)</code> dentro de un loop paralelo ‚Äî causar√° corrupci√≥n de memoria.
    </div>

    <h3>Diagn√≥sticos de paralelismo</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">test</span>(x):
    n = x.shape[<span class="num">0</span>]
    a = np.sin(x)
    b = np.cos(a * a)
    acc = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(n - <span class="num">2</span>):
        <span class="kw">for</span> j <span class="kw">in</span> prange(n - <span class="num">1</span>):
            acc += b[i] + b[j + <span class="num">1</span>]
    <span class="kw">return</span> acc

test(np.arange(<span class="num">10</span>))
test.parallel_diagnostics(level=<span class="num">4</span>)  <span class="cm"># Imprime qu√© se paraleliz√≥ y qu√© no</span></code></pre>
  </div>
</div>

<!-- ==================== M√ìDULO 6 ==================== -->
<div class="module" id="m6">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">06</span> @jitclass ‚Äî Clases compiladas</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>Permite definir clases cuyos m√©todos se compilan a c√≥digo nativo y cuya data se almacena como estructura C (sin overhead del int√©rprete).</p>

    <h3>Uso b√°sico con spec expl√≠cita</h3>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> int32, float32
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass

spec = [
    (<span class="st">'value'</span>, int32),
    (<span class="st">'array'</span>, float32[:]),
]

<span class="dec">@jitclass</span>(spec)
<span class="kw">class</span> <span class="fn">Bag</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, value):
        self.value = value
        self.array = np.zeros(value, dtype=np.float32)

    <span class="dec">@property</span>
    <span class="kw">def</span> <span class="fn">size</span>(self):
        <span class="kw">return</span> self.array.size

    <span class="kw">def</span> <span class="fn">increment</span>(self, val):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(self.size):
            self.array[i] += val
        <span class="kw">return</span> self.array

    <span class="dec">@staticmethod</span>
    <span class="kw">def</span> <span class="fn">add</span>(x, y):
        <span class="kw">return</span> x + y

mybag = Bag(<span class="num">21</span>)
mybag.increment(<span class="num">5.0</span>)</code></pre>

    <h3>Con type annotations (m√°s moderno)</h3>
<pre><code><span class="kw">from</span> typing <span class="kw">import</span> List
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass
<span class="kw">from</span> numba.typed <span class="kw">import</span> List <span class="kw">as</span> NumbaList

<span class="dec">@jitclass</span>
<span class="kw">class</span> <span class="fn">Counter</span>:
    value: int

    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.value = <span class="num">0</span>

    <span class="kw">def</span> <span class="fn">get</span>(self) -> int:
        ret = self.value
        self.value += <span class="num">1</span>
        <span class="kw">return</span> ret

<span class="dec">@jitclass</span>
<span class="kw">class</span> <span class="fn">ListLoopIterator</span>:
    counter: Counter
    items: List[float]

    <span class="kw">def</span> <span class="fn">__init__</span>(self, items: List[float]):
        self.items = items
        self.counter = Counter()</code></pre>

    <h3>Contenedores tipados como campos</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types, typed
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass

kv_ty = (types.int64, types.unicode_type)

<span class="dec">@jitclass</span>([
    (<span class="st">'d'</span>, types.DictType(*kv_ty)),
    (<span class="st">'l'</span>, types.ListType(types.float64))
])
<span class="kw">class</span> <span class="fn">ContainerHolder</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.d = typed.Dict.empty(*kv_ty)
        self.l = typed.List.empty_list(types.float64)

c = ContainerHolder()
c.d[<span class="num">1</span>] = <span class="st">"apple"</span>
c.l.append(<span class="num">123.</span>)</code></pre>

    <div class="warn">
      Los campos de contenedores (<code>Dict</code>, <code>List</code>) <strong>deben inicializarse expl√≠citamente</strong> en <code>__init__</code>. Escribir en un contenedor no inicializado causa <strong>segfault</strong>.
    </div>

    <h3>Dunder methods soportados</h3>
    <p>jitclass soporta una amplia gama de m√©todos especiales: <code>__abs__, __bool__, __getitem__, __setitem__, __len__, __hash__, __eq__, __ne__, __lt__, __le__, __gt__, __ge__, __add__, __mul__, __sub__, __truediv__, __floordiv__, __mod__, __pow__, __neg__, __pos__</code>, y todos sus variantes <code>__i*__</code> (in-place) y <code>__r*__</code> (reflected).</p>

    <div class="info">
      <strong>Limitaci√≥n actual:</strong> jitclass solo funciona en CPU. Soporte para GPU est√° planeado para versiones futuras.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 7 ==================== -->
<div class="module" id="m7">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">07</span> @stencil ‚Äî Patrones de c√°lculo sobre vecinos</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>Los stencils son patrones donde cada elemento del array resultado depende de un "vecindario" de elementos del array de entrada. Piensa en filtros de imagen, simulaciones de calor, aut√≥matas celulares, etc.</p>

    <h3>Uso b√°sico</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> stencil

<span class="cm"># Promedio de 4 vecinos (tipo Laplaciano discreto)</span>
<span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">kernel_laplacian</span>(a):
    <span class="kw">return</span> <span class="num">0.25</span> * (a[<span class="num">0</span>, <span class="num">1</span>] + a[<span class="num">1</span>, <span class="num">0</span>] + a[<span class="num">0</span>, <span class="num">-1</span>] + a[<span class="num">-1</span>, <span class="num">0</span>])

<span class="cm"># Los √≠ndices son RELATIVOS al punto actual</span>
<span class="cm"># a[0, 0] = elemento actual, a[-1, 0] = arriba, etc.</span>

input_arr = np.arange(<span class="num">100</span>).reshape((<span class="num">10</span>, <span class="num">10</span>)).astype(np.float64)
output = kernel_laplacian(input_arr)
<span class="cm"># Los bordes se ponen a 0 por defecto</span></code></pre>

    <h3>Media m√≥vil con neighborhood</h3>
<pre><code><span class="cm"># Media m√≥vil de 30 d√≠as (sin escribir 30 t√©rminos)</span>
<span class="dec">@stencil</span>(neighborhood=((<span class="num">-29</span>, <span class="num">0</span>),))
<span class="kw">def</span> <span class="fn">moving_average_30d</span>(a):
    cumul = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">-29</span>, <span class="num">1</span>):
        cumul += a[i]
    <span class="kw">return</span> cumul / <span class="num">30</span></code></pre>

    <h3>Opciones de stencil</h3>
    <table>
      <tr><th>Opci√≥n</th><th>Descripci√≥n</th></tr>
      <tr><td><code>cval=X</code></td><td>Valor para los bordes (default: 0)</td></tr>
      <tr><td><code>neighborhood=((min,max),...)</code></td><td>Define el rango de vecinos expl√≠citamente</td></tr>
      <tr><td><code>standard_indexing=("b",)</code></td><td>Arrays auxiliares con indexaci√≥n normal (no relativa)</td></tr>
    </table>

    <h3>Stencil + parallel (m√°xima velocidad)</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">apply_blur</span>(image):
    <span class="cm"># El stencil se paraleliza autom√°ticamente con parallel=True</span>
    <span class="kw">return</span> stencil(
        <span class="kw">lambda</span> a: <span class="num">0.2</span> * (a[<span class="num">-1</span>,<span class="num">0</span>] + a[<span class="num">1</span>,<span class="num">0</span>] + a[<span class="num">0</span>,<span class="num">-1</span>] + a[<span class="num">0</span>,<span class="num">1</span>] + a[<span class="num">0</span>,<span class="num">0</span>])
    )(image)</code></pre>
  </div>
</div>

<!-- ==================== M√ìDULO 8 ==================== -->
<div class="module" id="m8">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">08</span> Performance Tips ‚Äî Nivel profesional</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Tabla de rendimiento real (Intel i7, 10M elementos)</h3>
    <table>
      <tr><th>Configuraci√≥n</th><th>SVML</th><th>Tiempo</th><th>Speedup vs NumPy</th></tr>
      <tr><td>NumPy puro</td><td>‚Äî</td><td>5.84s</td><td>1x</td></tr>
      <tr><td><code>@njit</code></td><td>No</td><td>5.95s</td><td>~1x</td></tr>
      <tr><td><code>@njit</code></td><td>S√≠</td><td>2.26s</td><td>2.6x</td></tr>
      <tr><td><code>@njit(fastmath=True)</code></td><td>S√≠</td><td>1.8s</td><td>3.2x</td></tr>
      <tr><td><code>@njit(parallel=True)</code></td><td>S√≠</td><td>0.624s</td><td>9.4x</td></tr>
      <tr><td><code>@njit(parallel=True, fastmath=True)</code></td><td>S√≠</td><td>0.576s</td><td><strong>10x</strong></td></tr>
    </table>

    <h3>1. Instala Intel SVML</h3>
<pre><code>$ conda install intel-cmplr-lib-rt
<span class="cm"># Numba lo detecta autom√°ticamente</span>
<span class="cm"># Acelera funciones trascendentales (sin, cos, exp, log...)</span></code></pre>

    <h3>2. Loops > Vectorizaci√≥n NumPy (en Numba)</h3>
<pre><code><span class="cm"># Ambos son IGUAL de r√°pidos con @njit</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">vectorized_style</span>(x):
    <span class="kw">return</span> np.cos(x) ** <span class="num">2</span> + np.sin(x) ** <span class="num">2</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">loop_style</span>(x):
    r = np.empty_like(x)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(x)):
        r[i] = np.cos(x[i]) ** <span class="num">2</span> + np.sin(x[i]) ** <span class="num">2</span>
    <span class="kw">return</span> r

<span class="cm"># ¬°Escribe loops sin miedo! LLVM optimiza igual que C</span></code></pre>

    <div class="perf">
      En Numba, los loops son <strong>tan r√°pidos como el c√≥digo vectorizado</strong>, a diferencia de Python puro. Esto da m√°s flexibilidad y a menudo mejor uso de memoria (no creas arrays temporales).
    </div>

    <h3>3. Combinaci√≥n m√°xima</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>, cache=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">max_performance_sum</span>(A):
    n = <span class="fn">len</span>(A)
    acc = <span class="num">0.</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):  <span class="cm"># paralelo + fastmath = m√°xima velocidad</span>
        acc += np.sqrt(A[i])
    <span class="kw">return</span> acc
<span class="cm"># Resultado: ~5.37ms vs 35.2ms sin parallel/fastmath (6.5x)</span></code></pre>

    <h3>4. √Ålgebra lineal optimizada</h3>
<pre><code><span class="cm"># Aseg√∫rate de tener SciPy (para LAPACK/BLAS)</span>
<span class="cm"># Con Anaconda, SciPy usa Intel MKL autom√°ticamente</span>
$ pip install scipy

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">solve_system</span>(A, b):
    <span class="kw">return</span> np.linalg.solve(A, b)  <span class="cm"># Usa MKL internamente</span></code></pre>

    <h3>5. Evita object mode a toda costa</h3>
    <div class="warn">
      Si Numba no puede compilar algo en nopython mode, lanzar√° un error. Las causas m√°s comunes son: usar listas Python normales, llamar funciones no soportadas, o usar tipos de datos no soportados. Revisa los diagn√≥sticos de compilaci√≥n con <code>NUMBA_DEVELOPER_MODE=1</code>.
    </div>

    <h3>6. Memory layout matters</h3>
<pre><code><span class="cm"># C-contiguous (row-major) vs Fortran-contiguous (column-major)</span>
<span class="cm"># Acceder a memoria de forma contigua es MUCHO m√°s r√°pido</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">row_sum_fast</span>(matrix):   <span class="cm"># ‚úÖ Acceso por filas en C-order</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j]
    <span class="kw">return</span> total

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">col_sum_slow</span>(matrix):   <span class="cm"># ‚ùå Acceso por columnas en C-order (cache misses)</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
            total += matrix[i, j]
    <span class="kw">return</span> total</code></pre>
  </div>
</div>

<!-- ==================== M√ìDULO 9 ==================== -->
<div class="module" id="m9">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">09</span> Extendiendo Numba ‚Äî @overload e @intrinsic</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>El sistema de extensiones de Numba permite a√±adir soporte para funciones, m√©todos y tipos personalizados en nopython mode.</p>

    <h3>@overload ‚Äî Implementar funciones existentes</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.extending <span class="kw">import</span> overload

<span class="cm"># Supongamos que quieres que len() funcione con tuplas en Numba</span>
<span class="dec">@overload</span>(len)
<span class="kw">def</span> <span class="fn">tuple_len</span>(seq):
    <span class="kw">if</span> <span class="fn">isinstance</span>(seq, types.BaseTuple):
        n = <span class="fn">len</span>(seq)  <span class="cm"># esto se ejecuta en compile time</span>
        <span class="kw">def</span> <span class="fn">len_impl</span>(seq):
            <span class="kw">return</span> n   <span class="cm"># esto se ejecuta en runtime</span>
        <span class="kw">return</span> len_impl
    <span class="cm"># Si no retorna nada, Numba prueba otras implementaciones</span></code></pre>

    <h3>@overload_method ‚Äî A√±adir m√©todos a tipos</h3>
<pre><code><span class="kw">from</span> numba.extending <span class="kw">import</span> overload_method

<span class="dec">@overload_method</span>(types.Array, <span class="st">'take'</span>)
<span class="kw">def</span> <span class="fn">array_take</span>(arr, indices):
    <span class="kw">if</span> <span class="fn">isinstance</span>(indices, types.Array):
        <span class="kw">def</span> <span class="fn">take_impl</span>(arr, indices):
            n = indices.shape[<span class="num">0</span>]
            res = np.empty(n, arr.dtype)
            <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
                res[i] = arr[indices[i]]
            <span class="kw">return</span> res
        <span class="kw">return</span> take_impl</code></pre>

    <h3>@overload_attribute ‚Äî Propiedades personalizadas</h3>
<pre><code><span class="kw">from</span> numba.extending <span class="kw">import</span> overload_attribute

<span class="dec">@overload_attribute</span>(types.Array, <span class="st">'nbytes'</span>)
<span class="kw">def</span> <span class="fn">array_nbytes</span>(arr):
    <span class="kw">def</span> <span class="fn">get</span>(arr):
        <span class="kw">return</span> arr.size * arr.itemsize
    <span class="kw">return</span> get</code></pre>

    <h3>@intrinsic ‚Äî Escape hatch a LLVM IR</h3>
    <p>Para expertos: genera c√≥digo LLVM IR directamente. Se inlinea en el caller.</p>
<pre><code><span class="kw">from</span> numba.extending <span class="kw">import</span> intrinsic

<span class="dec">@intrinsic</span>
<span class="kw">def</span> <span class="fn">cast_int_to_byte_ptr</span>(typingctx, src):
    <span class="kw">if</span> <span class="fn">isinstance</span>(src, types.Integer):
        result_type = types.CPointer(types.uint8)
        sig = result_type(types.uintp)
        <span class="kw">def</span> <span class="fn">codegen</span>(context, builder, signature, args):
            [src] = args
            rtype = signature.return_type
            llrtype = context.get_value_type(rtype)
            <span class="kw">return</span> builder.inttoptr(src, llrtype)
        <span class="kw">return</span> sig, codegen</code></pre>

    <h3>Importar funciones Cython</h3>
<pre><code><span class="kw">import</span> ctypes
<span class="kw">from</span> numba.extending <span class="kw">import</span> get_cython_function_address

<span class="cm"># Obtener direcci√≥n de funci√≥n C definida en un m√≥dulo Cython</span>
addr = get_cython_function_address(<span class="st">"foo"</span>, <span class="st">"myexp"</span>)
functype = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double)
myexp = functype(addr)

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">double_myexp</span>(x):
    <span class="kw">return</span> <span class="num">2</span> * myexp(x)  <span class="cm"># ¬°Llama a la funci√≥n C dentro de Numba!</span></code></pre>

    <h3>StructRef ‚Äî Estructuras mutables por referencia</h3>
<pre><code><span class="kw">from</span> numba.core <span class="kw">import</span> types
<span class="kw">from</span> numba.experimental <span class="kw">import</span> structref

<span class="dec">@structref.register</span>
<span class="kw">class</span> <span class="fn">MyStructType</span>(types.StructRef):
    <span class="kw">def</span> <span class="fn">preprocess_fields</span>(self, fields):
        <span class="kw">return</span> <span class="fn">tuple</span>(
            (name, types.unliteral(typ)) <span class="kw">for</span> name, typ <span class="kw">in</span> fields
        )

<span class="cm"># Proxy Python para interactuar desde el int√©rprete</span>
<span class="kw">class</span> <span class="fn">MyStruct</span>(structref.StructRefProxy):
    <span class="kw">def</span> <span class="fn">__new__</span>(cls, name, vector):
        <span class="kw">return</span> structref.StructRefProxy.__new__(cls, name, vector)

<span class="cm"># Se pueden definir propiedades que delegan a funciones JIT</span></code></pre>
  </div>
</div>


<!--
  ============================================================
  M√ìDULOS: Memoria & Cache + Benchmarking Profesional
  ============================================================
  Despu√©s de haber observado el m√≥dulo de decoradores (‚òÖ), es importante ver esto
  ANTES del m√≥dulo 10 (Cheatsheet).
  ============================================================
-->

<!-- ==================== M√ìDULO MEMORIA ==================== -->
<div class="module" id="m-memory">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">‚ö°</span> Memoria, Cache y CPU: El Fundamento Invisible del Rendimiento</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Esto es lo que <strong>nadie te ense√±a</strong> pero determina el 80% del rendimiento de tu c√≥digo num√©rico. Tu CPU puede hacer miles de millones de operaciones por segundo, pero si est√° esperando datos de RAM, est√° literalmente sentada sin hacer nada. Entender esto transforma c√≥mo escribes c√≥digo para siempre.</p>

    <!-- ============ JERARQU√çA DE MEMORIA ============ -->
    <h3>üèóÔ∏è La jerarqu√≠a de memoria: por qu√© tu c√≥digo es lento</h3>

    <p>Tu CPU NO accede a RAM directamente. Existe una cadena de cach√©s intermedias, cada una m√°s r√°pida pero m√°s peque√±a:</p>

    <table>
      <tr><th>Nivel</th><th>Tama√±o t√≠pico</th><th>Latencia</th><th>Analog√≠a</th></tr>
      <tr><td><strong>Registros</strong></td><td>~1 KB</td><td>~0.3 ns (1 ciclo)</td><td>Tu mano: instant√°neo</td></tr>
      <tr><td><strong>Cache L1</strong></td><td>32-64 KB por core</td><td>~1 ns (3-4 ciclos)</td><td>Tu escritorio: 1 segundo</td></tr>
      <tr><td><strong>Cache L2</strong></td><td>256 KB - 1 MB por core</td><td>~4 ns (10-12 ciclos)</td><td>Tu estanter√≠a: 4 segundos</td></tr>
      <tr><td><strong>Cache L3</strong></td><td>8-64 MB compartido</td><td>~12 ns (30-40 ciclos)</td><td>La biblioteca del edificio: 12 segundos</td></tr>
      <tr><td><strong>RAM</strong></td><td>8-128 GB</td><td>~60-100 ns (200+ ciclos)</td><td>Amazon: un minuto esperando</td></tr>
      <tr><td><strong>Disco SSD</strong></td><td>TB</td><td>~10,000-100,000 ns</td><td>Pedir algo de China: horas</td></tr>
    </table>

    <div class="warn">
      Cuando tu CPU necesita un dato que NO est√° en cache (un <strong>cache miss</strong>), espera ~200 ciclos para traerlo de RAM. En esos 200 ciclos podr√≠a haber hecho 200 multiplicaciones de punto flotante. <strong>Un programa con muchos cache misses puede estar ejecutando a un 5% de la capacidad real de tu CPU.</strong>
    </div>

    <h3>üì¶ Cache lines: la unidad m√≠nima de memoria</h3>

    <p>La CPU nunca trae "un solo n√∫mero" de RAM. Siempre trae un bloque llamado <strong>cache line</strong>, t√≠picamente de <strong>64 bytes</strong>. Si trabajas con <code>float64</code> (8 bytes cada uno), una cache line trae 8 valores consecutivos de golpe.</p>

    <p>Esto tiene una consecuencia brutal:</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="cm"># Imagina una matriz 1000x1000 de float64 en C-order (row-major)</span>
<span class="cm"># En memoria, se almacena as√≠:</span>
<span class="cm"># [fila0_col0, fila0_col1, fila0_col2, ..., fila1_col0, fila1_col1, ...]</span>
<span class="cm">#  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
<span class="cm">#  Estos 8 valores est√°n en la MISMA cache line</span>

matrix = np.random.rand(<span class="num">1000</span>, <span class="num">1000</span>)

<span class="cm"># ‚úÖ ACCESO POR FILAS: aprovecha la cache line completa</span>
<span class="cm"># Cuando lees matrix[0, 0], la CPU trae matrix[0, 0:8] gratis</span>
<span class="cm"># El siguiente acceso matrix[0, 1] YA EST√Å EN CACHE ‚Üí 0 espera</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_row_major</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):      <span class="cm"># filas (dimensi√≥n externa)</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):  <span class="cm"># columnas (dimensi√≥n interna)</span>
            total += matrix[i, j]          <span class="cm"># ‚úÖ Acceso contiguo en memoria</span>
    <span class="kw">return</span> total

<span class="cm"># ‚ùå ACCESO POR COLUMNAS: destruye la cache</span>
<span class="cm"># Cuando lees matrix[0, 0], la CPU trae fila0_col0..col7</span>
<span class="cm"># Pero el siguiente acceso es matrix[1, 0] ‚Üí OTRA cache line ‚Üí cache miss</span>
<span class="cm"># Cada acceso salta 1000 float64 (8000 bytes) ‚Üí 125 cache lines de distancia</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_col_major</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):  <span class="cm"># columnas primero</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):  <span class="cm"># filas despu√©s</span>
            total += matrix[i, j]          <span class="cm"># ‚ùå Saltos de 8KB entre accesos</span>
    <span class="kw">return</span> total

<span class="cm"># Benchmark real en una matriz 4000x4000:</span>
<span class="cm"># sum_row_major: ~12ms  (cache hits: ~98%)</span>
<span class="cm"># sum_col_major: ~85ms  (cache hits: ~12%)</span>
<span class="cm"># ¬°~7x m√°s lento con EXACTAMENTE las mismas operaciones matem√°ticas!</span></code></pre>

    <div class="perf">
      <strong>Regla de oro:</strong> En C-order (default de NumPy), el loop m√°s interno siempre debe recorrer la <strong>√∫ltima dimensi√≥n</strong> (columnas en 2D). En Fortran-order, el loop m√°s interno recorre la <strong>primera dimensi√≥n</strong>. Si ves un loop donde el √≠ndice externo es el que var√≠a m√°s r√°pido, tu c√≥digo tiene un problema serio de cache.
    </div>

    <!-- ============ C-ORDER vs F-ORDER ============ -->
    <h3>üîÑ C-order vs Fortran-order: elige seg√∫n tu patr√≥n de acceso</h3>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># C-ORDER (row-major): filas son contiguas</span>
<span class="cm"># Memoria: [f0c0, f0c1, f0c2, f1c0, f1c1, f1c2, ...]</span>
c_matrix = np.zeros((<span class="num">1000</span>, <span class="num">1000</span>), order=<span class="st">'C'</span>)  <span class="cm"># default de NumPy</span>

<span class="cm"># FORTRAN-ORDER (column-major): columnas son contiguas</span>
<span class="cm"># Memoria: [f0c0, f1c0, f2c0, f0c1, f1c1, f2c1, ...]</span>
f_matrix = np.zeros((<span class="num">1000</span>, <span class="num">1000</span>), order=<span class="st">'F'</span>)
<span class="cm"># Equivalente en Numba: np.zeros((1000, 1000)[::-1]).T</span>

<span class="cm"># ¬øCU√ÅNDO USAR CADA UNO?</span>

<span class="cm"># Caso 1: Procesar datos tabulares (filas = registros, columnas = features)</span>
<span class="cm"># Si accedes fila por fila ‚Üí C-order ‚úÖ</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_registros</span>(data):  <span class="cm"># data es (n_registros, n_features) C-order</span>
    n = data.shape[<span class="num">0</span>]
    result = np.empty(n)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        <span class="cm"># Acceder a data[i, :] es contiguo en C-order ‚úÖ</span>
        result[i] = np.sum(data[i, :])
    <span class="kw">return</span> result

<span class="cm"># Caso 2: Series temporales (filas = tiempo, columnas = sensores)</span>
<span class="cm"># Si accedes columna por columna ‚Üí Fortran-order ‚úÖ</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_sensores</span>(data):  <span class="cm"># data es (n_tiempos, n_sensores) F-order</span>
    n_sensors = data.shape[<span class="num">1</span>]
    result = np.empty(n_sensors)
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(n_sensors):
        <span class="cm"># Acceder a data[:, j] es contiguo en F-order ‚úÖ</span>
        result[j] = np.mean(data[:, j])
    <span class="kw">return</span> result</code></pre>

    <h4>Verificar el layout de tus arrays</h4>
<pre><code>arr = np.zeros((<span class="num">100</span>, <span class="num">100</span>))
<span class="fn">print</span>(arr.flags)
<span class="cm">#   C_CONTIGUOUS : True    ‚Üê C-order</span>
<span class="cm">#   F_CONTIGUOUS : False</span>

<span class="cm"># Strides te dicen cu√°ntos bytes hay entre elementos consecutivos</span>
<span class="fn">print</span>(arr.strides)
<span class="cm"># (800, 8) ‚Üí saltar una fila = 800 bytes, saltar una columna = 8 bytes</span>
<span class="cm"># El stride m√°s peque√±o (8) es la dimensi√≥n contigua ‚Üí √∫ltima dim (columnas)</span>

<span class="cm"># ‚ö†Ô∏è CUIDADO: operaciones que destruyen la contig√ºidad</span>
sliced = arr[::2, :]  <span class="cm"># Slice con step</span>
<span class="fn">print</span>(sliced.flags.c_contiguous)  <span class="cm"># False ‚Äî ya no es contiguo</span>
<span class="fn">print</span>(sliced.strides)             <span class="cm"># (1600, 8) ‚Äî stride duplicado</span>

transposed = arr.T
<span class="fn">print</span>(transposed.flags.c_contiguous)  <span class="cm"># False</span>
<span class="fn">print</span>(transposed.flags.f_contiguous)  <span class="cm"># True ‚Äî ahora es F-order</span></code></pre>

    <h4>Forzar contig√ºidad cuando la necesitas</h4>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_seguro</span>(data):
    <span class="cm"># Si data puede llegar no-contiguo (ej: un slice), haz copia contigua</span>
    <span class="kw">if not</span> data.flags.c_contiguous:
        data = np.ascontiguousarray(data)  <span class="cm"># Copia, pero ahora es C-contiguo</span>

    <span class="cm"># NOTA: np.ascontiguousarray no est√° en Numba nopython mode</span>
    <span class="cm"># Alternativa dentro de @njit:</span>
    data_copy = data.copy()  <span class="cm"># .copy() devuelve C-contiguous por defecto</span>
    <span class="kw">return</span> data_copy</code></pre>

    <div class="tip">
      En las signatures de Numba puedes <strong>exigir</strong> un layout espec√≠fico:<br>
      <code>types.Array(types.float64, 2, 'C')</code> ‚Üí solo acepta C-contiguous<br>
      <code>types.Array(types.float64, 2, 'A')</code> ‚Üí acepta cualquier layout<br>
      Si declaras <code>'C'</code> y Numba sabe que el array es contiguo, puede generar c√≥digo m√°s agresivamente optimizado.
    </div>

    <!-- ============ PRE-ALOCACI√ìN ============ -->
    <h3>üè≠ Pre-alocaci√≥n: nunca aloquemos dentro del loop caliente</h3>

    <p>Cada <code>np.zeros()</code>, <code>np.empty()</code>, o <code>np.array()</code> dentro de un loop pide memoria al sistema operativo. Esto implica: syscall al kernel, posible page fault, invalidaci√≥n de cache, y trabajo para el garbage collector. En un loop que se ejecuta millones de veces, esto es devastador.</p>

<pre><code><span class="cm"># ‚ùå TERRIBLE: aloca memoria en CADA iteraci√≥n</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_terrible</span>(dataset, n_iter):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        temp = np.zeros(<span class="num">1000</span>)      <span class="cm"># ‚ùå Alocaci√≥n + inicializaci√≥n √ó n_iter veces</span>
        result = np.empty(<span class="num">1000</span>)    <span class="cm"># ‚ùå Otra alocaci√≥n √ó n_iter veces</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1000</span>):
            temp[j] = dataset[i, j] ** <span class="num">2</span>
            result[j] = np.sqrt(temp[j])
    <span class="kw">return</span> result

<span class="cm"># ‚úÖ CORRECTO: pre-aloca FUERA del loop, reutiliza buffers</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_optimo</span>(dataset, n_iter):
    temp = np.empty(<span class="num">1000</span>)        <span class="cm"># ‚úÖ Se aloca UNA sola vez</span>
    result = np.empty(<span class="num">1000</span>)      <span class="cm"># ‚úÖ Se aloca UNA sola vez</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1000</span>):
            temp[j] = dataset[i, j] ** <span class="num">2</span>  <span class="cm"># Reutiliza el buffer</span>
            result[j] = np.sqrt(temp[j])
    <span class="kw">return</span> result
<span class="cm"># Con n_iter=100000: terrible ~8.2s vs √≥ptimo ~1.1s ‚Üí 7.5x m√°s r√°pido</span></code></pre>

    <h4>Allocation hoisting: Numba lo intenta, pero no siempre puede</h4>
<pre><code><span class="cm"># Numba con parallel=True INTENTA hoistear alocaciones autom√°ticamente:</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">auto_hoist</span>(n):
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        temp = np.zeros((<span class="num">50</span>, <span class="num">50</span>))  <span class="cm"># Numba intenta sacar esto fuera del loop</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">50</span>):
            temp[j, j] = i

<span class="cm"># Internamente, Numba transforma esto a:</span>
<span class="cm"># temp = np.empty((50, 50))        ‚Üê alocaci√≥n hoisted fuera del loop</span>
<span class="cm"># for i in prange(n):</span>
<span class="cm">#     temp[:] = 0                   ‚Üê solo la inicializaci√≥n queda dentro</span>
<span class="cm">#     for j in range(50):</span>
<span class="cm">#         temp[j, j] = i</span>

<span class="cm"># PERO: solo funciona con np.empty y np.zeros</span>
<span class="cm"># Alocaciones m√°s complejas o condicionales NO se hoistean</span>
<span class="cm"># REGLA: no conf√≠es en el hoisting autom√°tico. Pre-aloca t√∫ mismo.</span></code></pre>

    <!-- ============ EVITAR TEMPORALES ============ -->
    <h3>üö´ Eliminar arrays temporales innecesarios</h3>

    <p>Una de las ventajas m√°s grandes de Numba sobre NumPy puro es que puedes eliminar arrays temporales. En NumPy vectorizado, cada operaci√≥n intermedia crea un array temporal:</p>

<pre><code><span class="cm"># Con NumPy puro ‚Äî CADA operaci√≥n crea un array temporal en RAM:</span>
<span class="kw">def</span> <span class="fn">numpy_style</span>(x):
    temp1 = np.sin(x)           <span class="cm"># temp1: 80MB si x tiene 10M float64</span>
    temp2 = temp1 ** <span class="num">2</span>          <span class="cm"># temp2: otros 80MB</span>
    temp3 = np.cos(x)           <span class="cm"># temp3: otros 80MB</span>
    temp4 = temp3 ** <span class="num">2</span>          <span class="cm"># temp4: otros 80MB</span>
    result = temp1 + temp3      <span class="cm"># result: otros 80MB</span>
    <span class="kw">return</span> result
    <span class="cm"># Total: ~400MB de temporales para un input de 80MB</span>
    <span class="cm"># Cada temporal destruye la cache y causa page faults</span>

<span class="cm"># Con Numba ‚Äî CERO temporales, todo en registros/cache:</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">numba_fused</span>(x):
    n = <span class="fn">len</span>(x)
    result = np.empty(n)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        <span class="cm"># Cada valor se calcula y se guarda INMEDIATAMENTE</span>
        <span class="cm"># sin crear arrays intermedios</span>
        s = np.sin(x[i])
        c = np.cos(x[i])
        result[i] = s ** <span class="num">2</span> + c ** <span class="num">2</span>
        <span class="cm"># s y c viven en registros de CPU, no en RAM</span>
    <span class="kw">return</span> result
    <span class="cm"># Total: 0 bytes de temporales. Solo input + output.</span>

<span class="cm"># Con 10M elementos:</span>
<span class="cm"># numpy_style:  ~650ms (memory bandwidth limited)</span>
<span class="cm"># numba_fused:  ~180ms (compute limited ‚Äî que es lo que queremos)</span></code></pre>

    <div class="perf">
      <strong>Patr√≥n fundamental:</strong> Fusiona operaciones que NumPy har√≠a en pasos separados en un solo loop. Esto mantiene los datos en cache L1/L2 durante todo el c√°lculo. Es la raz√≥n #1 por la que Numba puede superar a NumPy incluso en operaciones vectorizadas.
    </div>

    <!-- ============ CHUNKING ============ -->
    <h3>üß© Chunking: procesamiento por bloques que caben en cache</h3>

    <p>Cuando tu dataset es enorme (cientos de MB o GB), ni siquiera un loop fila-por-fila es √≥ptimo. El truco es procesar en <strong>bloques que quepan en cache L2/L3</strong>.</p>

<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_naive</span>(matrix):
    <span class="cm">"""Suma de todos los elementos ‚Äî naive."""</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j]
    <span class="kw">return</span> total
<span class="cm"># Para matrices peque√±as, esto est√° bien.</span>
<span class="cm"># Para matrices de 10000x10000+, la cache L2 (256KB) no puede</span>
<span class="cm"># contener una fila entera si es muy ancha.</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_blocked</span>(matrix, block_size=<span class="num">64</span>):
    <span class="cm">"""Suma con blocking: procesa sub-matrices que caben en cache L2."""</span>
    total = <span class="num">0.0</span>
    rows, cols = matrix.shape

    <span class="cm"># Recorre por bloques de block_size √ó block_size</span>
    <span class="kw">for</span> bi <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, rows, block_size):
        <span class="kw">for</span> bj <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, cols, block_size):
            <span class="cm"># Este bloque (64√ó64 float64 = 32KB) cabe en L1 cache</span>
            bi_end = <span class="fn">min</span>(bi + block_size, rows)
            bj_end = <span class="fn">min</span>(bj + block_size, cols)
            <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(bi, bi_end):
                <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(bj, bj_end):
                    total += matrix[i, j]
    <span class="kw">return</span> total

<span class="cm"># Ejemplo cl√°sico donde blocking es CR√çTICO: multiplicaci√≥n de matrices</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">matmul_blocked</span>(A, B, block_size=<span class="num">64</span>):
    <span class="cm">"""Multiplicaci√≥n de matrices con cache blocking.
    Para matrices grandes, puede ser 3-5x m√°s r√°pido que el naive."""</span>
    M, K = A.shape
    K2, N = B.shape
    C = np.zeros((M, N))

    <span class="kw">for</span> bi <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, M, block_size):
        <span class="kw">for</span> bj <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, N, block_size):
            <span class="kw">for</span> bk <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, K, block_size):
                <span class="cm"># Multiplicar sub-bloques que caben en cache</span>
                bi_end = <span class="fn">min</span>(bi + block_size, M)
                bj_end = <span class="fn">min</span>(bj + block_size, N)
                bk_end = <span class="fn">min</span>(bk + block_size, K)
                <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(bi, bi_end):
                    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(bj, bj_end):
                        temp = <span class="num">0.0</span>
                        <span class="kw">for</span> k <span class="kw">in</span> <span class="fn">range</span>(bk, bk_end):
                            temp += A[i, k] * B[k, j]
                        C[i, j] += temp
    <span class="kw">return</span> C</code></pre>

    <h4>¬øC√≥mo elegir el tama√±o del bloque?</h4>
    <table>
      <tr><th>Cache objetivo</th><th>Tama√±o t√≠pico</th><th>block_size para float64</th><th>C√°lculo</th></tr>
      <tr><td>L1 (32KB)</td><td>32,768 bytes</td><td><strong>64√ó64</strong></td><td>64√ó64√ó8 = 32,768 bytes</td></tr>
      <tr><td>L2 (256KB)</td><td>262,144 bytes</td><td><strong>128√ó128</strong></td><td>128√ó128√ó8 = 131,072 (dejas espacio para otros datos)</td></tr>
      <tr><td>L3 (8MB)</td><td>8,388,608 bytes</td><td><strong>512√ó512</strong></td><td>Para datasets gigantes con poca reutilizaci√≥n</td></tr>
    </table>

    <div class="info">
      La regla general: el bloque completo (3 sub-matrices para matmul: bloque de A + bloque de B + bloque de C) debe caber en el cache que est√°s apuntando. Para L1 targeting con matmul: <code>3 √ó block_size¬≤ √ó 8 bytes < 32KB</code>, as√≠ que <code>block_size ‚âà 36</code>. En la pr√°ctica, <strong>potencias de 2 entre 32 y 128</strong> suelen funcionar bien. Experimenta con benchmarking.
    </div>

    <!-- ============ PATRONES DE ACCESO ============ -->
    <h3>üîÅ Patrones de acceso: de terrible a √≥ptimo</h3>

<pre><code><span class="cm"># Escenario: Transponer y procesar una matriz grande</span>
<span class="cm"># Datos: 5000√ó5000 de float64 (200MB)</span>

<span class="cm"># ‚ùå NIVEL 1 ‚Äî TERRIBLE: acceso aleatorio</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">terrible</span>(matrix, indices):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> idx <span class="kw">in</span> indices:  <span class="cm"># indices aleatorios ‚Üí cache miss garantizado</span>
        total += matrix.flat[idx]
    <span class="kw">return</span> total

<span class="cm"># ‚ùå NIVEL 2 ‚Äî MALO: stride grande (acceso por columnas en C-order)</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">malo</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
            total += matrix[i, j]  <span class="cm"># stride = n_cols √ó 8 bytes</span>
    <span class="kw">return</span> total

<span class="cm"># üÜó NIVEL 3 ‚Äî DECENTE: acceso secuencial contiguo</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">decente</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j]  <span class="cm"># stride = 8 bytes (contiguo)</span>
    <span class="kw">return</span> total

<span class="cm"># ‚úÖ NIVEL 4 ‚Äî BUENO: contiguo + sin temporales + parallel</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">bueno</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(matrix.shape[<span class="num">0</span>]):
        row_sum = <span class="num">0.0</span>  <span class="cm"># acumulador local ‚Üí vive en registro</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            row_sum += matrix[i, j]
        total += row_sum  <span class="cm"># reducci√≥n paralela segura</span>
    <span class="kw">return</span> total

<span class="cm"># ‚ö° NIVEL 5 ‚Äî √ìPTIMO: blocking + parallel + fastmath + SVML</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">optimo</span>(matrix, block_size=<span class="num">64</span>):
    total = <span class="num">0.0</span>
    rows, cols = matrix.shape
    <span class="kw">for</span> bi <span class="kw">in</span> prange(<span class="num">0</span>, rows, block_size):  <span class="cm"># bloques en paralelo</span>
        block_sum = <span class="num">0.0</span>
        bi_end = <span class="fn">min</span>(bi + block_size, rows)
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(bi, bi_end):
            <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(cols):
                block_sum += matrix[i, j]
        total += block_sum
    <span class="kw">return</span> total

<span class="cm"># Rendimiento relativo (5000√ó5000 float64):</span>
<span class="cm"># terrible: ~850ms   (baseline)</span>
<span class="cm"># malo:     ~340ms   (2.5x m√°s r√°pido)</span>
<span class="cm"># decente:  ~48ms    (17x m√°s r√°pido)</span>
<span class="cm"># bueno:    ~12ms    (70x m√°s r√°pido)</span>
<span class="cm"># √≥ptimo:   ~6ms     (140x m√°s r√°pido)</span></code></pre>

    <!-- ============ STRUCT OF ARRAYS ============ -->
    <h3>üìê Structure of Arrays (SoA) vs Array of Structures (AoS)</h3>

    <p>Este es un patr√≥n de dise√±o de datos que puede cambiar el rendimiento dram√°ticamente. Supongamos que tienes 1 mill√≥n de part√≠culas con posici√≥n (x, y, z) y velocidad (vx, vy, vz):</p>

<pre><code><span class="cm"># ‚ùå ARRAY OF STRUCTURES (AoS) ‚Äî Natural pero lento para procesamiento masivo</span>
<span class="cm"># Cada part√≠cula es una "fila" con todos sus atributos juntos</span>
<span class="cm"># Memoria: [x0,y0,z0,vx0,vy0,vz0, x1,y1,z1,vx1,vy1,vz1, ...]</span>
particles_aos = np.zeros((<span class="num">1_000_000</span>, <span class="num">6</span>))  <span class="cm"># columnas: x,y,z,vx,vy,vz</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">update_positions_aos</span>(particles, dt):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(particles.shape[<span class="num">0</span>]):
        <span class="cm"># Para actualizar x, la CPU trae una cache line que contiene</span>
        <span class="cm"># [x0, y0, z0, vx0, vy0, vz0, x1, y1] ‚Äî solo usa x0 y vx0</span>
        <span class="cm"># 6 de cada 8 valores en la cache line son BASURA para esta operaci√≥n</span>
        particles[i, <span class="num">0</span>] += particles[i, <span class="num">3</span>] * dt  <span class="cm"># x += vx * dt</span>
        particles[i, <span class="num">1</span>] += particles[i, <span class="num">4</span>] * dt  <span class="cm"># y += vy * dt</span>
        particles[i, <span class="num">2</span>] += particles[i, <span class="num">5</span>] * dt  <span class="cm"># z += vz * dt</span>

<span class="cm"># ‚úÖ STRUCTURE OF ARRAYS (SoA) ‚Äî Cada atributo en su propio array</span>
<span class="cm"># Memoria de x: [x0, x1, x2, x3, x4, x5, x6, x7, ...]</span>
<span class="cm"># Memoria de vx: [vx0, vx1, vx2, vx3, vx4, vx5, vx6, vx7, ...]</span>
n = <span class="num">1_000_000</span>
x  = np.zeros(n)
y  = np.zeros(n)
z  = np.zeros(n)
vx = np.random.randn(n)
vy = np.random.randn(n)
vz = np.random.randn(n)

<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">update_positions_soa</span>(x, y, z, vx, vy, vz, dt):
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(x)):
        <span class="cm"># La CPU trae [x0, x1, x2, x3, x4, x5, x6, x7]</span>
        <span class="cm"># TODOS los 8 valores en la cache line son √∫tiles</span>
        <span class="cm"># Adem√°s, LLVM puede usar instrucciones SIMD (procesar 4 a la vez)</span>
        x[i] += vx[i] * dt
        y[i] += vy[i] * dt
        z[i] += vz[i] * dt

<span class="cm"># Benchmarks con 1M part√≠culas:</span>
<span class="cm"># AoS: ~4.8ms</span>
<span class="cm"># SoA: ~0.6ms ‚Üí 8x m√°s r√°pido con los mismos c√°lculos</span></code></pre>

    <div class="perf">
      <strong>¬øCu√°ndo usar SoA?</strong> Siempre que proceses <strong>un atributo a la vez</strong> sobre muchas entidades (actualizar todas las posiciones, calcular todas las energ√≠as, etc.). <strong>¬øCu√°ndo usar AoS?</strong> Cuando proceses <strong>todos los atributos de una entidad</strong> juntos y esa entidad sea peque√±a (cabe en pocas cache lines).
    </div>

    <!-- ============ RESUMEN VISUAL ============ -->
    <h3>üìã Checklist de optimizaci√≥n de memoria</h3>
    <table>
      <tr><th>#</th><th>Verificar</th><th>C√≥mo</th></tr>
      <tr><td>1</td><td>¬øLoop interno recorre dimensi√≥n contigua?</td><td>√öltima dim en C-order, primera en F-order</td></tr>
      <tr><td>2</td><td>¬øAloco arrays dentro de loops calientes?</td><td>Mover <code>np.empty/zeros</code> fuera del loop</td></tr>
      <tr><td>3</td><td>¬øCreo arrays temporales innecesarios?</td><td>Fusionar operaciones en un solo loop</td></tr>
      <tr><td>4</td><td>¬øMis arrays son contiguos?</td><td>Verificar <code>arr.flags.c_contiguous</code></td></tr>
      <tr><td>5</td><td>¬øMi dataset cabe en cache?</td><td>Si no ‚Üí usar blocking con tama√±o < L2</td></tr>
      <tr><td>6</td><td>¬øEstructura de datos apropiada?</td><td>SoA si proceso un atributo a la vez sobre muchas entidades</td></tr>
      <tr><td>7</td><td>¬øAcceso aleatorio a arrays grandes?</td><td>Reordenar datos o usar √≠ndices sorted</td></tr>
      <tr><td>8</td><td>¬øUso <code>parallel=True</code> en ops de arrays?</td><td>Divide el trabajo en cache-friendly chunks entre cores</td></tr>
    </table>

  </div>
</div>

<!-- ==================== M√ìDULO BENCHMARKING ==================== -->
<div class="module" id="m-bench">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">üî¨</span> Benchmarking: Medir Sin Mentirte</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Sin benchmarking correcto, todas las optimizaciones anteriores son adivinanzas. La mayor√≠a de la gente benchmarkea Numba mal, obtiene n√∫meros incorrectos, y toma decisiones equivocadas. Esta secci√≥n te ense√±a a medir de verdad.</p>

    <!-- ============ ERROR #1: COMPILACI√ìN ============ -->
    <h3>üö® Error #1: Medir el tiempo de compilaci√≥n</h3>

    <p>El error m√°s com√∫n y m√°s grave. La primera llamada a una funci√≥n Numba incluye el tiempo de compilaci√≥n (0.1s - 5s). Si mides eso, tus benchmarks no significan nada.</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit
<span class="kw">import</span> time

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">mi_funcion</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))

data = np.random.rand(<span class="num">1_000_000</span>)

<span class="cm"># ‚ùå MAL: incluye compilaci√≥n</span>
start = time.time()
mi_funcion(data)
<span class="fn">print</span>(f<span class="st">"Tiempo: {time.time() - start:.4f}s"</span>)
<span class="cm"># Resultado: "Tiempo: 0.8523s" ‚Üê ¬°MENTIRA! 0.85s son de compilaci√≥n</span>

<span class="cm"># ‚úÖ BIEN: calentar primero, luego medir</span>
mi_funcion(data)  <span class="cm"># ‚Üê WARMUP: primera llamada compila (descartar)</span>

start = time.time()
<span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">100</span>):
    mi_funcion(data)
elapsed = (time.time() - start) / <span class="num">100</span>
<span class="fn">print</span>(f<span class="st">"Tiempo real: {elapsed*1000:.2f}ms"</span>)
<span class="cm"># Resultado: "Tiempo real: 1.23ms" ‚Üê La verdad</span></code></pre>

    <div class="warn">
      <strong>SIEMPRE</strong> haz al menos una llamada de warmup antes de medir. Si est√°s midiendo diferentes funciones para comparar, haz warmup de TODAS antes de empezar a medir CUALQUIERA.
    </div>

    <!-- ============ TEMPLATE DE BENCHMARK ============ -->
    <h3>üß™ Template de benchmarking profesional</h3>

    <p>Este es el patr√≥n que debes usar siempre. C√≥pialo y ad√°ptalo:</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit, prange
<span class="kw">import</span> time
<span class="kw">import</span> sys

<span class="kw">def</span> <span class="fn">benchmark</span>(func, *args, n_warmup=<span class="num">3</span>, n_runs=<span class="num">50</span>, label=<span class="st">""</span>):
    <span class="cm">"""Benchmark profesional: warmup + m√∫ltiples runs + estad√≠sticas."""</span>

    <span class="cm"># 1. WARMUP: compilar + estabilizar cache</span>
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_warmup):
        func(*args)

    <span class="cm"># 2. MEDIR: m√∫ltiples ejecuciones</span>
    times = []
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_runs):
        start = time.perf_counter()  <span class="cm"># ‚Üê perf_counter, NO time.time()</span>
        result = func(*args)
        elapsed = time.perf_counter() - start
        times.append(elapsed)

    <span class="cm"># 3. ANALIZAR: no solo la media</span>
    times = np.array(times)
    <span class="fn">print</span>(f<span class="st">"{'‚îÄ'*50}"</span>)
    <span class="fn">print</span>(f<span class="st">"  {label or func.__name__}"</span>)
    <span class="fn">print</span>(f<span class="st">"{'‚îÄ'*50}"</span>)
    <span class="fn">print</span>(f<span class="st">"  Mediana:   {np.median(times)*1000:10.3f} ms"</span>)  <span class="cm"># ‚Üê MEDIANA, no media</span>
    <span class="fn">print</span>(f<span class="st">"  Media:     {np.mean(times)*1000:10.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"  Min:       {np.min(times)*1000:10.3f} ms"</span>)  <span class="cm"># ‚Üê mejor caso real</span>
    <span class="fn">print</span>(f<span class="st">"  Max:       {np.max(times)*1000:10.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"  Std:       {np.std(times)*1000:10.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"  Runs:      {n_runs}"</span>)
    <span class="fn">print</span>()
    <span class="kw">return</span> np.median(times)

<span class="cm"># USO:</span>
data = np.random.rand(<span class="num">10_000_000</span>)

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">version_a</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))

<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">version_b</span>(x):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(x)):
        total += np.sqrt(x[i])
    <span class="kw">return</span> total

t_a = benchmark(version_a, data, label=<span class="st">"@njit b√°sico"</span>)
t_b = benchmark(version_b, data, label=<span class="st">"@njit parallel+fastmath"</span>)
t_np = benchmark(np.sum, np.sqrt(data), label=<span class="st">"NumPy puro"</span>)

<span class="fn">print</span>(f<span class="st">"Speedup B vs A: {t_a/t_b:.1f}x"</span>)
<span class="fn">print</span>(f<span class="st">"Speedup B vs NumPy: {t_np/t_b:.1f}x"</span>)</code></pre>

    <h4>¬øPor qu√© mediana y no media?</h4>
    <p>Porque los outliers de tiempo alto (causados por garbage collection, interrupciones del OS, context switches) distorsionan la media. La mediana te dice "en la mitad de los casos tard√≥ menos que X" ‚Äî mucho m√°s representativo del rendimiento real.</p>

    <h4>¬øPor qu√© <code>time.perf_counter()</code> y no <code>time.time()</code>?</h4>
    <p><code>perf_counter</code> usa el reloj monot√≥nico de mayor resoluci√≥n disponible en el sistema (nanosegundos). <code>time.time()</code> puede tener resoluci√≥n de milisegundos y es afectado por ajustes del reloj del sistema.</p>

    <!-- ============ MEDIR COMPILACI√ìN ============ -->
    <h3>‚è±Ô∏è Medir el tiempo de compilaci√≥n por separado</h3>

    <p>A veces necesitas saber cu√°nto tarda Numba en compilar (para decidir si usar <code>cache=True</code>):</p>

<pre><code><span class="kw">def</span> <span class="fn">benchmark_compilation</span>(func, *args):
    <span class="cm">"""Mide el tiempo de compilaci√≥n vs ejecuci√≥n por separado."""</span>

    <span class="cm"># Forzar recompilaci√≥n</span>
    func.recompile()  <span class="cm"># Limpia todas las especializaciones</span>

    <span class="cm"># Medir primera llamada (compilaci√≥n + ejecuci√≥n)</span>
    start = time.perf_counter()
    func(*args)
    first_call = time.perf_counter() - start

    <span class="cm"># Medir segunda llamada (solo ejecuci√≥n)</span>
    start = time.perf_counter()
    func(*args)
    second_call = time.perf_counter() - start

    compilation = first_call - second_call

    <span class="fn">print</span>(f<span class="st">"Compilaci√≥n:  {compilation*1000:.1f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"Ejecuci√≥n:    {second_call*1000:.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"Ratio:        compilaci√≥n es {compilation/second_call:.0f}x la ejecuci√≥n"</span>)

    <span class="cm"># Si la ratio es > 1000, cache=True es casi obligatorio</span>
    <span class="kw">if</span> compilation / max(second_call, <span class="num">1e-9</span>) > <span class="num">1000</span>:
        <span class="fn">print</span>(<span class="st">"  ‚Üí RECOMENDACI√ìN: usa cache=True"</span>)</code></pre>

    <!-- ============ SCALING TEST ============ -->
    <h3>üìà Scaling test: ¬øtu optimizaci√≥n escala?</h3>

    <p>Un benchmark con un solo tama√±o de datos es incompleto. Las optimizaciones se comportan diferente seg√∫n la escala:</p>

<pre><code><span class="kw">def</span> <span class="fn">scaling_test</span>(funcs, sizes, labels=<span class="num">None</span>, gen_data=<span class="num">None</span>):
    <span class="cm">"""Compara funciones en m√∫ltiples tama√±os de datos.
    Revela si una optimizaci√≥n escala o tiene overhead fijo."""</span>

    <span class="kw">if</span> gen_data <span class="kw">is</span> <span class="num">None</span>:
        gen_data = <span class="kw">lambda</span> n: np.random.rand(n)
    <span class="kw">if</span> labels <span class="kw">is</span> <span class="num">None</span>:
        labels = [f.__name__ <span class="kw">for</span> f <span class="kw">in</span> funcs]

    <span class="cm"># Warmup todas las funciones con el tama√±o m√°s peque√±o</span>
    small_data = gen_data(sizes[<span class="num">0</span>])
    <span class="kw">for</span> func <span class="kw">in</span> funcs:
        func(small_data)

    <span class="fn">print</span>(f<span class="st">"\n{'Tama√±o':>12}"</span>, end=<span class="st">""</span>)
    <span class="kw">for</span> label <span class="kw">in</span> labels:
        <span class="fn">print</span>(f<span class="st">"{label:>15}"</span>, end=<span class="st">""</span>)
    <span class="fn">print</span>(f<span class="st">"{'Speedup':>12}"</span>)
    <span class="fn">print</span>(<span class="st">"‚îÄ"</span> * (<span class="num">12</span> + <span class="num">15</span> * <span class="fn">len</span>(funcs) + <span class="num">12</span>))

    <span class="kw">for</span> size <span class="kw">in</span> sizes:
        data = gen_data(size)
        times = []
        <span class="kw">for</span> func <span class="kw">in</span> funcs:
            t = []
            <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">20</span>):
                start = time.perf_counter()
                func(data)
                t.append(time.perf_counter() - start)
            times.append(np.median(t))

        <span class="fn">print</span>(f<span class="st">"{size:>12,}"</span>, end=<span class="st">""</span>)
        <span class="kw">for</span> t <span class="kw">in</span> times:
            <span class="fn">print</span>(f<span class="st">"{t*1000:>14.3f}ms"</span>, end=<span class="st">""</span>)
        speedup = times[<span class="num">0</span>] / times[<span class="num">-1</span>] <span class="kw">if</span> times[<span class="num">-1</span>] > <span class="num">0</span> <span class="kw">else</span> float(<span class="st">'inf'</span>)
        <span class="fn">print</span>(f<span class="st">"{speedup:>11.1f}x"</span>)

<span class="cm"># Uso:</span>
scaling_test(
    funcs=[version_a, version_b],
    sizes=[<span class="num">1_000</span>, <span class="num">10_000</span>, <span class="num">100_000</span>, <span class="num">1_000_000</span>, <span class="num">10_000_000</span>],
    labels=[<span class="st">"njit"</span>, <span class="st">"parallel+fast"</span>]
)

<span class="cm"># Output t√≠pico:</span>
<span class="cm">#      Tama√±o           njit  parallel+fast     Speedup</span>
<span class="cm"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span class="cm">#        1,000        0.002ms        0.045ms       0.0x  ‚Üê parallel PIERDE en datos chicos</span>
<span class="cm">#       10,000        0.012ms        0.048ms       0.3x  ‚Üê todav√≠a pierde</span>
<span class="cm">#      100,000        0.108ms        0.065ms       1.7x  ‚Üê empieza a ganar</span>
<span class="cm">#    1,000,000        1.052ms        0.342ms       3.1x  ‚Üê gana claramente</span>
<span class="cm">#   10,000,000       10.891ms        2.187ms       5.0x  ‚Üê domina</span></code></pre>

    <div class="info">
      Este test revela algo crucial: <code>parallel=True</code> tiene un overhead fijo de ~0.04ms por crear el thread pool. Con datos peque√±os (<10K), ese overhead domina y la versi√≥n paralela es M√ÅS LENTA. Con datos grandes, el overhead es insignificante y la paralelizaci√≥n domina. <strong>Sin un scaling test, nunca ver√≠as esto.</strong>
    </div>

    <!-- ============ THROUGHPUT ============ -->
    <h3>üìä Medir throughput, no solo tiempo</h3>

    <p>El tiempo absoluto no te dice si tu c√≥digo est√° cerca del l√≠mite te√≥rico. El throughput (GB/s procesados) s√≠:</p>

<pre><code><span class="kw">def</span> <span class="fn">benchmark_throughput</span>(func, data, n_runs=<span class="num">50</span>, label=<span class="st">""</span>):
    <span class="cm">"""Mide throughput en GB/s para saber si est√°s cerca del l√≠mite de memoria."""</span>

    <span class="cm"># Warmup</span>
    func(data)
    func(data)

    <span class="cm"># Medir</span>
    times = []
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_runs):
        start = time.perf_counter()
        func(data)
        times.append(time.perf_counter() - start)

    median_time = np.median(times)
    data_bytes = data.nbytes
    throughput_gbs = (data_bytes / median_time) / <span class="num">1e9</span>

    <span class="fn">print</span>(f<span class="st">"{label:30s} | {median_time*1000:8.3f} ms | {throughput_gbs:6.1f} GB/s"</span>)

    <span class="cm"># Referencia: bandwidth de RAM t√≠pica</span>
    <span class="cm"># DDR4-3200: ~25 GB/s por canal, ~50 GB/s dual channel</span>
    <span class="cm"># DDR5-5600: ~45 GB/s por canal, ~90 GB/s dual channel</span>
    <span class="cm"># Si tu throughput est√° cerca de estos n√∫meros, est√°s</span>
    <span class="cm"># MEMORY BOUND y optimizar la CPU no ayudar√° m√°s.</span>
    <span class="cm"># La √∫nica forma de ir m√°s r√°pido es reducir los datos</span>
    <span class="cm"># que necesitas leer (mejor layout, menos temporales, etc.)</span>

<span class="cm"># Uso:</span>
data = np.random.rand(<span class="num">50_000_000</span>)  <span class="cm"># ~400MB</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">just_sum</span>(x):
    <span class="kw">return</span> np.sum(x)

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">par_sum</span>(x):
    <span class="kw">return</span> np.sum(x)

benchmark_throughput(just_sum, data, label=<span class="st">"np.sum @njit"</span>)
benchmark_throughput(par_sum, data, label=<span class="st">"np.sum @njit parallel"</span>)
benchmark_throughput(np.sum, data, label=<span class="st">"np.sum NumPy puro"</span>)

<span class="cm"># Output t√≠pico:</span>
<span class="cm"># np.sum @njit                  |   17.234 ms |  23.2 GB/s</span>
<span class="cm"># np.sum @njit parallel         |    4.891 ms |  81.8 GB/s  ‚Üê cerca del l√≠mite de RAM</span>
<span class="cm"># np.sum NumPy puro             |   18.103 ms |  22.1 GB/s</span>
<span class="cm">#</span>
<span class="cm"># La versi√≥n parallel a 81.8 GB/s est√° CERCA del l√≠mite de DDR5 (~90 GB/s)</span>
<span class="cm"># No va a ir m√°s r√°pido sin importar qu√© hagas en c√≥digo.</span>
<span class="cm"># ¬°Est√°s memory-bound! La CPU est√° esperando datos de RAM.</span></code></pre>

    <div class="perf">
      <strong>Si tu throughput se acerca al bandwidth de tu RAM, est√°s memory-bound.</strong> Ninguna optimizaci√≥n de CPU (fastmath, SVML, mejor algoritmo) te va a ayudar. La √∫nica salida es reducir la cantidad de datos que lees: fusionar operaciones (un solo pass en vez de m√∫ltiples), usar tipos m√°s peque√±os (float32 en vez de float64 duplica tu throughput efectivo), o reestructurar datos (SoA).
    </div>

    <!-- ============ DIAGNOSTICS ============ -->
    <h3>üîç Verificar qu√© hizo Numba realmente</h3>

    <p>No asumas que Numba hizo lo que esperabas. <strong>Verifica.</strong></p>

<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">mi_funcion</span>(x):
    n = x.shape[<span class="num">0</span>]
    a = np.sin(x)
    b = np.cos(a * a)
    acc = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(n - <span class="num">2</span>):
        <span class="kw">for</span> j <span class="kw">in</span> prange(n - <span class="num">1</span>):
            acc += b[i] + b[j + <span class="num">1</span>]
    <span class="kw">return</span> acc

<span class="cm"># Forzar compilaci√≥n</span>
mi_funcion(np.arange(<span class="num">10</span>, dtype=np.float64))

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 1: inspect_types() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Muestra qu√© tipo infiri√≥ Numba para CADA variable</span>
mi_funcion.inspect_types()
<span class="cm"># Busca sorpresas: ¬øuna variable es float64 cuando esperabas int?</span>
<span class="cm"># ¬øUn array es 'A' (any layout) cuando deber√≠a ser 'C'?</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 2: parallel_diagnostics() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Muestra qu√© se paraleliz√≥, qu√© se fusion√≥, y qu√© NO</span>
mi_funcion.parallel_diagnostics(level=<span class="num">4</span>)
<span class="cm"># Nivel 1: resumen b√°sico</span>
<span class="cm"># Nivel 2: + info de fusi√≥n de loops</span>
<span class="cm"># Nivel 3: + estructura antes/despu√©s de optimizaci√≥n</span>
<span class="cm"># Nivel 4: + hoisting de c√≥digo + instrucciones</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 3: inspect_llvm() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Para expertos: el c√≥digo LLVM IR generado</span>
llvm_ir = mi_funcion.inspect_llvm(mi_funcion.signatures[<span class="num">0</span>])
<span class="cm"># Buscar: "vector.body" ‚Üí LLVM vectoriz√≥ el loop (SIMD)</span>
<span class="cm"># Buscar: "llvm.sqrt.v4f64" ‚Üí est√° usando SIMD de 4 doubles</span>
<span class="cm"># Buscar: "svml" ‚Üí est√° usando Intel SVML</span>
<span class="kw">if</span> <span class="st">'svml'</span> <span class="kw">in</span> llvm_ir.lower():
    <span class="fn">print</span>(<span class="st">"‚úÖ SVML activo"</span>)
<span class="kw">else</span>:
    <span class="fn">print</span>(<span class="st">"‚ö†Ô∏è SVML NO detectado ‚Äî instala intel-cmplr-lib-rt"</span>)

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 4: inspect_asm() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Para ultra-expertos: el assembly nativo generado</span>
asm = mi_funcion.inspect_asm(mi_funcion.signatures[<span class="num">0</span>])
<span class="cm"># Buscar: "vmulpd" / "vaddpd" ‚Üí instrucciones AVX (256-bit SIMD)</span>
<span class="cm"># Buscar: "zmm" ‚Üí instrucciones AVX-512 (512-bit SIMD)</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 5: debug de vectorizaci√≥n LLVM ‚îÄ‚îÄ‚îÄ</span>
<span class="kw">import</span> llvmlite.binding <span class="kw">as</span> llvm
llvm.set_option(<span class="st">''</span>, <span class="st">'--debug-only=loop-vectorize'</span>)
<span class="cm"># NOTA: requiere LLVM con assertions (build de desarrollo)</span>
<span class="cm"># Mensajes comunes:</span>
<span class="cm"># "LV: Vectorization is possible but not beneficial" ‚Üí el loop es muy corto</span>
<span class="cm"># "LV: Can't vectorize due to memory conflicts" ‚Üí acceso no contiguo</span>
<span class="cm"># "LV: Not vectorizing: loop did not meet vectorization requirements" ‚Üí dependencia</span></code></pre>

    <!-- ============ COMPARAR CONTRA NUMPY ============ -->
    <h3>‚öñÔ∏è Comparar contra NumPy de forma justa</h3>

<pre><code><span class="cm"># ERROR MUY COM√öN: comparar operaciones diferentes</span>

data = np.random.rand(<span class="num">1_000_000</span>)

<span class="cm"># Esto NO es una comparaci√≥n justa:</span>
<span class="cm"># NumPy:</span>
result_np = np.sum(np.sqrt(data))   <span class="cm"># crea array temporal de sqrt</span>

<span class="cm"># Numba:</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">fused</span>(x):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(x)):
        total += np.sqrt(x[i])  <span class="cm"># sin array temporal</span>
    <span class="kw">return</span> total
<span class="cm"># La versi√≥n Numba es m√°s r√°pida EN PARTE porque evita el temporal,</span>
<span class="cm"># no solo por la compilaci√≥n JIT.</span>

<span class="cm"># COMPARACI√ìN JUSTA: misma operaci√≥n, mismo patr√≥n de memoria</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">numba_same_as_numpy</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))  <span class="cm"># misma operaci√≥n que NumPy</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">numba_fused</span>(x):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(x)):
        total += np.sqrt(x[i])
    <span class="kw">return</span> total

<span class="cm"># Ahora puedes decir:</span>
<span class="cm"># "Numba sin fusi√≥n (misma operaci√≥n que NumPy): 1.2x m√°s r√°pido"</span>
<span class="cm"># "Numba CON fusi√≥n de loops: 3.5x m√°s r√°pido"</span>
<span class="cm"># "De ese 3.5x, 1.2x viene de JIT y 2.9x viene de eliminar temporales"</span></code></pre>

    <!-- ============ FLOAT32 ============ -->
    <h3>üî¢ Truco nuclear: float32 en vez de float64</h3>

<pre><code><span class="cm"># Cambiar de float64 a float32 tiene TRES beneficios simult√°neos:</span>
<span class="cm"># 1. La mitad de bytes ‚Üí el doble de datos caben en cache</span>
<span class="cm"># 2. La mitad de bytes ‚Üí el doble de throughput de memoria</span>
<span class="cm"># 3. SIMD procesa el doble de elementos por instrucci√≥n</span>
<span class="cm">#    (8 float32 vs 4 float64 en AVX-256)</span>

data64 = np.random.rand(<span class="num">10_000_000</span>).astype(np.float64)  <span class="cm"># 80 MB</span>
data32 = data64.astype(np.float32)                        <span class="cm"># 40 MB</span>

<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process</span>(x):
    result = np.empty_like(x)
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(x)):
        result[i] = np.sin(x[i]) ** <span class="num">2</span> + np.cos(x[i]) ** <span class="num">2</span>
    <span class="kw">return</span> result

benchmark(process, data64, label=<span class="st">"float64 (80MB)"</span>)
benchmark(process, data32, label=<span class="st">"float32 (40MB)"</span>)

<span class="cm"># Resultado t√≠pico:</span>
<span class="cm"># float64:  12.3ms</span>
<span class="cm"># float32:   4.1ms ‚Üí 3x m√°s r√°pido con ~7 d√≠gitos de precisi√≥n (vs ~15)</span>

<span class="cm"># ¬øCu√°ndo es aceptable float32?</span>
<span class="cm"># ‚úÖ Machine Learning (pesos, activaciones, gradientes)</span>
<span class="cm"># ‚úÖ Procesamiento de se√±ales e im√°genes</span>
<span class="cm"># ‚úÖ Simulaciones donde 7 d√≠gitos bastan</span>
<span class="cm"># ‚úÖ Visualizaci√≥n y gr√°ficos</span>
<span class="cm"># ‚ùå C√°lculos financieros precisos</span>
<span class="cm"># ‚ùå √Ålgebra lineal con matrices mal condicionadas</span>
<span class="cm"># ‚ùå Acumulaci√≥n de sumas muy largas (error se acumula)</span></code></pre>

    <!-- ============ CHECKLIST FINAL ============ -->
    <h3>üìã Checklist de benchmarking</h3>
    <table>
      <tr><th>#</th><th>Verificar</th><th>Si no lo haces...</th></tr>
      <tr><td>1</td><td>¬øHice warmup antes de medir?</td><td>Mides compilaci√≥n, no ejecuci√≥n</td></tr>
      <tr><td>2</td><td>¬øUso <code>time.perf_counter()</code>?</td><td>Resoluci√≥n insuficiente para funciones r√°pidas</td></tr>
      <tr><td>3</td><td>¬øM√∫ltiples ejecuciones + mediana?</td><td>GC y OS noise distorsionan un solo run</td></tr>
      <tr><td>4</td><td>¬øProb√© con m√∫ltiples tama√±os de datos?</td><td>Tu "optimizaci√≥n" puede ser m√°s lenta para datos reales</td></tr>
      <tr><td>5</td><td>¬øVerifiqu√© throughput en GB/s?</td><td>Podr√≠as estar optimizando CPU cuando est√°s memory-bound</td></tr>
      <tr><td>6</td><td>¬øCompar√© operaciones equivalentes?</td><td>Speedup falso por comparar cosas diferentes</td></tr>
      <tr><td>7</td><td>¬øRevis√© <code>parallel_diagnostics()</code>?</td><td>parallel=True podr√≠a no estar haciendo nada</td></tr>
      <tr><td>8</td><td>¬øVerifiqu√© que SVML est√° activo?</td><td>Podr√≠as estar a 3x del rendimiento √≥ptimo sin saberlo</td></tr>
      <tr><td>9</td><td>¬øProb√© float32 vs float64?</td><td>Posible 2-3x gratis si la precisi√≥n no es cr√≠tica</td></tr>
    </table>

  </div>
</div>
        
<!--
  ============================================================
  M√ìDULO EXTRA: Gu√≠a Exhaustiva de Decoradores
  ============================================================
  INSTRUCCIONES: Copia todo este bloque y p√©galo en tu gu√≠a
  JUSTO ANTES de la l√≠nea:
      <!-- ==================== M√ìDULO 10 ==================== -->
      <a href="#m-decorators">‚òÖ Gu√≠a Exhaustiva: ¬øQu√© decorador uso?</a>

<!-- ==================== M√ìDULO DECORADORES ==================== -->
<div class="module" id="m-decorators">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">‚òÖ</span> Gu√≠a Exhaustiva: ¬øQu√© decorador uso y cu√°ndo?</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Esta secci√≥n resuelve la pregunta m√°s importante al usar Numba: <strong>¬øqu√© decorador necesito para MI caso?</strong> Cada decorador existe para resolver un problema distinto. Usar el incorrecto no solo no ayuda ‚Äî puede hacer tu c√≥digo m√°s lento, incorrecto, o directamente roto.</p>

    <!-- ============ MAPA DE DECISI√ìN ============ -->
    <h3>üó∫Ô∏è Mapa de decisi√≥n r√°pido</h3>
    <p>Antes de leer todo, usa esta gu√≠a r√°pida:</p>
    <table>
      <tr><th>Tu situaci√≥n</th><th>Decorador correcto</th></tr>
      <tr><td>Tengo una funci√≥n con loops num√©ricos y quiero que vaya r√°pido</td><td><code>@njit</code></td></tr>
      <tr><td>Tengo una operaci√≥n escalar que quiero aplicar a todo un array</td><td><code>@vectorize</code></td></tr>
      <tr><td>Tengo una operaci√≥n que recibe/devuelve sub-arrays (no escalares)</td><td><code>@guvectorize</code></td></tr>
      <tr><td>Tengo un c√°lculo donde cada elemento depende de sus vecinos</td><td><code>@stencil</code></td></tr>
      <tr><td>Necesito una clase con estado y m√©todos r√°pidos</td><td><code>@jitclass</code></td></tr>
      <tr><td>Necesito que una funci√≥n Python sea llamable desde C/Fortran</td><td><code>@cfunc</code></td></tr>
      <tr><td>Quiero que una funci√≥n de terceros funcione dentro de @njit</td><td><code>@overload</code></td></tr>
    </table>

    <!-- ============================================ -->
    <!-- ============ @njit / @jit ================= -->
    <!-- ============================================ -->
    <h3 style="color: #58a6ff; border-color: #58a6ff;">1. <code>@njit</code> / <code>@jit</code> ‚Äî La navaja suiza</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Toma tu funci√≥n Python, analiza el bytecode, infiere los tipos de todas las variables, y genera c√≥digo m√°quina nativo v√≠a LLVM. Es como si un compilador de C tomara tu Python y lo convirtiera en un ejecutable. Desde Numba 0.59, <code>@jit</code> y <code>@njit</code> son id√©nticos.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Funciones con loops num√©ricos sobre arrays NumPy</li>
      <li>C√°lculos matem√°ticos puros (sin I/O, sin strings complejos)</li>
      <li>Funciones auxiliares peque√±as que ser√°n llamadas desde otro c√≥digo Numba</li>
      <li>Cualquier funci√≥n donde "escribir√≠as un loop en C"</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Funciones que hacen I/O (leer archivos, requests HTTP, print extensivo)</li>
      <li>C√≥digo que manipula strings complejos, listas Python, o DataFrames de Pandas</li>
      <li>Funciones que ya son 100% operaciones vectorizadas de NumPy (Numba no las acelera mucho m√°s)</li>
      <li>Scripts cortos que se ejecutan una sola vez (la compilaci√≥n tarda m√°s que la ejecuci√≥n)</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Filtro de part√≠culas en una simulaci√≥n f√≠sica</strong></p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">actualizar_particulas</span>(posiciones, velocidades, masas, dt, gravedad):
    <span class="cm">"""Simulaci√≥n N-body simplificada: actualiza posiciones por un paso dt."""</span>
    n = posiciones.shape[<span class="num">0</span>]
    fuerzas = np.zeros_like(posiciones)

    <span class="cm"># Calcular fuerzas gravitacionales entre pares</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(i + <span class="num">1</span>, n):
            dx = posiciones[j, <span class="num">0</span>] - posiciones[i, <span class="num">0</span>]
            dy = posiciones[j, <span class="num">1</span>] - posiciones[i, <span class="num">1</span>]
            dist_sq = dx * dx + dy * dy + <span class="num">1e-10</span>  <span class="cm"># evitar div/0</span>
            dist = np.sqrt(dist_sq)
            fuerza = gravedad * masas[i] * masas[j] / dist_sq
            fx = fuerza * dx / dist
            fy = fuerza * dy / dist
            fuerzas[i, <span class="num">0</span>] += fx
            fuerzas[i, <span class="num">1</span>] += fy
            fuerzas[j, <span class="num">0</span>] -= fx
            fuerzas[j, <span class="num">1</span>] -= fy

    <span class="cm"># Integrar: actualizar velocidades y posiciones</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        velocidades[i, <span class="num">0</span>] += (fuerzas[i, <span class="num">0</span>] / masas[i]) * dt
        velocidades[i, <span class="num">1</span>] += (fuerzas[i, <span class="num">1</span>] / masas[i]) * dt
        posiciones[i, <span class="num">0</span>] += velocidades[i, <span class="num">0</span>] * dt
        posiciones[i, <span class="num">1</span>] += velocidades[i, <span class="num">1</span>] * dt

<span class="cm"># Uso</span>
n_particulas = <span class="num">1000</span>
pos = np.random.rand(n_particulas, <span class="num">2</span>) * <span class="num">100</span>
vel = np.zeros((n_particulas, <span class="num">2</span>))
masas = np.random.rand(n_particulas) * <span class="num">10</span> + <span class="num">1</span>
actualizar_particulas(pos, vel, masas, <span class="num">0.01</span>, <span class="num">9.8</span>)
<span class="cm"># Sin Numba: ~15 segundos | Con @njit: ~0.02 segundos</span></code></pre>

    <p><strong>Ejemplo 2: B√∫squeda en historial de precios (datos financieros)</strong></p>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">max_drawdown</span>(precios):
    <span class="cm">"""Calcula la m√°xima ca√≠da desde un pico en una serie de precios.
    M√©trica fundamental en finanzas para medir riesgo."""</span>
    n = <span class="fn">len</span>(precios)
    pico = precios[<span class="num">0</span>]
    max_dd = <span class="num">0.0</span>

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, n):
        <span class="kw">if</span> precios[i] > pico:
            pico = precios[i]
        dd = (pico - precios[i]) / pico
        <span class="kw">if</span> dd > max_dd:
            max_dd = dd

    <span class="kw">return</span> max_dd

<span class="cm"># 10 a√±os de datos diarios</span>
precios = np.cumsum(np.random.randn(<span class="num">2520</span>)) + <span class="num">100</span>
resultado = max_drawdown(precios)  <span class="cm"># instant√°neo</span></code></pre>

    <p><strong>Ejemplo 3: Procesamiento de se√±al (correlaci√≥n cruzada manual)</strong></p>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">cross_correlation</span>(signal, template):
    <span class="cm">"""Encuentra d√≥nde un template aparece en una se√±al larga."""</span>
    n_signal = <span class="fn">len</span>(signal)
    n_template = <span class="fn">len</span>(template)
    n_output = n_signal - n_template + <span class="num">1</span>
    result = np.empty(n_output)

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_output):
        total = <span class="num">0.0</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(n_template):
            total += signal[i + j] * template[j]
        result[i] = total

    <span class="kw">return</span> result</code></pre>

    <h4>üö´ Errores comunes con @njit</h4>
<pre><code><span class="cm"># ERROR 1: Usar listas Python normales</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">malo_lista</span>(n):
    resultado = []                  <span class="cm"># ‚ùå Lista Python, NO funciona</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        resultado.append(i * <span class="num">2</span>)
    <span class="kw">return</span> resultado

<span class="cm"># SOLUCI√ìN: Usa array NumPy o numba.typed.List</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">bueno_array</span>(n):
    resultado = np.empty(n, dtype=np.int64)  <span class="cm"># ‚úÖ Pre-aloca array</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        resultado[i] = i * <span class="num">2</span>
    <span class="kw">return</span> resultado

<span class="cm"># ERROR 2: Llamar funciones no soportadas</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">malo_pandas</span>(df):
    <span class="kw">return</span> df.groupby(<span class="st">'col'</span>).mean()  <span class="cm"># ‚ùå Pandas NO funciona en Numba</span>

<span class="cm"># ERROR 3: Usar @njit en funciones que YA son r√°pidas con NumPy</span>
<span class="dec">@njit</span>   <span class="cm"># ‚ùå Innecesario: NumPy ya hace esto en C internamente</span>
<span class="kw">def</span> <span class="fn">innecesario</span>(a, b):
    <span class="kw">return</span> np.dot(a, b)  <span class="cm"># np.dot ya llama a BLAS optimizado</span>

<span class="cm"># ERROR 4: Olvidar decorar funciones auxiliares</span>
<span class="kw">def</span> <span class="fn">helper</span>(x):        <span class="cm"># ‚ùå Sin @njit, Numba sale al int√©rprete</span>
    <span class="kw">return</span> x ** <span class="num">2</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">principal</span>(arr):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(arr)):
        arr[i] = helper(arr[i])  <span class="cm"># Lent√≠simo: sale de nativo a Python en cada iteraci√≥n</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @vectorize =================== -->
    <!-- ============================================ -->
    <h3 style="color: #3fb950; border-color: #3fb950;">2. <code>@vectorize</code> ‚Äî F√°brica de ufuncs escalares</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>T√∫ escribes una funci√≥n que opera sobre <strong>un solo par de valores escalares</strong>. Numba genera autom√°ticamente el loop que aplica esa operaci√≥n a arrays completos, con soporte autom√°tico de broadcasting, reduce, accumulate ‚Äî como un ufunc nativo de NumPy escrito en C, pero sin escribir C.</p>

    <h4>‚úÖ Cu√°ndo usarlo (en vez de @njit)</h4>
    <ul>
      <li>Tu operaci√≥n es naturalmente <strong>escalar ‚Üí escalar</strong> (recibe n√∫meros, devuelve un n√∫mero)</li>
      <li>Necesitas <strong>broadcasting</strong> autom√°tico entre arrays de distintas formas</li>
      <li>Necesitas <strong>reduce</strong> o <strong>accumulate</strong> sobre tu operaci√≥n</li>
      <li>Quieres paralelizar f√°cilmente con <code>target='parallel'</code></li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Tu funci√≥n necesita ver m√°s de un elemento a la vez (usa @guvectorize)</li>
      <li>Tu funci√≥n tiene estado interno o acumuladores (usa @njit con loop)</li>
      <li>Solo necesitas aplicar una operaci√≥n a un array sin broadcasting/reduce (un @njit simple es m√°s directo)</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: C√°lculo de impuestos con tramos progresivos</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> vectorize, float64

<span class="dec">@vectorize</span>([float64(float64)])
<span class="kw">def</span> <span class="fn">calcular_impuesto</span>(ingreso):
    <span class="cm">"""Impuesto progresivo: aplica a CADA empleado de un array."""</span>
    <span class="kw">if</span> ingreso <= <span class="num">12000</span>:
        <span class="kw">return</span> ingreso * <span class="num">0.0</span>
    <span class="kw">elif</span> ingreso <= <span class="num">30000</span>:
        <span class="kw">return</span> (ingreso - <span class="num">12000</span>) * <span class="num">0.15</span>
    <span class="kw">elif</span> ingreso <= <span class="num">60000</span>:
        <span class="kw">return</span> <span class="num">2700</span> + (ingreso - <span class="num">30000</span>) * <span class="num">0.25</span>
    <span class="kw">else</span>:
        <span class="kw">return</span> <span class="num">10200</span> + (ingreso - <span class="num">60000</span>) * <span class="num">0.35</span>

<span class="cm"># Funciona sobre arrays completos autom√°ticamente</span>
salarios = np.array([<span class="num">8000</span>, <span class="num">25000</span>, <span class="num">45000</span>, <span class="num">120000</span>])
impuestos = calcular_impuesto(salarios)
<span class="cm"># ‚Üí array([0., 1950., 6450., 31200.])</span>

<span class="cm"># ¬°Broadcasting gratis! Funciona con matrices tambi√©n</span>
salarios_departamentos = np.random.rand(<span class="num">50</span>, <span class="num">200</span>) * <span class="num">80000</span>
impuestos_todos = calcular_impuesto(salarios_departamentos)  <span class="cm"># shape (50, 200)</span></code></pre>

    <p><strong>Ejemplo 2: Funci√≥n de activaci√≥n personalizada para ML</strong></p>
<pre><code><span class="kw">import</span> math

<span class="dec">@vectorize</span>([float64(float64)], target=<span class="st">'parallel'</span>)
<span class="kw">def</span> <span class="fn">swish</span>(x):
    <span class="cm">"""Swish activation: x * sigmoid(x). Mejor que ReLU en muchos casos."""</span>
    <span class="kw">return</span> x / (<span class="num">1.0</span> + math.exp(-x))

<span class="cm"># Aplicar a toda una capa de red neuronal (millones de valores)</span>
activaciones = np.random.randn(<span class="num">512</span>, <span class="num">1024</span>)
resultado = swish(activaciones)  <span class="cm"># Paralelo en todos los cores</span></code></pre>

    <p><strong>Ejemplo 3: Distancia con reduce (¬°imposible con @njit!)</strong></p>
<pre><code><span class="dec">@vectorize</span>([float64(float64, float64)])
<span class="kw">def</span> <span class="fn">max_custom</span>(a, b):
    <span class="cm">"""Un max que funciona como ufunc con reduce."""</span>
    <span class="kw">if</span> a >= b:
        <span class="kw">return</span> a
    <span class="kw">return</span> b

data = np.array([<span class="num">3.0</span>, <span class="num">7.0</span>, <span class="num">2.0</span>, <span class="num">9.0</span>, <span class="num">1.0</span>])
max_custom.reduce(data)  <span class="cm"># ‚Üí 9.0 (reduce funciona autom√°ticamente)</span>

<span class="cm"># Con una matriz: m√°ximo por filas o columnas</span>
matrix = np.random.rand(<span class="num">100</span>, <span class="num">50</span>)
max_custom.reduce(matrix, axis=<span class="num">1</span>)     <span class="cm"># m√°ximo de cada fila</span>
max_custom.accumulate(matrix, axis=<span class="num">0</span>) <span class="cm"># m√°ximo acumulado por columnas</span></code></pre>

    <h4>üö´ Errores comunes con @vectorize</h4>
<pre><code><span class="cm"># ERROR 1: Intentar acceder a √≠ndices del array</span>
<span class="dec">@vectorize</span>([float64(float64[:])])  <span class="cm"># ‚ùå No puedes pasar arrays</span>
<span class="kw">def</span> <span class="fn">malo</span>(arr):
    <span class="kw">return</span> arr[<span class="num">0</span>] + arr[<span class="num">1</span>]
<span class="cm"># vectorize solo recibe ESCALARES. Para arrays usa @guvectorize</span>

<span class="cm"># ERROR 2: Orden incorrecto de signatures (tipos m√°s espec√≠ficos primero)</span>
<span class="dec">@vectorize</span>([
    float64(float64, float64),  <span class="cm"># ‚ùå float64 antes que int ‚Üí int nunca se usa</span>
    int64(int64, int64),
])
<span class="kw">def</span> <span class="fn">malo_orden</span>(a, b):
    <span class="kw">return</span> a + b

<span class="cm"># CORRECTO: tipos m√°s espec√≠ficos/peque√±os primero</span>
<span class="dec">@vectorize</span>([
    int32(int32, int32),     <span class="cm"># ‚úÖ M√°s espec√≠fico primero</span>
    int64(int64, int64),
    float32(float32, float32),
    float64(float64, float64),
])
<span class="kw">def</span> <span class="fn">bien_orden</span>(a, b):
    <span class="kw">return</span> a + b

<span class="cm"># ERROR 3: Usar target='parallel' con datos muy peque√±os</span>
<span class="dec">@vectorize</span>([float64(float64)], target=<span class="st">'parallel'</span>)
<span class="kw">def</span> <span class="fn">doble</span>(x):
    <span class="kw">return</span> x * <span class="num">2</span>

doble(np.array([<span class="num">1.0</span>, <span class="num">2.0</span>, <span class="num">3.0</span>]))  <span class="cm"># ‚ùå 3 elementos: el overhead de threading</span>
                                      <span class="cm">#    es mayor que el c√°lculo en s√≠</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @guvectorize ================= -->
    <!-- ============================================ -->
    <h3 style="color: #d2a8ff; border-color: #d2a8ff;">3. <code>@guvectorize</code> ‚Äî Ufuncs sobre sub-arrays</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Como <code>@vectorize</code>, pero tu funci√≥n recibe <strong>porciones de arrays</strong> (filas, vectores, submatrices) en lugar de escalares. T√∫ defines un "layout" simb√≥lico que dice qu√© dimensiones tiene cada argumento. NumPy se encarga del broadcasting y de iterar sobre las dimensiones externas.</p>

    <h4>‚úÖ Cu√°ndo usarlo (en vez de @njit o @vectorize)</h4>
    <ul>
      <li>Tu operaci√≥n necesita ver <strong>una fila/vector/subarray completo</strong> para producir un resultado</li>
      <li>Quieres aplicar la misma operaci√≥n a cada fila de una matriz, cada frame de un video, etc.</li>
      <li>Necesitas broadcasting autom√°tico entre inputs de diferentes dimensiones</li>
      <li>Est√°s implementando algo tipo: media por fila, normalizaci√≥n por vector, convoluci√≥n 1D por canal</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Tu operaci√≥n es escalar ‚Üí escalar (usa @vectorize, es m√°s simple)</li>
      <li>Tienes un solo array y un loop secuencial con dependencias (usa @njit)</li>
      <li>No necesitas broadcasting ‚Äî un @njit simple es m√°s legible</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Normalizaci√≥n por filas (muy com√∫n en ML)</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> guvectorize, float64

<span class="dec">@guvectorize</span>(
    [(float64[:], float64[:])],
    <span class="st">'(n)->(n)'</span>  <span class="cm"># recibe vector de n elementos, devuelve vector de n</span>
)
<span class="kw">def</span> <span class="fn">normalize_row</span>(row, result):
    <span class="cm">"""Normaliza un vector a norma unitaria (L2)."""</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(row.shape[<span class="num">0</span>]):
        total += row[i] ** <span class="num">2</span>
    norm = np.sqrt(total)
    <span class="kw">if</span> norm > <span class="num">0</span>:
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(row.shape[<span class="num">0</span>]):
            result[i] = row[i] / norm
    <span class="kw">else</span>:
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(row.shape[<span class="num">0</span>]):
            result[i] = <span class="num">0.0</span>

<span class="cm"># Aplica autom√°ticamente a CADA FILA de una matriz</span>
embeddings = np.random.rand(<span class="num">10000</span>, <span class="num">768</span>)  <span class="cm"># 10K vectores de 768 dimensiones</span>
normalized = normalize_row(embeddings)    <span class="cm"># shape (10000, 768), cada fila normalizada</span></code></pre>

    <p><strong>Ejemplo 2: Media m√≥vil por serie temporal (datos financieros)</strong></p>
<pre><code><span class="dec">@guvectorize</span>(
    [(float64[:], float64[:], float64[:])],
    <span class="st">'(n),(m)->(n)'</span>  <span class="cm"># se√±al de n puntos, ventana de m pesos ‚Üí n resultados</span>
)
<span class="kw">def</span> <span class="fn">weighted_moving_avg</span>(signal, weights, result):
    <span class="cm">"""Media m√≥vil ponderada. weights define la ventana."""</span>
    m = weights.shape[<span class="num">0</span>]
    w_sum = <span class="num">0.0</span>
    <span class="kw">for</span> k <span class="kw">in</span> <span class="fn">range</span>(m):
        w_sum += weights[k]

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(signal.shape[<span class="num">0</span>]):
        <span class="kw">if</span> i < m - <span class="num">1</span>:
            result[i] = np.nan  <span class="cm"># no hay suficientes datos</span>
        <span class="kw">else</span>:
            total = <span class="num">0.0</span>
            <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(m):
                total += signal[i - m + <span class="num">1</span> + j] * weights[j]
            result[i] = total / w_sum

<span class="cm"># 500 acciones, 252 d√≠as de cotizaci√≥n cada una</span>
cotizaciones = np.random.rand(<span class="num">500</span>, <span class="num">252</span>) * <span class="num">100</span> + <span class="num">50</span>
pesos = np.array([<span class="num">1.0</span>, <span class="num">2.0</span>, <span class="num">3.0</span>, <span class="num">4.0</span>, <span class="num">5.0</span>])  <span class="cm"># ventana de 5 d√≠as, peso creciente</span>
medias = weighted_moving_avg(cotizaciones, pesos)  <span class="cm"># shape (500, 252)</span>
<span class="cm"># ¬°Broadcasting autom√°tico! La misma ventana se aplica a las 500 series</span></code></pre>

    <p><strong>Ejemplo 3: Distancia euclidiana entre dos conjuntos de vectores</strong></p>
<pre><code><span class="dec">@guvectorize</span>(
    [(float64[:], float64[:], float64[:])],
    <span class="st">'(d),(d)->()'</span>  <span class="cm"># dos vectores de d dimensiones ‚Üí un escalar</span>
)
<span class="kw">def</span> <span class="fn">euclidean_dist</span>(a, b, result):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(a.shape[<span class="num">0</span>]):
        diff = a[i] - b[i]
        total += diff * diff
    result[<span class="num">0</span>] = np.sqrt(total)

<span class="cm"># Distancia entre 1000 pares de vectores 3D</span>
puntos_a = np.random.rand(<span class="num">1000</span>, <span class="num">3</span>)
puntos_b = np.random.rand(<span class="num">1000</span>, <span class="num">3</span>)
distancias = euclidean_dist(puntos_a, puntos_b)  <span class="cm"># shape (1000,)</span>

<span class="cm"># Broadcasting: distancia de UN punto a TODOS los dem√°s</span>
query = np.array([<span class="num">0.5</span>, <span class="num">0.5</span>, <span class="num">0.5</span>])
todas_dist = euclidean_dist(puntos_a, query)  <span class="cm"># shape (1000,) ‚Äî autom√°tico</span></code></pre>

    <h4>üö´ Errores comunes con @guvectorize</h4>
<pre><code><span class="cm"># ERROR 1: Intentar retornar un valor (como en @vectorize)</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'(n)->()'</span>)
<span class="kw">def</span> <span class="fn">malo</span>(arr, result):
    <span class="kw">return</span> np.sum(arr)  <span class="cm"># ‚ùå El return se IGNORA</span>
    <span class="cm"># ‚úÖ Debes escribir en result[0] = np.sum(arr)</span>

<span class="cm"># ERROR 2: Layout incorrecto ‚Äî olvidar los par√©ntesis del escalar</span>
<span class="cm"># Escalar en el layout se escribe como () no como (1)</span>
<span class="st">'(n)->(1)'</span>   <span class="cm"># ‚ùå Esto crea un array de 1 elemento</span>
<span class="st">'(n)->()'</span>    <span class="cm"># ‚úÖ Esto crea un escalar</span>

<span class="cm"># ERROR 3: Modificar un array de entrada pensando que se guarda</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'()->()'</span>)
<span class="kw">def</span> <span class="fn">malo_overwrite</span>(inval, outval):
    inval[<span class="num">0</span>] = <span class="num">99.0</span>   <span class="cm"># ‚ùå Puede no guardarse si NumPy hizo un cast</span>
    outval[<span class="num">0</span>] = <span class="num">42.0</span>
<span class="cm"># SOLUCI√ìN: usa writable_args=('inval',) si necesitas modificar inputs</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @stencil ===================== -->
    <!-- ============================================ -->
    <h3 style="color: #f0883e; border-color: #f0883e;">4. <code>@stencil</code> ‚Äî Patrones de vecindario</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Defines c√≥mo calcular UN elemento del resultado en funci√≥n de sus <strong>vecinos</strong> en el array de entrada (usando √≠ndices relativos). Numba genera todo el loop y maneja los bordes autom√°ticamente. Cuando se combina con <code>parallel=True</code>, se paraleliza.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Procesamiento de im√°genes (blur, detecci√≥n de bordes, sharpening)</li>
      <li>Simulaciones de ecuaciones diferenciales parciales (difusi√≥n de calor, Laplace)</li>
      <li>Aut√≥matas celulares (Game of Life, etc.)</li>
      <li>Cualquier c√°lculo donde el resultado en (i,j) depende de valores vecinos en el input</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Operaciones que no dependen de vecinos (usa @vectorize o @njit)</li>
      <li>El "vecindario" es variable o depende de los datos (usa @njit con loop manual)</li>
      <li>Necesitas acceder a posiciones absolutas, no relativas (o usa <code>standard_indexing</code>)</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Detecci√≥n de bordes (Sobel) en una imagen</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> stencil, njit, prange
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">sobel_x</span>(img):
    <span class="cm">"""Filtro Sobel horizontal: detecta bordes verticales."""</span>
    <span class="kw">return</span> (-<span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">-1</span>] + <span class="num">0</span> * img[<span class="num">-1</span>, <span class="num">0</span>] + <span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">1</span>] +
            -<span class="num">2</span> * img[ <span class="num">0</span>, <span class="num">-1</span>] + <span class="num">0</span> * img[ <span class="num">0</span>, <span class="num">0</span>] + <span class="num">2</span> * img[ <span class="num">0</span>, <span class="num">1</span>] +
            -<span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">-1</span>] + <span class="num">0</span> * img[ <span class="num">1</span>, <span class="num">0</span>] + <span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">1</span>])

<span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">sobel_y</span>(img):
    <span class="cm">"""Filtro Sobel vertical: detecta bordes horizontales."""</span>
    <span class="kw">return</span> (-<span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">-1</span>] - <span class="num">2</span> * img[<span class="num">-1</span>, <span class="num">0</span>] - <span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">1</span>] +
             <span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">-1</span>] + <span class="num">2</span> * img[ <span class="num">1</span>, <span class="num">0</span>] + <span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">1</span>])

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">edge_detection</span>(imagen):
    <span class="cm">"""Magnitud del gradiente = sqrt(Gx¬≤ + Gy¬≤)"""</span>
    gx = sobel_x(imagen)
    gy = sobel_y(imagen)
    <span class="cm"># np.sqrt se paraleliza autom√°ticamente</span>
    <span class="kw">return</span> np.sqrt(gx ** <span class="num">2</span> + gy ** <span class="num">2</span>)

imagen = np.random.rand(<span class="num">1920</span>, <span class="num">1080</span>)  <span class="cm"># imagen Full HD</span>
bordes = edge_detection(imagen)</code></pre>

    <p><strong>Ejemplo 2: Simulaci√≥n de difusi√≥n de calor</strong></p>
<pre><code><span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">heat_step</span>(T):
    <span class="cm">"""Un paso de la ecuaci√≥n de calor 2D discretizada."""</span>
    <span class="kw">return</span> <span class="num">0.25</span> * (T[<span class="num">-1</span>, <span class="num">0</span>] + T[<span class="num">1</span>, <span class="num">0</span>] + T[<span class="num">0</span>, <span class="num">-1</span>] + T[<span class="num">0</span>, <span class="num">1</span>])

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">simulate_heat</span>(grid, n_steps):
    <span class="cm">"""Simula difusi√≥n de calor por n pasos."""</span>
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_steps):
        grid = heat_step(grid)
    <span class="kw">return</span> grid

<span class="cm"># Placa de metal 200x200, caliente en el centro</span>
placa = np.zeros((<span class="num">200</span>, <span class="num">200</span>))
placa[<span class="num">90</span>:<span class="num">110</span>, <span class="num">90</span>:<span class="num">110</span>] = <span class="num">100.0</span>  <span class="cm"># fuente de calor</span>
resultado = simulate_heat(placa, <span class="num">500</span>)</code></pre>

    <p><strong>Ejemplo 3: Suavizado de datos de sensor (1D)</strong></p>
<pre><code><span class="dec">@stencil</span>(neighborhood=((<span class="num">-2</span>, <span class="num">2</span>),))
<span class="kw">def</span> <span class="fn">smooth_sensor</span>(data):
    <span class="cm">"""Suavizado con ventana de 5 puntos (media simple)."""</span>
    <span class="kw">return</span> (data[<span class="num">-2</span>] + data[<span class="num">-1</span>] + data[<span class="num">0</span>] + data[<span class="num">1</span>] + data[<span class="num">2</span>]) / <span class="num">5.0</span>

sensor_ruidoso = np.sin(np.linspace(<span class="num">0</span>, <span class="num">10</span>, <span class="num">10000</span>)) + np.random.randn(<span class="num">10000</span>) * <span class="num">0.3</span>
sensor_limpio = smooth_sensor(sensor_ruidoso)</code></pre>

    <!-- ============================================ -->
    <!-- ============ @jitclass ==================== -->
    <!-- ============================================ -->
    <h3 style="color: #f85149; border-color: #f85149;">5. <code>@jitclass</code> ‚Äî Clases con estado compilado</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Compila una clase entera: los datos se almacenan como una estructura C en memoria (sin overhead de objetos Python) y todos los m√©todos se compilan a nopython. Ideal cuando necesitas <strong>encapsular estado mutable</strong> y operarlo eficientemente.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Simulaciones donde entidades tienen estado (part√≠culas, agentes, celdas)</li>
      <li>Estructuras de datos num√©ricas personalizadas (matrices sparse, √°rboles KD simplificados)</li>
      <li>Algoritmos iterativos con estado (optimizadores, filtros Kalman)</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Solo necesitas funciones sin estado ‚Üí usa @njit</li>
      <li>Tu clase tiene herencia compleja, propiedades din√°micas, o m√©todos con strings ‚Üí no es compatible</li>
      <li>Solo necesitas pasar datos entre funciones ‚Üí un namedtuple o un array es m√°s simple</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Filtro Kalman 1D para tracking de sensor</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> float64
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass

<span class="dec">@jitclass</span>([
    (<span class="st">'x'</span>, float64),         <span class="cm"># estimaci√≥n actual</span>
    (<span class="st">'P'</span>, float64),         <span class="cm"># incertidumbre</span>
    (<span class="st">'Q'</span>, float64),         <span class="cm"># ruido del proceso</span>
    (<span class="st">'R'</span>, float64),         <span class="cm"># ruido de medici√≥n</span>
])
<span class="kw">class</span> <span class="fn">KalmanFilter1D</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, x0, P0, Q, R):
        self.x = x0
        self.P = P0
        self.Q = Q
        self.R = R

    <span class="kw">def</span> <span class="fn">predict</span>(self):
        <span class="cm"># En modelo simple: predicci√≥n = estado anterior</span>
        self.P += self.Q

    <span class="kw">def</span> <span class="fn">update</span>(self, measurement):
        K = self.P / (self.P + self.R)   <span class="cm"># Kalman gain</span>
        self.x += K * (measurement - self.x)
        self.P *= (<span class="num">1.0</span> - K)

    <span class="kw">def</span> <span class="fn">filter_signal</span>(self, measurements, output):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(measurements)):
            self.predict()
            self.update(measurements[i])
            output[i] = self.x

<span class="cm"># Uso</span>
kf = KalmanFilter1D(x0=<span class="num">0.0</span>, P0=<span class="num">1.0</span>, Q=<span class="num">0.01</span>, R=<span class="num">0.5</span>)
mediciones = np.sin(np.linspace(<span class="num">0</span>, <span class="num">10</span>, <span class="num">1000</span>)) + np.random.randn(<span class="num">1000</span>) * <span class="num">0.5</span>
salida = np.empty(<span class="num">1000</span>)
kf.filter_signal(mediciones, salida)</code></pre>

    <p><strong>Ejemplo 2: Acumulador de estad√≠sticas online (Welford)</strong></p>
<pre><code><span class="dec">@jitclass</span>([
    (<span class="st">'n'</span>, float64),
    (<span class="st">'mean'</span>, float64),
    (<span class="st">'M2'</span>, float64),
])
<span class="kw">class</span> <span class="fn">OnlineStats</span>:
    <span class="cm">"""Calcula media y varianza incrementalmente (Welford's algorithm).
    √ötil cuando los datos llegan en streaming y no caben en memoria."""</span>

    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.n = <span class="num">0.0</span>
        self.mean = <span class="num">0.0</span>
        self.M2 = <span class="num">0.0</span>

    <span class="kw">def</span> <span class="fn">add</span>(self, x):
        self.n += <span class="num">1</span>
        delta = x - self.mean
        self.mean += delta / self.n
        delta2 = x - self.mean
        self.M2 += delta * delta2

    <span class="kw">def</span> <span class="fn">variance</span>(self):
        <span class="kw">if</span> self.n < <span class="num">2</span>:
            <span class="kw">return</span> <span class="num">0.0</span>
        <span class="kw">return</span> self.M2 / (self.n - <span class="num">1</span>)

<span class="cm"># Procesar un stream de datos enorme</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_stream</span>(data):
    stats = OnlineStats()
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        stats.add(data[i])
    <span class="kw">return</span> stats.mean, stats.variance()

data = np.random.randn(<span class="num">10_000_000</span>)
media, var = procesar_stream(data)</code></pre>

    <h4>üö´ Errores comunes con @jitclass</h4>
<pre><code><span class="cm"># ERROR 1: No inicializar contenedores en __init__</span>
<span class="dec">@jitclass</span>([(<span class="st">'d'</span>, types.DictType(types.int64, types.float64))])
<span class="kw">class</span> <span class="fn">Malo</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.d[<span class="num">1</span>] = <span class="num">10.0</span>  <span class="cm"># ‚ùå SEGFAULT: d no fue inicializado</span>

<span class="cm"># CORRECTO:</span>
<span class="kw">def</span> <span class="fn">__init__</span>(self):
    self.d = typed.Dict.empty(types.int64, types.float64)  <span class="cm"># ‚úÖ primero inicializar</span>
    self.d[<span class="num">1</span>] = <span class="num">10.0</span>

<span class="cm"># ERROR 2: NumPy arrays necesitan spec expl√≠cito (no se infieren de annotations)</span>
<span class="dec">@jitclass</span>
<span class="kw">class</span> <span class="fn">Malo2</span>:
    data: np.ndarray  <span class="cm"># ‚ùå Numba no sabe el dtype ni dimensiones</span>

<span class="cm"># CORRECTO: spec expl√≠cito para arrays</span>
<span class="dec">@jitclass</span>([(<span class="st">'data'</span>, float64[:])])
<span class="kw">class</span> <span class="fn">Bueno</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, n):
        self.data = np.zeros(n)</code></pre>

    <!-- ============================================ -->
    <!-- ============ @cfunc ======================= -->
    <!-- ============================================ -->
    <h3 style="color: #8b949e; border-color: #8b949e;">6. <code>@cfunc</code> ‚Äî Callbacks para C/Fortran</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Compila una funci√≥n Python en un callback con convenci√≥n de llamada C. Es la forma de pasar funciones Numba a librer√≠as C o a <code>scipy.integrate</code>, <code>scipy.optimize</code>, etc.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Quieres pasar una funci√≥n como callback a <code>scipy.integrate.quad</code></li>
      <li>Interact√∫as con c√≥digo C/Fortran v√≠a ctypes</li>
      <li>Necesitas una funci√≥n llamable desde fuera de Python</li>
    </ul>

    <h4>Ejemplo realista</h4>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> cfunc, float64
<span class="kw">from</span> scipy <span class="kw">import</span> integrate

<span class="cm"># Integraci√≥n num√©rica con SciPy, pero el integrando es Numba-compilado</span>
<span class="dec">@cfunc</span>(float64(float64))
<span class="kw">def</span> <span class="fn">integrando</span>(x):
    <span class="kw">return</span> np.exp(-x ** <span class="num">2</span>) * np.cos(<span class="num">2</span> * x)

<span class="cm"># .ctypes pasa el puntero C a SciPy</span>
resultado, error = integrate.quad(integrando.ctypes, <span class="num">0</span>, <span class="num">10</span>)
<span class="cm"># Mucho m√°s r√°pido que pasar una funci√≥n Python normal a quad()</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @overload ==================== -->
    <!-- ============================================ -->
    <h3 style="color: #58a6ff; border-color: #58a6ff;">7. <code>@overload</code> ‚Äî Extender el ecosistema</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Te permite ense√±arle a Numba c√≥mo ejecutar funciones que no soporta nativamente. Defines una implementaci√≥n alternativa que Numba usar√° en nopython mode. Se ejecuta en <strong>compile time</strong> (no runtime) ‚Äî Numba llama a tu overload con los TIPOS de los argumentos, y t√∫ devuelves una funci√≥n que se compilar√°.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Usas una funci√≥n de terceros dentro de @njit que Numba no soporta</li>
      <li>Quieres que funciones de tu librer√≠a sean usables en c√≥digo Numba</li>
      <li>Necesitas comportamiento polim√≥rfico (diferente implementaci√≥n seg√∫n el tipo)</li>
    </ul>

    <h4>Ejemplo realista</h4>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.extending <span class="kw">import</span> overload

<span class="cm"># Sup√≥n que tienes esta funci√≥n Python que usas en tu proyecto</span>
<span class="kw">def</span> <span class="fn">clip</span>(value, low, high):
    <span class="kw">return</span> min(max(value, low), high)

<span class="cm"># La haces funcionar dentro de @njit</span>
<span class="dec">@overload</span>(clip)
<span class="kw">def</span> <span class="fn">clip_overload</span>(value, low, high):
    <span class="cm"># Este c√≥digo se ejecuta en COMPILE TIME</span>
    <span class="cm"># value, low, high son TIPOS, no valores</span>
    <span class="kw">if</span> <span class="fn">isinstance</span>(value, types.Float):
        <span class="kw">def</span> <span class="fn">impl</span>(value, low, high):
            <span class="cm"># Este c√≥digo se ejecuta en RUNTIME</span>
            <span class="kw">if</span> value < low:
                <span class="kw">return</span> low
            <span class="kw">elif</span> value > high:
                <span class="kw">return</span> high
            <span class="kw">return</span> value
        <span class="kw">return</span> impl
    <span class="cm"># Si retorna None, Numba prueba otros overloads</span>

<span class="cm"># Ahora clip() funciona dentro de c√≥digo @njit</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar</span>(arr):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(arr)):
        arr[i] = clip(arr[i], <span class="num">0.0</span>, <span class="num">1.0</span>)  <span class="cm"># ‚úÖ Funciona gracias al overload</span>
    <span class="kw">return</span> arr</code></pre>

    <!-- ============================================ -->
    <!-- ============ COMPARACI√ìN DIRECTA ========== -->
    <!-- ============================================ -->
    <h3>üìä Comparaci√≥n directa: el mismo problema, diferentes decoradores</h3>
    <p>Para que quede claro cu√°ndo elegir cada uno, veamos c√≥mo resolver√≠as <strong>"calcular la norma L2 de vectores"</strong> con cada decorador y por qu√© elegir√≠as uno u otro.</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit, vectorize, guvectorize, stencil, float64, prange

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN A: @njit ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Cuando tienes UNA matriz y quieres las normas de cada fila</span>
<span class="cm"># M√°s control, escribes el loop t√∫ mismo</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">normas_njit</span>(matrix):
    n_rows = matrix.shape[<span class="num">0</span>]
    result = np.empty(n_rows)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n_rows):
        total = <span class="num">0.0</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j] ** <span class="num">2</span>
        result[i] = np.sqrt(total)
    <span class="kw">return</span> result
<span class="cm"># ‚úÖ Mejor cuando: necesitas control total del loop, o la l√≥gica es compleja</span>
<span class="cm"># ‚ùå Evitar cuando: necesitas broadcasting autom√°tico</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN B: @guvectorize ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Cuando quieres que funcione con arrays de CUALQUIER forma via broadcasting</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'(n)->()'</span>)
<span class="kw">def</span> <span class="fn">norma_guv</span>(vec, result):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(vec.shape[<span class="num">0</span>]):
        total += vec[i] ** <span class="num">2</span>
    result[<span class="num">0</span>] = np.sqrt(total)
<span class="cm"># ‚úÖ Mejor cuando: quieres broadcasting autom√°tico</span>
<span class="cm">#    norma_guv(vector_1d) ‚Üí escalar</span>
<span class="cm">#    norma_guv(matrix_2d) ‚Üí vector (norma por fila)</span>
<span class="cm">#    norma_guv(tensor_3d) ‚Üí matrix (norma por √∫ltima dimensi√≥n)</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN C: @vectorize ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># NO es adecuado aqu√≠ porque vectorize es para operaciones escalar‚Üíescalar</span>
<span class="cm"># La norma necesita ver el vector completo, no elemento por elemento</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN D: @stencil ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># NO es adecuado aqu√≠ porque la norma no es un patr√≥n de vecindario</span>
<span class="cm"># Stencil es para cuando el resultado en posici√≥n [i] depende de [i-1], [i+1], etc.</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ RESUMEN FINAL ================ -->
    <!-- ============================================ -->
    <h3>üéØ Resumen final: √°rbol de decisi√≥n</h3>
<pre><code><span class="cm">¬øQu√© tipo de operaci√≥n tienes?
‚îÇ
‚îú‚îÄ Escalar ‚Üí Escalar (sin ver vecinos ni el array completo)
‚îÇ  ‚îî‚îÄ ¬øNecesitas broadcasting/reduce/accumulate?
‚îÇ     ‚îú‚îÄ S√ç ‚Üí <strong style="color:#3fb950">@vectorize</strong>
‚îÇ     ‚îî‚îÄ NO ‚Üí <strong style="color:#58a6ff">@njit</strong> (m√°s simple)
‚îÇ
‚îú‚îÄ Sub-array ‚Üí Escalar o Sub-array (operas sobre filas, vectores, etc.)
‚îÇ  ‚îî‚îÄ ¬øNecesitas broadcasting entre inputs de distintas dimensiones?
‚îÇ     ‚îú‚îÄ S√ç ‚Üí <strong style="color:#d2a8ff">@guvectorize</strong>
‚îÇ     ‚îî‚îÄ NO ‚Üí <strong style="color:#58a6ff">@njit</strong> con loop manual
‚îÇ
‚îú‚îÄ Cada resultado depende de VECINOS en un patr√≥n fijo
‚îÇ  ‚îî‚îÄ <strong style="color:#f0883e">@stencil</strong> (+ parallel=True en la funci√≥n que lo llama)
‚îÇ
‚îú‚îÄ Necesitas una clase con estado y m√©todos r√°pidos
‚îÇ  ‚îî‚îÄ <strong style="color:#f85149">@jitclass</strong>
‚îÇ
‚îú‚îÄ Necesitas pasar una funci√≥n como callback a C/SciPy
‚îÇ  ‚îî‚îÄ <strong style="color:#8b949e">@cfunc</strong>
‚îÇ
‚îî‚îÄ Necesitas que una funci√≥n externa funcione dentro de @njit
   ‚îî‚îÄ <strong style="color:#58a6ff">@overload</strong></span></code></pre>

    <div class="tip">
      <strong>Regla de oro:</strong> Empieza siempre con <code>@njit</code>. Solo cambia a otro decorador cuando necesites una capacidad espec√≠fica que <code>@njit</code> no te da (broadcasting ‚Üí @vectorize/@guvectorize, vecinos ‚Üí @stencil, estado ‚Üí @jitclass).
    </div>

  </div>
</div>




<!-- ==================== M√ìDULO 10 ==================== -->
<div class="module" id="m10">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">10</span> Cheatsheet y Patrones Comunes</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Referencia r√°pida de decoradores</h3>
    <table>
      <tr><th>Decorador</th><th>Para qu√©</th><th>Ejemplo clave</th></tr>
      <tr><td><code>@jit / @njit</code></td><td>Compilar funciones generales</td><td><code>@njit(parallel=True, fastmath=True, cache=True)</code></td></tr>
      <tr><td><code>@vectorize</code></td><td>Crear ufuncs (escalar‚Üíescalar)</td><td><code>@vectorize([float64(float64, float64)])</code></td></tr>
      <tr><td><code>@guvectorize</code></td><td>Ufuncs generalizados (array‚Üíarray)</td><td><code>@guvectorize([(f64[:], f64[:])], '(n)->()')</code></td></tr>
      <tr><td><code>@stencil</code></td><td>Operaciones sobre vecindarios</td><td><code>@stencil(neighborhood=((-1,1),(-1,1)))</code></td></tr>
      <tr><td><code>@jitclass</code></td><td>Clases compiladas con estado</td><td><code>@jitclass([('x', float64)])</code></td></tr>
      <tr><td><code>@cfunc</code></td><td>Crear callbacks C</td><td><code>@cfunc("float64(float64)")</code></td></tr>
      <tr><td><code>@overload</code></td><td>Extender funciones para nopython</td><td><code>@overload(mi_funcion)</code></td></tr>
      <tr><td><code>@intrinsic</code></td><td>Generar LLVM IR directamente</td><td>Para expertos en LLVM</td></tr>
    </table>

    <h3>Opciones de @jit combinadas</h3>
<pre><code><span class="cm"># El "full power" Numba decorator</span>
<span class="dec">@njit</span>(
    parallel=<span class="num">True</span>,     <span class="cm"># Auto-paralelizar operaciones</span>
    fastmath=<span class="num">True</span>,     <span class="cm"># Relajar IEEE 754</span>
    cache=<span class="num">True</span>,        <span class="cm"># Guardar compilaci√≥n en disco</span>
    nogil=<span class="num">True</span>,        <span class="cm"># Liberar GIL para multithreading</span>
    boundscheck=<span class="num">False</span>, <span class="cm"># No verificar l√≠mites de array (default)</span>
    error_model=<span class="st">'numpy'</span>,  <span class="cm"># Seguir sem√°ntica de errores de NumPy</span>
)
<span class="kw">def</span> <span class="fn">ultimate_function</span>(data):
    ...</code></pre>

    <h3>Patr√≥n: Procesar datos en paralelo con threading</h3>
<pre><code><span class="kw">import</span> threading
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_chunk</span>(data, result, start, end):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(start, end):
        result[i] = np.sqrt(data[i]) * np.log(data[i] + <span class="num">1</span>)

<span class="cm"># Dividir trabajo en threads</span>
data = np.random.rand(<span class="num">10_000_000</span>)
result = np.empty_like(data)
n_threads = <span class="num">4</span>
chunk = <span class="fn">len</span>(data) // n_threads

threads = []
<span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_threads):
    start = t * chunk
    end = start + chunk <span class="kw">if</span> t < n_threads - <span class="num">1</span> <span class="kw">else</span> <span class="fn">len</span>(data)
    thread = threading.Thread(target=process_chunk, args=(data, result, start, end))
    threads.append(thread)
    thread.start()
<span class="kw">for</span> t <span class="kw">in</span> threads:
    t.join()</code></pre>

    <h3>Patr√≥n: Ahead-of-Time compilation</h3>
<pre><code><span class="kw">from</span> numba.pycc <span class="kw">import</span> CC

cc = CC(<span class="st">'my_module'</span>)

<span class="dec">@cc.export</span>(<span class="st">'multf'</span>, <span class="st">'f8(f8, f8)'</span>)
<span class="kw">def</span> <span class="fn">mult</span>(a, b):
    <span class="kw">return</span> a * b

<span class="dec">@cc.export</span>(<span class="st">'square'</span>, <span class="st">'f8(f8)'</span>)
<span class="kw">def</span> <span class="fn">square</span>(a):
    <span class="kw">return</span> a ** <span class="num">2</span>

cc.compile()  <span class="cm"># Genera my_module.so / my_module.pyd</span>
<span class="cm"># Luego: import my_module; my_module.multf(3.0, 4.0)</span></code></pre>

    <h3>Variables de entorno √∫tiles</h3>
    <table>
      <tr><th>Variable</th><th>Efecto</th></tr>
      <tr><td><code>NUMBA_NUM_THREADS=N</code></td><td>N√∫mero de threads para paralelismo</td></tr>
      <tr><td><code>NUMBA_PARALLEL_DIAGNOSTICS=4</code></td><td>Diagn√≥sticos de auto-paralelizaci√≥n</td></tr>
      <tr><td><code>NUMBA_DEVELOPER_MODE=1</code></td><td>Mensajes de error detallados</td></tr>
      <tr><td><code>NUMBA_DISABLE_JIT=1</code></td><td>Desactiva JIT (para debugging)</td></tr>
      <tr><td><code>NUMBA_CACHE_DIR=path</code></td><td>Directorio para el cache de compilaci√≥n</td></tr>
      <tr><td><code>NUMBA_THREADING_LAYER=tbb</code></td><td>Seleccionar backend de threading (tbb/omp/workqueue)</td></tr>
    </table>

    <h3>Lo que Numba NO soporta (trampas comunes)</h3>
    <ul>
      <li>Listas de Python est√°ndar (usa <code>numba.typed.List</code>)</li>
      <li>Diccionarios Python est√°ndar (usa <code>numba.typed.Dict</code>)</li>
      <li>Excepciones personalizadas con argumentos</li>
      <li>Clases regulares de Python (usa <code>@jitclass</code>)</li>
      <li><code>try/except</code> limitado (solo excepciones b√°sicas)</li>
      <li>Strings con operaciones complejas</li>
      <li>Generadores con <code>yield</code> (soporte limitado)</li>
      <li>Recursi√≥n con tipos diferentes en cada nivel</li>
      <li><code>**kwargs</code> (solo <code>*args</code> limitado)</li>
    </ul>

    <div class="tip">
      <strong>Regla de oro:</strong> Si tu c√≥digo es principalmente num√©rico con arrays NumPy y loops, Numba lo acelerar√°. Si depende mucho de objetos Python, strings, o I/O, busca otras herramientas (Cython, C extensions).
    </div>
  </div>
</div>

</div>

<script>
function toggle(header) {
  header.parentElement.classList.toggle('open');
}
</script>

</body>
</html>
