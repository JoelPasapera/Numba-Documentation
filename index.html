<!DOCTYPE html>
<html lang="es">
<head>
<!-- metadatos SEO -->
<meta name="description" content="Gu√≠a completa de Numba en espa√±ol: desde @jit b√°sico hasta @overload avanzado. Optimizaci√≥n Python para computaci√≥n num√©rica.">
<meta name="keywords" content="numba, python, optimizaci√≥n, jit, vectorize, paralelismo, numpy">
<meta name="author" content="Joel Pasapera">
<meta property="og:title" content="Gu√≠a Completa de Numba">
<meta property="og:description" content="De principiante a experto en optimizaci√≥n num√©rica con Python">
<meta property="og:type" content="article">
<meta property="og:url" content="https://joelpasapera.github.io/Numba-Documentation/"> <!-- URL_REAL_DEL_GITHUB_PAGES -->
<meta property="og:image" content="https://joelpasapera.github.io/Numba-Documentation/preview.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<!-- ;) -->
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Gu√≠a completa de Numba ‚Äî de principiante a avanzado</title>
<!-- usar√© un estilo dark mode -->
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --border: #30363d;
    --text: #e6edf3;
    --muted: #8b949e;
    --accent: #58a6ff;
    --accent2: #3fb950;
    --accent3: #d2a8ff;
    --accent4: #f0883e;
    --danger: #f85149;
    --code-bg: #0d1117;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
    background: var(--bg); color: var(--text);
    line-height: 1.7; font-size: 16px;
  }
  .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }

  /* Header */
  .hero {
    text-align: center; padding: 3rem 0 2rem;
    border-bottom: 1px solid var(--border); margin-bottom: 2rem;
  }
  .hero h1 { font-size: 2.4rem; font-weight: 800; margin-bottom: 0.5rem;
    background: linear-gradient(135deg, var(--accent), var(--accent3));
    -webkit-background-clip: text; -webkit-text-fill-color: transparent;
  }
  .hero .subtitle { color: var(--muted); font-size: 1.1rem; }
  .badge { display: inline-block; padding: 3px 10px; border-radius: 20px;
    font-size: 0.75rem; font-weight: 600; margin: 0.5rem 0.25rem;
  }
  .badge.green { background: rgba(63,185,80,0.15); color: var(--accent2); border: 1px solid rgba(63,185,80,0.3); }
  .badge.blue { background: rgba(88,166,255,0.15); color: var(--accent); border: 1px solid rgba(88,166,255,0.3); }
  .badge.purple { background: rgba(210,168,255,0.15); color: var(--accent3); border: 1px solid rgba(210,168,255,0.3); }
  .badge.orange { background: rgba(240,136,62,0.15); color: var(--accent4); border: 1px solid rgba(240,136,62,0.3); }

  /* Navigation */
  .toc { background: var(--surface); border: 1px solid var(--border);
    border-radius: 12px; padding: 1.5rem; margin-bottom: 2.5rem;
  }
  .toc h2 { font-size: 1.1rem; color: var(--accent); margin-bottom: 1rem; }
  .toc-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 0.3rem 2rem; }
  .toc a { color: var(--muted); text-decoration: none; font-size: 0.9rem; padding: 2px 0; display: block; }
  .toc a:hover { color: var(--accent); }
  .toc .module-label { font-size: 0.7rem; color: var(--accent3); text-transform: uppercase;
    letter-spacing: 1px; margin-top: 0.7rem; margin-bottom: 0.2rem; }

  /* Sections */
  .module {
    border: 1px solid var(--border); border-radius: 12px;
    margin-bottom: 2rem; overflow: hidden;
  }
  .module-header {
    background: var(--surface); padding: 1.2rem 1.5rem;
    border-bottom: 1px solid var(--border); cursor: pointer;
    display: flex; align-items: center; justify-content: space-between;
  }
  .module-header:hover { background: #1c2129; }
  .module-header h2 { font-size: 1.3rem; }
  .module-header .num { color: var(--accent); margin-right: 0.75rem; font-weight: 800; }
  .module-body { padding: 1.5rem; display: none; }
  .module.open .module-body { display: block; }
  .module-header .arrow { transition: transform 0.2s; color: var(--muted); font-size: 1.2rem; }
  .module.open .module-header .arrow { transform: rotate(90deg); }

  h3 { color: var(--accent2); font-size: 1.1rem; margin: 1.5rem 0 0.75rem;
    padding-bottom: 0.3rem; border-bottom: 1px solid var(--border);
  }
  h4 { color: var(--accent3); font-size: 0.95rem; margin: 1.2rem 0 0.5rem; }
  p { margin-bottom: 0.75rem; }

  /* Code blocks */
  pre {
    background: var(--code-bg); border: 1px solid var(--border);
    border-radius: 8px; padding: 1rem 1.2rem; overflow-x: auto;
    margin: 0.75rem 0 1rem; font-size: 0.88rem; line-height: 1.6;
  }
  code { font-family: 'SF Mono', 'Fira Code', 'Cascadia Code', Consolas, monospace; }
  p code, li code {
    background: rgba(88,166,255,0.1); padding: 2px 6px; border-radius: 4px;
    font-size: 0.88em; color: var(--accent);
  }

  /* Syntax highlighting */
  .kw { color: #ff7b72; }
  .fn { color: #d2a8ff; }
  .st { color: #a5d6ff; }
  .cm { color: #8b949e; font-style: italic; }
  .num { color: #79c0ff; }
  .dec { color: #f0883e; }
  .op { color: #ff7b72; }
  .typ { color: #ffa657; }

  /* Callouts */
  .tip, .warn, .info, .perf {
    border-radius: 8px; padding: 1rem 1.2rem; margin: 1rem 0;
    border-left: 4px solid;
  }
  .tip { background: rgba(63,185,80,0.08); border-color: var(--accent2); }
  .warn { background: rgba(248,81,73,0.08); border-color: var(--danger); }
  .info { background: rgba(88,166,255,0.08); border-color: var(--accent); }
  .perf { background: rgba(240,136,62,0.08); border-color: var(--accent4); }
  .tip::before { content: "üí° Tip"; font-weight: 700; color: var(--accent2); display: block; margin-bottom: 0.3rem; }
  .warn::before { content: "‚ö†Ô∏è Cuidado"; font-weight: 700; color: var(--danger); display: block; margin-bottom: 0.3rem; }
  .info::before { content: "‚ÑπÔ∏è Info"; font-weight: 700; color: var(--accent); display: block; margin-bottom: 0.3rem; }
  .perf::before { content: "‚ö° Rendimiento"; font-weight: 700; color: var(--accent4); display: block; margin-bottom: 0.3rem; }

  ul, ol { margin: 0.5rem 0 1rem 1.5rem; }
  li { margin-bottom: 0.3rem; }

  /* Comparison tables */
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
  th { background: var(--surface); color: var(--accent); text-align: left; padding: 0.6rem 0.8rem;
    border: 1px solid var(--border); font-weight: 600; }
  td { padding: 0.5rem 0.8rem; border: 1px solid var(--border); }
  tr:nth-child(even) td { background: rgba(255,255,255,0.02); }

  .progress-map {
    display: flex; gap: 0.5rem; flex-wrap: wrap; margin: 1rem 0;
  }
  .progress-step {
    flex: 1; min-width: 120px; text-align: center; padding: 0.8rem 0.5rem;
    border-radius: 8px; border: 1px solid var(--border); background: var(--surface);
    font-size: 0.8rem;
  }
  .progress-step .step-num { font-size: 1.5rem; font-weight: 800; }
  .progress-step.s1 .step-num { color: var(--accent2); }
  .progress-step.s2 .step-num { color: var(--accent); }
  .progress-step.s3 .step-num { color: var(--accent3); }
  .progress-step.s4 .step-num { color: var(--accent4); }
  .progress-step.s5 .step-num { color: var(--danger); }

  @media(max-width:600px) {
    .toc-grid { grid-template-columns: 1fr; }
    .hero h1 { font-size: 1.8rem; }
    .progress-map { flex-direction: column; }
  }
</style>
</head>
<body>
<div class="container">

<div class="hero">
  <h1>Gu√≠a Completa de Numba</h1>
  <p class="subtitle">De principiante a profesional ‚Äî con ejemplos pr√°cticos</p>
  <div>
    <span class="badge green">CPU Optimization</span>
    <span class="badge blue">Paralelismo</span>
    <span class="badge purple">NumPy Integration</span>
    <span class="badge orange">Extensiones</span>
  </div>
</div>

<!-- Mapa de progreso -->
<div class="progress-map">
  <div class="progress-step s1"><div class="step-num">1</div>Fundamentos<br>@jit b√°sico</div>
  <div class="progress-step s2"><div class="step-num">2</div>Ufuncs<br>vectorize</div>
  <div class="progress-step s3"><div class="step-num">3</div>Paralelismo<br>prange</div>
  <div class="progress-step s4"><div class="step-num">4</div>Estructuras<br>jitclass</div>
  <div class="progress-step s5"><div class="step-num">5</div>Extensiones<br>overload</div>
</div>

<!-- TOC -->
<div class="toc">
  <h2>üìö Tabla de Contenidos</h2>
  <div class="toc-grid">
    <div>
      <div class="module-label">Fundamentos</div>
      <a href="#m1">1. ¬øQu√© es Numba y c√≥mo funciona?</a>
      <a href="#m2">2. @jit ‚Äî Tu primer decorador</a>
      <a href="#m3">3. Tipos y Signatures</a>
      <div class="module-label">Intermedio</div>
      <a href="#m4">4. @vectorize y @guvectorize</a>
      <a href="#m5">5. Paralelismo con parallel y prange</a>
    </div>
    <div>
      <div class="module-label">Avanzado</div>
      <a href="#m6">6. @jitclass ‚Äî Clases compiladas</a>
      <a href="#m7">7. @stencil ‚Äî Patrones de c√°lculo</a>
      <a href="#m8">8. Performance Tips profesionales</a>
      <div class="module-label">Experto</div>
      <a href="#m9">9. Extendiendo Numba (@overload, @intrinsic)</a>
      <a href="#m10">10. Cheatsheet y Patrones Comunes</a>
      <a href="#m11">11. üå≥ √Årbol de Decisi√≥n Interactivo</a>
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 1 ==================== -->
<div class="module open" id="m1">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">01</span> ¬øQu√© es Numba y c√≥mo funciona?</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>Numba es un compilador <strong>JIT (Just-In-Time)</strong> para Python que traduce funciones Python a c√≥digo m√°quina nativo usando LLVM. Funciona mejor con c√≥digo num√©rico que usa arrays NumPy y loops. Imagina que tienes un loop que procesa 100 millones de datos y tarda 15 segundos en Python puro; con Numba, agregas @njit encima de tu funci√≥n y baja a 0.05 segundos. Es especialmente brutal en c√°lculos num√©ricos masivos (simulaciones, procesamiento de se√±ales, machine learning, y trading algor√≠tmico) donde millones de operaciones por segundo marcan la diferencia entre una estrategia viable y una idea te√≥rica</p>

    <h3>¬øC√≥mo funciona internamente?</h3>
    <p>Cuando decoras una funci√≥n con <code>@jit</code>, Numba hace lo siguiente:</p>
    <ol>
      <li><strong>Analiza el bytecode</strong> de Python de tu funci√≥n</li>
      <li><strong>Infiere los tipos</strong> de todas las variables (en la primera llamada)</li>
      <li><strong>Genera LLVM IR</strong> (representaci√≥n intermedia)</li>
      <li><strong>Compila a c√≥digo m√°quina</strong> nativo optimizado</li>
      <li><strong>Cachea la funci√≥n compilada</strong> para llamadas futuras con los mismos tipos</li>
    </ol>

    <h3>Instalaci√≥n</h3>
<pre><code><span class="cm"># Con conda (recomendado)</span>
$ conda install numba

<span class="cm"># Con pip</span>
$ pip install numba

<span class="cm"># Extras para m√°ximo rendimiento</span>
$ conda install intel-cmplr-lib-rt  <span class="cm"># Intel SVML para funciones matem√°ticas r√°pidas</span>
$ pip install scipy                  <span class="cm"># Para numpy.linalg optimizado</span></code></pre>

    <h3>El ejemplo m√°s simple</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> jit
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="dec">@jit</span>  <span class="cm"># Numba compila esto a c√≥digo m√°quina</span>
<span class="kw">def</span> <span class="fn">suma_cuadrados</span>(arr):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(arr)):
        total += arr[i] ** <span class="num">2</span>
    <span class="kw">return</span> total

<span class="cm"># Primera llamada: compila + ejecuta (m√°s lenta)</span>
arr = np.arange(<span class="num">1_000_000</span>, dtype=np.float64)
resultado = suma_cuadrados(arr)  <span class="cm"># ~compilaci√≥n: 0.5s</span>

<span class="cm"># Segunda llamada: solo ejecuta (rapid√≠sima)</span>
resultado = suma_cuadrados(arr)  <span class="cm"># ~ejecuci√≥n: 0.002s vs ~0.5s en Python puro</span></code></pre>

    <div class="info">
      Numba genera <strong>especializaciones diferentes</strong> seg√∫n los tipos de entrada. Si llamas a la misma funci√≥n con <code>int64</code> y luego con <code>float64</code>, se compilan dos versiones distintas.
    </div>

    <h3>Modos de compilaci√≥n</h3>
    <table>
      <tr><th>Modo</th><th>Descripci√≥n</th><th>Velocidad</th></tr>
      <tr><td><strong>nopython</strong> (default)</td><td>C√≥digo 100% nativo, sin usar el int√©rprete Python</td><td>‚ö°‚ö°‚ö° M√°xima</td></tr>
      <tr><td><strong>object mode</strong></td><td>Fallback que usa objetos Python (casi no acelera)</td><td>‚ö° M√≠nima</td></tr>
    </table>

    <div class="warn">
      Desde Numba 0.59, <code>@jit</code> es equivalente a <code>@jit(nopython=True)</code> y a <code>@njit</code>. El modo object ya no es el fallback por defecto. Si tu c√≥digo no es compatible con nopython mode, Numba lanzar√° un error en lugar de compilar silenciosamente en object mode.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 2 ==================== -->
<div class="module" id="m2">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">02</span> @jit ‚Äî Tu primer decorador</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Compilaci√≥n Lazy vs Eager</h3>
    <h4>Lazy (recomendado para empezar)</h4>
    <p>Numba infiere los tipos autom√°ticamente en la primera llamada:</p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> jit

<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">f</span>(x, y):
    <span class="kw">return</span> x + y

f(<span class="num">1</span>, <span class="num">2</span>)       <span class="cm"># Compila versi√≥n int64</span>
f(<span class="num">1j</span>, <span class="num">2</span>)      <span class="cm"># Compila OTRA versi√≥n para complex128</span>
f(<span class="num">1.0</span>, <span class="num">2.0</span>)  <span class="cm"># Compila OTRA versi√≥n para float64</span></code></pre>

    <h4>Eager (control total de tipos)</h4>
    <p>Defines exactamente qu√© tipos acepta. M√°s r√°pido en la primera llamada (no hay inferencia):</p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> jit, int32, float64

<span class="dec">@jit</span>(int32(int32, int32))  <span class="cm"># retorno(arg1, arg2)</span>
<span class="kw">def</span> <span class="fn">add_int</span>(x, y):
    <span class="kw">return</span> x + y

<span class="cm"># M√∫ltiples signatures</span>
<span class="dec">@jit</span>([int32(int32, int32),
     float64(float64, float64)])
<span class="kw">def</span> <span class="fn">add_multi</span>(x, y):
    <span class="kw">return</span> x + y</code></pre>

    <h3>Opciones clave del decorador</h3>

    <h4><code>cache=True</code> ‚Äî Evita recompilar</h4>
<pre><code><span class="dec">@jit</span>(cache=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">heavy_computation</span>(x):
    <span class="cm"># Se guarda en disco tras la primera compilaci√≥n</span>
    <span class="cm"># Las siguientes ejecuciones del script no recompilan</span>
    <span class="kw">return</span> np.sum(np.sqrt(x))</code></pre>

    <div class="warn">
      Limitaciones del cache: no detecta cambios en funciones importadas de otros m√≥dulos, y las variables globales se tratan como constantes (el cache recuerda el valor al momento de compilar).
    </div>

    <h4><code>nogil=True</code> ‚Äî Libera el GIL</h4>
<pre><code><span class="dec">@jit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_chunk</span>(data):
    <span class="cm"># Se puede ejecutar concurrentemente con otros threads</span>
    <span class="cm"># ¬°Ideal para multithreading real en Python!</span>
    result = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        result += data[i] ** <span class="num">2</span>
    <span class="kw">return</span> result</code></pre>

    <h4><code>fastmath=True</code> ‚Äî Matem√°ticas m√°s r√°pidas</h4>
<pre><code><span class="cm"># Relaja IEEE 754 para mayor velocidad</span>
<span class="dec">@jit</span>(fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">do_sum_fast</span>(A):
    acc = <span class="num">0.</span>
    <span class="kw">for</span> x <span class="kw">in</span> A:
        acc += np.sqrt(x)
    <span class="kw">return</span> acc  <span class="cm"># ~2x m√°s r√°pido que sin fastmath</span>

<span class="cm"># Control granular con flags LLVM espec√≠ficas</span>
<span class="dec">@jit</span>(fastmath={<span class="st">'reassoc'</span>, <span class="st">'nsz'</span>})  <span class="cm"># solo reasociaci√≥n y no-signed-zero</span>
<span class="kw">def</span> <span class="fn">add_assoc</span>(x, y):
    <span class="kw">return</span> (x - y) + y</code></pre>

    <h3>Llamando otras funciones compiladas</h3>
<pre><code><span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">square</span>(x):
    <span class="kw">return</span> x ** <span class="num">2</span>

<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">hypot</span>(x, y):
    <span class="kw">return</span> math.sqrt(square(x) + square(y))  <span class="cm"># LLVM puede inlinear square()</span></code></pre>

    <div class="tip">
      <strong>Siempre</strong> decora con <code>@jit</code> las funciones auxiliares que llamas desde c√≥digo Numba. Si no lo haces, Numba genera c√≥digo mucho m√°s lento al tener que "salir" al int√©rprete Python.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 3 ==================== -->
<div class="module" id="m3">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">03</span> Tipos y Signatures</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <h3>Tipos escalares</h3>
    <table>
      <tr><th>Tipo Numba</th><th>Equivalente</th><th>Uso t√≠pico</th></tr>
      <tr><td><code>int8, int16, int32, int64</code></td><td>Enteros con signo</td><td>√çndices, contadores</td></tr>
      <tr><td><code>uint8, uint16, uint32, uint64</code></td><td>Enteros sin signo</td><td>Datos binarios</td></tr>
      <tr><td><code>float32, float64</code></td><td>Punto flotante</td><td>C√°lculos num√©ricos</td></tr>
      <tr><td><code>complex64, complex128</code></td><td>Complejos</td><td>Se√±ales, FFT</td></tr>
      <tr><td><code>boolean</code></td><td>Booleano</td><td>Flags</td></tr>
      <tr><td><code>intp, uintp</code></td><td>Tama√±o de puntero</td><td>√çndices de arrays</td></tr>
      <tr><td><code>void</code></td><td>None</td><td>Funciones sin retorno</td></tr>
    </table>

    <h3>Tipos de arrays</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> float64, int32

<span class="cm"># Indexas el tipo escalar para crear tipo de array</span>
float64[:]       <span class="cm"># Array 1D de float64</span>
int32[:, :]      <span class="cm"># Array 2D de int32</span>
float64[:, :, :] <span class="cm"># Array 3D de float64</span>

<span class="cm"># En signatures completas</span>
<span class="dec">@jit</span>(float64[:](float64[:], float64[:]))
<span class="kw">def</span> <span class="fn">add_arrays</span>(a, b):
    <span class="kw">return</span> a + b

<span class="cm"># Arrays C-contiguous vs Fortran-contiguous</span>
<span class="kw">from</span> numba <span class="kw">import</span> types
types.Array(types.float64, <span class="num">2</span>, <span class="st">'C'</span>)  <span class="cm"># 2D, C-order (row-major)</span>
types.Array(types.float64, <span class="num">2</span>, <span class="st">'F'</span>)  <span class="cm"># 2D, Fortran-order (column-major)</span>
types.Array(types.float64, <span class="num">2</span>, <span class="st">'A'</span>)  <span class="cm"># 2D, cualquier layout</span></code></pre>

    <h3>Contenedores tipados</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.typed <span class="kw">import</span> Dict, List

<span class="cm"># Listas tipadas (se pueden usar en @jit)</span>
<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">usar_lista</span>():
    lst = List()
    lst.append(<span class="num">1</span>)
    lst.append(<span class="num">2</span>)
    lst.append(<span class="num">3</span>)
    <span class="kw">return</span> lst

<span class="cm"># Diccionarios tipados</span>
<span class="dec">@jit</span>
<span class="kw">def</span> <span class="fn">usar_dict</span>():
    d = Dict()
    d[<span class="num">1</span>] = <span class="num">10.0</span>
    d[<span class="num">2</span>] = <span class="num">20.0</span>
    <span class="kw">return</span> d

<span class="cm"># Tipos expl√≠citos</span>
types.DictType(types.int64, types.float64)
types.ListType(types.float64)</code></pre>

    <div class="info">
      Las listas y diccionarios de Python est√°ndar <strong>no funcionan</strong> en nopython mode. Debes usar <code>numba.typed.List</code> y <code>numba.typed.Dict</code>.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 4 ==================== -->
<div class="module" id="m4">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">04</span> @vectorize y @guvectorize ‚Äî Ufuncs a medida</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>@vectorize ‚Äî Ufuncs elemento a elemento</h3>
    <p>Escribes la operaci√≥n escalar y Numba genera el loop autom√°ticamente, con todas las features de un ufunc NumPy (broadcasting, reduce, accumulate).</p>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> vectorize, float64, int64

<span class="cm"># Eager: defines los tipos expl√≠citamente</span>
<span class="dec">@vectorize</span>([float64(float64, float64)])
<span class="kw">def</span> <span class="fn">clip_value</span>(x, threshold):
    <span class="kw">if</span> x > threshold:
        <span class="kw">return</span> threshold
    <span class="kw">elif</span> x < -threshold:
        <span class="kw">return</span> -threshold
    <span class="kw">return</span> x

<span class="cm"># Ahora funciona como cualquier ufunc de NumPy</span>
arr = np.random.randn(<span class="num">1000</span>)
result = clip_value(arr, <span class="num">1.5</span>)  <span class="cm"># Broadcasting autom√°tico</span>

<span class="cm"># ¬°Tambi√©n reduce y accumulate!</span>
<span class="dec">@vectorize</span>([float64(float64, float64)])
<span class="kw">def</span> <span class="fn">add</span>(x, y):
    <span class="kw">return</span> x + y

a = np.arange(<span class="num">12</span>).reshape(<span class="num">3</span>, <span class="num">4</span>)
add.reduce(a, axis=<span class="num">0</span>)       <span class="cm"># Suma por columnas</span>
add.accumulate(a, axis=<span class="num">1</span>)   <span class="cm"># Suma acumulada por filas</span></code></pre>

    <h3>Targets: CPU, Parallel, CUDA</h3>
    <table>
      <tr><th>Target</th><th>Cu√°ndo usar</th><th>Overhead</th></tr>
      <tr><td><code>target='cpu'</code></td><td>Datos peque√±os (&lt; 1KB), baja intensidad</td><td>M√≠nimo</td></tr>
      <tr><td><code>target='parallel'</code></td><td>Datos medianos (&lt; 1MB)</td><td>Bajo (threading)</td></tr>
      <tr><td><code>target='cuda'</code></td><td>Datos grandes (&gt; 1MB), alta intensidad</td><td>Alto (transferencia GPU)</td></tr>
    </table>

<pre><code><span class="cm"># Paralelizado autom√°ticamente en todos los cores</span>
<span class="dec">@vectorize</span>([float64(float64, float64)], target=<span class="st">'parallel'</span>)
<span class="kw">def</span> <span class="fn">add_parallel</span>(x, y):
    <span class="kw">return</span> x + y</code></pre>

    <h3>@guvectorize ‚Äî Operaciones sobre sub-arrays</h3>
    <p>Cuando necesitas operar sobre <strong>porciones de arrays</strong>, no solo escalares. Defines un layout simb√≥lico que indica las dimensiones de entrada/salida.</p>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> guvectorize, float64

<span class="cm"># Media m√≥vil: recibe array 1D, devuelve escalar por cada "fila"</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'(n)->()'</span>)
<span class="kw">def</span> <span class="fn">moving_mean</span>(arr, result):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(arr.shape[<span class="num">0</span>]):
        total += arr[i]
    result[<span class="num">0</span>] = total / arr.shape[<span class="num">0</span>]

<span class="cm"># Aplicar sobre una matriz (cada fila es un vector)</span>
data = np.random.rand(<span class="num">100</span>, <span class="num">10</span>)
means = moving_mean(data)  <span class="cm"># ‚Üí array de 100 medias (broadcasting autom√°tico)</span>

<span class="cm"># Ejemplo m√°s completo: suma con peso</span>
<span class="dec">@guvectorize</span>(
    [(float64[:], float64[:], float64[:])],
    <span class="st">'(n),(n)->(n)'</span>  <span class="cm"># dos arrays entrada ‚Üí un array salida, misma forma</span>
)
<span class="kw">def</span> <span class="fn">weighted_add</span>(a, weights, result):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(a.shape[<span class="num">0</span>]):
        result[i] = a[i] * weights[i]</code></pre>

    <div class="tip">
      En <code>@guvectorize</code> los resultados se escriben en el argumento de salida (<code>result</code>), no se retornan. NumPy aloca el array de salida autom√°ticamente.
    </div>

    <h4>Notaci√≥n de layouts</h4>
    <table>
      <tr><th>Layout</th><th>Significado</th></tr>
      <tr><td><code>(n),()->(n)</code></td><td>Array + escalar ‚Üí array</td></tr>
      <tr><td><code>(n)->()</code></td><td>Array ‚Üí escalar (reducci√≥n)</td></tr>
      <tr><td><code>(m,n),(n)->(m)</code></td><td>Matriz √ó vector ‚Üí vector</td></tr>
      <tr><td><code>(n),(n)->(n)</code></td><td>Dos arrays ‚Üí array elemento a elemento</td></tr>
    </table>
  </div>
</div>

<!-- ==================== M√ìDULO 5 ==================== -->
<div class="module" id="m5">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">05</span> Paralelismo con parallel y prange</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Auto-paralelizaci√≥n de operaciones</h3>
    <p>Con <code>parallel=True</code>, Numba identifica operaciones con sem√°ntica paralela y las fusiona en kernels que se ejecutan en m√∫ltiples threads.</p>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> njit
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">ident_parallel</span>(x):
    <span class="kw">return</span> np.cos(x) ** <span class="num">2</span> + np.sin(x) ** <span class="num">2</span>

<span class="cm"># ~5x m√°s r√°pido que NumPy puro</span>
<span class="cm"># ~6x m√°s r√°pido que @njit sin parallel</span></code></pre>

    <h3>Operaciones auto-paralelizables</h3>
    <ul>
      <li>Operaciones aritm√©ticas entre arrays y escalares (<code>+ - * / ** %</code>)</li>
      <li>Operadores de comparaci√≥n (<code>== != < <= > >=</code>)</li>
      <li>Ufuncs de NumPy soportados en nopython mode</li>
      <li>Reducciones: <code>sum, prod, min, max, argmin, argmax, mean, var, std</code></li>
      <li>Creaci√≥n de arrays: <code>zeros, ones, arange, linspace</code></li>
      <li><code>numpy.dot</code> (matrix √ó vector, vector √ó vector)</li>
      <li>Funciones random (<code>rand, randn, normal, uniform</code>, etc.)</li>
    </ul>

    <h3>prange ‚Äî Loops paralelos expl√≠citos</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> njit, prange
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># Reducci√≥n paralela simple</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">parallel_sum</span>(A):
    s = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(A.shape[<span class="num">0</span>]):  <span class="cm"># cada iteraci√≥n en un thread distinto</span>
        s += A[i]
    <span class="kw">return</span> s

<span class="cm"># Reducci√≥n sobre array 2D</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">parallel_product_2d</span>(n):
    shp = (<span class="num">13</span>, <span class="num">17</span>)
    result = <span class="num">2</span> * np.ones(shp, np.int_)
    tmp = <span class="num">2</span> * np.ones_like(result)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        result *= tmp
    <span class="kw">return</span> result</code></pre>

    <div class="perf">
      Las reducciones soportadas por prange son: <code>+=, +, -=, -, *=, *, /=, /, max(), min()</code>. El operador <code>//=</code> NO est√° soportado (depende del orden de aplicaci√≥n).
    </div>

    <h3>Ejemplo real: Regresi√≥n Log√≠stica paralela</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">logistic_regression</span>(Y, X, w, iterations):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(iterations):
        w -= np.dot(
            ((<span class="num">1.0</span> / (<span class="num">1.0</span> + np.exp(-Y * np.dot(X, w))) - <span class="num">1.0</span>) * Y), X
        )
    <span class="kw">return</span> w

<span class="cm"># Numba fusiona autom√°ticamente las operaciones element-wise</span>
<span class="cm"># en un solo kernel paralelo. ¬°Sin cambiar el c√≥digo!</span></code></pre>

    <h3>‚ö†Ô∏è Race Conditions ‚Äî Errores comunes</h3>
<pre><code><span class="cm"># ‚ùå INCORRECTO: race condition en slices</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">prange_wrong</span>(x):
    y = np.zeros(<span class="num">4</span>)
    <span class="kw">for</span> i <span class="kw">in</span> prange(x.shape[<span class="num">0</span>]):
        y[:] += x[i]  <span class="cm"># ¬°M√∫ltiples threads escriben en y al mismo tiempo!</span>
    <span class="kw">return</span> y

<span class="cm"># ‚úÖ CORRECTO: reducci√≥n sobre array completo</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">prange_correct</span>(x):
    y = np.zeros(<span class="num">4</span>)
    <span class="kw">for</span> i <span class="kw">in</span> prange(x.shape[<span class="num">0</span>]):
        y += x[i]  <span class="cm"># Numba detecta la reducci√≥n correctamente</span>
    <span class="kw">return</span> y</code></pre>

    <div class="warn">
      Mutar listas/sets/dicts dentro de un <code>prange</code> <strong>NO es thread-safe</strong>. Nunca hagas <code>z.append(i)</code> dentro de un loop paralelo ‚Äî causar√° corrupci√≥n de memoria.
    </div>

    <h3>Diagn√≥sticos de paralelismo</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">test</span>(x):
    n = x.shape[<span class="num">0</span>]
    a = np.sin(x)
    b = np.cos(a * a)
    acc = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(n - <span class="num">2</span>):
        <span class="kw">for</span> j <span class="kw">in</span> prange(n - <span class="num">1</span>):
            acc += b[i] + b[j + <span class="num">1</span>]
    <span class="kw">return</span> acc

test(np.arange(<span class="num">10</span>))
test.parallel_diagnostics(level=<span class="num">4</span>)  <span class="cm"># Imprime qu√© se paraleliz√≥ y qu√© no</span></code></pre>
  </div>
</div>

<!-- ==================== M√ìDULO 6 ==================== -->
<div class="module" id="m6">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">06</span> @jitclass ‚Äî Clases compiladas</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>Permite definir clases cuyos m√©todos se compilan a c√≥digo nativo y cuya data se almacena como estructura C (sin overhead del int√©rprete).</p>

    <h3>Uso b√°sico con spec expl√≠cita</h3>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> int32, float32
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass

spec = [
    (<span class="st">'value'</span>, int32),
    (<span class="st">'array'</span>, float32[:]),
]

<span class="dec">@jitclass</span>(spec)
<span class="kw">class</span> <span class="fn">Bag</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, value):
        self.value = value
        self.array = np.zeros(value, dtype=np.float32)

    <span class="dec">@property</span>
    <span class="kw">def</span> <span class="fn">size</span>(self):
        <span class="kw">return</span> self.array.size

    <span class="kw">def</span> <span class="fn">increment</span>(self, val):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(self.size):
            self.array[i] += val
        <span class="kw">return</span> self.array

    <span class="dec">@staticmethod</span>
    <span class="kw">def</span> <span class="fn">add</span>(x, y):
        <span class="kw">return</span> x + y

mybag = Bag(<span class="num">21</span>)
mybag.increment(<span class="num">5.0</span>)</code></pre>

    <h3>Con type annotations (m√°s moderno)</h3>
<pre><code><span class="kw">from</span> typing <span class="kw">import</span> List
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass
<span class="kw">from</span> numba.typed <span class="kw">import</span> List <span class="kw">as</span> NumbaList

<span class="dec">@jitclass</span>
<span class="kw">class</span> <span class="fn">Counter</span>:
    value: int

    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.value = <span class="num">0</span>

    <span class="kw">def</span> <span class="fn">get</span>(self) -> int:
        ret = self.value
        self.value += <span class="num">1</span>
        <span class="kw">return</span> ret

<span class="dec">@jitclass</span>
<span class="kw">class</span> <span class="fn">ListLoopIterator</span>:
    counter: Counter
    items: List[float]

    <span class="kw">def</span> <span class="fn">__init__</span>(self, items: List[float]):
        self.items = items
        self.counter = Counter()</code></pre>

    <h3>Contenedores tipados como campos</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types, typed
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass

kv_ty = (types.int64, types.unicode_type)

<span class="dec">@jitclass</span>([
    (<span class="st">'d'</span>, types.DictType(*kv_ty)),
    (<span class="st">'l'</span>, types.ListType(types.float64))
])
<span class="kw">class</span> <span class="fn">ContainerHolder</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.d = typed.Dict.empty(*kv_ty)
        self.l = typed.List.empty_list(types.float64)

c = ContainerHolder()
c.d[<span class="num">1</span>] = <span class="st">"apple"</span>
c.l.append(<span class="num">123.</span>)</code></pre>

    <div class="warn">
      Los campos de contenedores (<code>Dict</code>, <code>List</code>) <strong>deben inicializarse expl√≠citamente</strong> en <code>__init__</code>. Escribir en un contenedor no inicializado causa <strong>segfault</strong>.
    </div>

    <h3>Dunder methods soportados</h3>
    <p>jitclass soporta una amplia gama de m√©todos especiales: <code>__abs__, __bool__, __getitem__, __setitem__, __len__, __hash__, __eq__, __ne__, __lt__, __le__, __gt__, __ge__, __add__, __mul__, __sub__, __truediv__, __floordiv__, __mod__, __pow__, __neg__, __pos__</code>, y todos sus variantes <code>__i*__</code> (in-place) y <code>__r*__</code> (reflected).</p>

    <div class="info">
      <strong>Limitaci√≥n actual:</strong> jitclass solo funciona en CPU. Soporte para GPU est√° planeado para versiones futuras.
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 7 ==================== -->
<div class="module" id="m7">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">07</span> @stencil ‚Äî Patrones de c√°lculo sobre vecinos</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>Los stencils son patrones donde cada elemento del array resultado depende de un "vecindario" de elementos del array de entrada. Piensa en filtros de imagen, simulaciones de calor, aut√≥matas celulares, etc.</p>

    <h3>Uso b√°sico</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> stencil

<span class="cm"># Promedio de 4 vecinos (tipo Laplaciano discreto)</span>
<span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">kernel_laplacian</span>(a):
    <span class="kw">return</span> <span class="num">0.25</span> * (a[<span class="num">0</span>, <span class="num">1</span>] + a[<span class="num">1</span>, <span class="num">0</span>] + a[<span class="num">0</span>, <span class="num">-1</span>] + a[<span class="num">-1</span>, <span class="num">0</span>])

<span class="cm"># Los √≠ndices son RELATIVOS al punto actual</span>
<span class="cm"># a[0, 0] = elemento actual, a[-1, 0] = arriba, etc.</span>

input_arr = np.arange(<span class="num">100</span>).reshape((<span class="num">10</span>, <span class="num">10</span>)).astype(np.float64)
output = kernel_laplacian(input_arr)
<span class="cm"># Los bordes se ponen a 0 por defecto</span></code></pre>

    <h3>Media m√≥vil con neighborhood</h3>
<pre><code><span class="cm"># Media m√≥vil de 30 d√≠as (sin escribir 30 t√©rminos)</span>
<span class="dec">@stencil</span>(neighborhood=((<span class="num">-29</span>, <span class="num">0</span>),))
<span class="kw">def</span> <span class="fn">moving_average_30d</span>(a):
    cumul = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">-29</span>, <span class="num">1</span>):
        cumul += a[i]
    <span class="kw">return</span> cumul / <span class="num">30</span></code></pre>

    <h3>Opciones de stencil</h3>
    <table>
      <tr><th>Opci√≥n</th><th>Descripci√≥n</th></tr>
      <tr><td><code>cval=X</code></td><td>Valor para los bordes (default: 0)</td></tr>
      <tr><td><code>neighborhood=((min,max),...)</code></td><td>Define el rango de vecinos expl√≠citamente</td></tr>
      <tr><td><code>standard_indexing=("b",)</code></td><td>Arrays auxiliares con indexaci√≥n normal (no relativa)</td></tr>
    </table>

    <h3>Stencil + parallel (m√°xima velocidad)</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">apply_blur</span>(image):
    <span class="cm"># El stencil se paraleliza autom√°ticamente con parallel=True</span>
    <span class="kw">return</span> stencil(
        <span class="kw">lambda</span> a: <span class="num">0.2</span> * (a[<span class="num">-1</span>,<span class="num">0</span>] + a[<span class="num">1</span>,<span class="num">0</span>] + a[<span class="num">0</span>,<span class="num">-1</span>] + a[<span class="num">0</span>,<span class="num">1</span>] + a[<span class="num">0</span>,<span class="num">0</span>])
    )(image)</code></pre>
  </div>
</div>

<!-- ==================== M√ìDULO 8 ==================== -->
<div class="module" id="m8">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">08</span> Performance Tips ‚Äî Nivel profesional</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Tabla de rendimiento real (Intel i7, 10M elementos)</h3>
    <table>
      <tr><th>Configuraci√≥n</th><th>SVML</th><th>Tiempo</th><th>Speedup vs NumPy</th></tr>
      <tr><td>NumPy puro</td><td>‚Äî</td><td>5.84s</td><td>1x</td></tr>
      <tr><td><code>@njit</code></td><td>No</td><td>5.95s</td><td>~1x</td></tr>
      <tr><td><code>@njit</code></td><td>S√≠</td><td>2.26s</td><td>2.6x</td></tr>
      <tr><td><code>@njit(fastmath=True)</code></td><td>S√≠</td><td>1.8s</td><td>3.2x</td></tr>
      <tr><td><code>@njit(parallel=True)</code></td><td>S√≠</td><td>0.624s</td><td>9.4x</td></tr>
      <tr><td><code>@njit(parallel=True, fastmath=True)</code></td><td>S√≠</td><td>0.576s</td><td><strong>10x</strong></td></tr>
    </table>

    <h3>1. Instala Intel SVML</h3>
<pre><code>$ conda install intel-cmplr-lib-rt
<span class="cm"># Numba lo detecta autom√°ticamente</span>
<span class="cm"># Acelera funciones trascendentales (sin, cos, exp, log...)</span></code></pre>

    <h3>2. Loops > Vectorizaci√≥n NumPy (en Numba)</h3>
<pre><code><span class="cm"># Ambos son IGUAL de r√°pidos con @njit</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">vectorized_style</span>(x):
    <span class="kw">return</span> np.cos(x) ** <span class="num">2</span> + np.sin(x) ** <span class="num">2</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">loop_style</span>(x):
    r = np.empty_like(x)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(x)):
        r[i] = np.cos(x[i]) ** <span class="num">2</span> + np.sin(x[i]) ** <span class="num">2</span>
    <span class="kw">return</span> r

<span class="cm"># ¬°Escribe loops sin miedo! LLVM optimiza igual que C</span></code></pre>

    <div class="perf">
      En Numba, los loops son <strong>tan r√°pidos como el c√≥digo vectorizado</strong>, a diferencia de Python puro. Esto da m√°s flexibilidad y a menudo mejor uso de memoria (no creas arrays temporales).
    </div>

    <h3>3. Combinaci√≥n m√°xima</h3>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>, cache=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">max_performance_sum</span>(A):
    n = <span class="fn">len</span>(A)
    acc = <span class="num">0.</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):  <span class="cm"># paralelo + fastmath = m√°xima velocidad</span>
        acc += np.sqrt(A[i])
    <span class="kw">return</span> acc
<span class="cm"># Resultado: ~5.37ms vs 35.2ms sin parallel/fastmath (6.5x)</span></code></pre>

    <h3>4. √Ålgebra lineal optimizada</h3>
<pre><code><span class="cm"># Aseg√∫rate de tener SciPy (para LAPACK/BLAS)</span>
<span class="cm"># Con Anaconda, SciPy usa Intel MKL autom√°ticamente</span>
$ pip install scipy

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">solve_system</span>(A, b):
    <span class="kw">return</span> np.linalg.solve(A, b)  <span class="cm"># Usa MKL internamente</span></code></pre>

    <h3>5. Evita object mode a toda costa</h3>
    <div class="warn">
      Si Numba no puede compilar algo en nopython mode, lanzar√° un error. Las causas m√°s comunes son: usar listas Python normales, llamar funciones no soportadas, o usar tipos de datos no soportados. Revisa los diagn√≥sticos de compilaci√≥n con <code>NUMBA_DEVELOPER_MODE=1</code>.
    </div>

    <h3>6. Memory layout matters</h3>
<pre><code><span class="cm"># C-contiguous (row-major) vs Fortran-contiguous (column-major)</span>
<span class="cm"># Acceder a memoria de forma contigua es MUCHO m√°s r√°pido</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">row_sum_fast</span>(matrix):   <span class="cm"># ‚úÖ Acceso por filas en C-order</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j]
    <span class="kw">return</span> total

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">col_sum_slow</span>(matrix):   <span class="cm"># ‚ùå Acceso por columnas en C-order (cache misses)</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
            total += matrix[i, j]
    <span class="kw">return</span> total</code></pre>
  </div>
</div>

<!-- ==================== M√ìDULO 9 ==================== -->
<div class="module" id="m9">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">09</span> Extendiendo Numba ‚Äî @overload e @intrinsic</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">
    <p>El sistema de extensiones de Numba permite a√±adir soporte para funciones, m√©todos y tipos personalizados en nopython mode.</p>

    <h3>@overload ‚Äî Implementar funciones existentes</h3>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.extending <span class="kw">import</span> overload

<span class="cm"># Supongamos que quieres que len() funcione con tuplas en Numba</span>
<span class="dec">@overload</span>(len)
<span class="kw">def</span> <span class="fn">tuple_len</span>(seq):
    <span class="kw">if</span> <span class="fn">isinstance</span>(seq, types.BaseTuple):
        n = <span class="fn">len</span>(seq)  <span class="cm"># esto se ejecuta en compile time</span>
        <span class="kw">def</span> <span class="fn">len_impl</span>(seq):
            <span class="kw">return</span> n   <span class="cm"># esto se ejecuta en runtime</span>
        <span class="kw">return</span> len_impl
    <span class="cm"># Si no retorna nada, Numba prueba otras implementaciones</span></code></pre>

    <h3>@overload_method ‚Äî A√±adir m√©todos a tipos</h3>
<pre><code><span class="kw">from</span> numba.extending <span class="kw">import</span> overload_method

<span class="dec">@overload_method</span>(types.Array, <span class="st">'take'</span>)
<span class="kw">def</span> <span class="fn">array_take</span>(arr, indices):
    <span class="kw">if</span> <span class="fn">isinstance</span>(indices, types.Array):
        <span class="kw">def</span> <span class="fn">take_impl</span>(arr, indices):
            n = indices.shape[<span class="num">0</span>]
            res = np.empty(n, arr.dtype)
            <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
                res[i] = arr[indices[i]]
            <span class="kw">return</span> res
        <span class="kw">return</span> take_impl</code></pre>

    <h3>@overload_attribute ‚Äî Propiedades personalizadas</h3>
<pre><code><span class="kw">from</span> numba.extending <span class="kw">import</span> overload_attribute

<span class="dec">@overload_attribute</span>(types.Array, <span class="st">'nbytes'</span>)
<span class="kw">def</span> <span class="fn">array_nbytes</span>(arr):
    <span class="kw">def</span> <span class="fn">get</span>(arr):
        <span class="kw">return</span> arr.size * arr.itemsize
    <span class="kw">return</span> get</code></pre>

    <h3>@intrinsic ‚Äî Escape hatch a LLVM IR</h3>
    <p>Para expertos: genera c√≥digo LLVM IR directamente. Se inlinea en el caller.</p>
<pre><code><span class="kw">from</span> numba.extending <span class="kw">import</span> intrinsic

<span class="dec">@intrinsic</span>
<span class="kw">def</span> <span class="fn">cast_int_to_byte_ptr</span>(typingctx, src):
    <span class="kw">if</span> <span class="fn">isinstance</span>(src, types.Integer):
        result_type = types.CPointer(types.uint8)
        sig = result_type(types.uintp)
        <span class="kw">def</span> <span class="fn">codegen</span>(context, builder, signature, args):
            [src] = args
            rtype = signature.return_type
            llrtype = context.get_value_type(rtype)
            <span class="kw">return</span> builder.inttoptr(src, llrtype)
        <span class="kw">return</span> sig, codegen</code></pre>

    <h3>Importar funciones Cython</h3>
<pre><code><span class="kw">import</span> ctypes
<span class="kw">from</span> numba.extending <span class="kw">import</span> get_cython_function_address

<span class="cm"># Obtener direcci√≥n de funci√≥n C definida en un m√≥dulo Cython</span>
addr = get_cython_function_address(<span class="st">"foo"</span>, <span class="st">"myexp"</span>)
functype = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double)
myexp = functype(addr)

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">double_myexp</span>(x):
    <span class="kw">return</span> <span class="num">2</span> * myexp(x)  <span class="cm"># ¬°Llama a la funci√≥n C dentro de Numba!</span></code></pre>

    <h3>StructRef ‚Äî Estructuras mutables por referencia</h3>
<pre><code><span class="kw">from</span> numba.core <span class="kw">import</span> types
<span class="kw">from</span> numba.experimental <span class="kw">import</span> structref

<span class="dec">@structref.register</span>
<span class="kw">class</span> <span class="fn">MyStructType</span>(types.StructRef):
    <span class="kw">def</span> <span class="fn">preprocess_fields</span>(self, fields):
        <span class="kw">return</span> <span class="fn">tuple</span>(
            (name, types.unliteral(typ)) <span class="kw">for</span> name, typ <span class="kw">in</span> fields
        )

<span class="cm"># Proxy Python para interactuar desde el int√©rprete</span>
<span class="kw">class</span> <span class="fn">MyStruct</span>(structref.StructRefProxy):
    <span class="kw">def</span> <span class="fn">__new__</span>(cls, name, vector):
        <span class="kw">return</span> structref.StructRefProxy.__new__(cls, name, vector)

<span class="cm"># Se pueden definir propiedades que delegan a funciones JIT</span></code></pre>
  </div>
</div>


<!--
  ============================================================
  M√ìDULOS: Memoria & Cache + Benchmarking Profesional
  ============================================================
  Despu√©s de haber observado el m√≥dulo de decoradores (‚òÖ), es importante ver esto
  ANTES del m√≥dulo 10 (Cheatsheet).
  ============================================================
-->
<a href="#m-memory">‚ö° Memoria, Cache y CPU: El Fundamento Invisible</a>
<!-- ==================== M√ìDULO MEMORIA ==================== -->
<div class="module" id="m-memory">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">‚ö°</span> Memoria, Cache y CPU: El Fundamento Invisible del Rendimiento</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Esto es lo que <strong>nadie te ense√±a</strong> pero determina el 80% del rendimiento de tu c√≥digo num√©rico. Tu CPU puede hacer miles de millones de operaciones por segundo, pero si est√° esperando datos de RAM, est√° literalmente sentada sin hacer nada. Entender esto transforma c√≥mo escribes c√≥digo para siempre.</p>

    <!-- ============ JERARQU√çA DE MEMORIA ============ -->
    <h3>üèóÔ∏è La jerarqu√≠a de memoria: por qu√© tu c√≥digo es lento</h3>

    <p>Tu CPU NO accede a RAM directamente. Existe una cadena de cach√©s intermedias, cada una m√°s r√°pida pero m√°s peque√±a:</p>

    <table>
      <tr><th>Nivel</th><th>Tama√±o t√≠pico</th><th>Latencia</th><th>Analog√≠a</th></tr>
      <tr><td><strong>Registros</strong></td><td>~1 KB</td><td>~0.3 ns (1 ciclo)</td><td>Tu mano: instant√°neo</td></tr>
      <tr><td><strong>Cache L1</strong></td><td>32-64 KB por core</td><td>~1 ns (3-4 ciclos)</td><td>Tu escritorio: 1 segundo</td></tr>
      <tr><td><strong>Cache L2</strong></td><td>256 KB - 1 MB por core</td><td>~4 ns (10-12 ciclos)</td><td>Tu estanter√≠a: 4 segundos</td></tr>
      <tr><td><strong>Cache L3</strong></td><td>8-64 MB compartido</td><td>~12 ns (30-40 ciclos)</td><td>La biblioteca del edificio: 12 segundos</td></tr>
      <tr><td><strong>RAM</strong></td><td>8-128 GB</td><td>~60-100 ns (200+ ciclos)</td><td>Amazon: un minuto esperando</td></tr>
      <tr><td><strong>Disco SSD</strong></td><td>TB</td><td>~10,000-100,000 ns</td><td>Pedir algo de China: horas</td></tr>
    </table>

    <div class="warn">
      Cuando tu CPU necesita un dato que NO est√° en cache (un <strong>cache miss</strong>), espera ~200 ciclos para traerlo de RAM. En esos 200 ciclos podr√≠a haber hecho 200 multiplicaciones de punto flotante. <strong>Un programa con muchos cache misses puede estar ejecutando a un 5% de la capacidad real de tu CPU.</strong>
    </div>

    <h3>üì¶ Cache lines: la unidad m√≠nima de memoria</h3>

    <p>La CPU nunca trae "un solo n√∫mero" de RAM. Siempre trae un bloque llamado <strong>cache line</strong>, t√≠picamente de <strong>64 bytes</strong>. Si trabajas con <code>float64</code> (8 bytes cada uno), una cache line trae 8 valores consecutivos de golpe.</p>

    <p>Esto tiene una consecuencia brutal:</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="cm"># Imagina una matriz 1000x1000 de float64 en C-order (row-major)</span>
<span class="cm"># En memoria, se almacena as√≠:</span>
<span class="cm"># [fila0_col0, fila0_col1, fila0_col2, ..., fila1_col0, fila1_col1, ...]</span>
<span class="cm">#  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
<span class="cm">#  Estos 8 valores est√°n en la MISMA cache line</span>

matrix = np.random.rand(<span class="num">1000</span>, <span class="num">1000</span>)

<span class="cm"># ‚úÖ ACCESO POR FILAS: aprovecha la cache line completa</span>
<span class="cm"># Cuando lees matrix[0, 0], la CPU trae matrix[0, 0:8] gratis</span>
<span class="cm"># El siguiente acceso matrix[0, 1] YA EST√Å EN CACHE ‚Üí 0 espera</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_row_major</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):      <span class="cm"># filas (dimensi√≥n externa)</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):  <span class="cm"># columnas (dimensi√≥n interna)</span>
            total += matrix[i, j]          <span class="cm"># ‚úÖ Acceso contiguo en memoria</span>
    <span class="kw">return</span> total

<span class="cm"># ‚ùå ACCESO POR COLUMNAS: destruye la cache</span>
<span class="cm"># Cuando lees matrix[0, 0], la CPU trae fila0_col0..col7</span>
<span class="cm"># Pero el siguiente acceso es matrix[1, 0] ‚Üí OTRA cache line ‚Üí cache miss</span>
<span class="cm"># Cada acceso salta 1000 float64 (8000 bytes) ‚Üí 125 cache lines de distancia</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_col_major</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):  <span class="cm"># columnas primero</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):  <span class="cm"># filas despu√©s</span>
            total += matrix[i, j]          <span class="cm"># ‚ùå Saltos de 8KB entre accesos</span>
    <span class="kw">return</span> total

<span class="cm"># Benchmark real en una matriz 4000x4000:</span>
<span class="cm"># sum_row_major: ~12ms  (cache hits: ~98%)</span>
<span class="cm"># sum_col_major: ~85ms  (cache hits: ~12%)</span>
<span class="cm"># ¬°~7x m√°s lento con EXACTAMENTE las mismas operaciones matem√°ticas!</span></code></pre>

    <div class="perf">
      <strong>Regla de oro:</strong> En C-order (default de NumPy), el loop m√°s interno siempre debe recorrer la <strong>√∫ltima dimensi√≥n</strong> (columnas en 2D). En Fortran-order, el loop m√°s interno recorre la <strong>primera dimensi√≥n</strong>. Si ves un loop donde el √≠ndice externo es el que var√≠a m√°s r√°pido, tu c√≥digo tiene un problema serio de cache.
    </div>

    <!-- ============ C-ORDER vs F-ORDER ============ -->
    <h3>üîÑ C-order vs Fortran-order: elige seg√∫n tu patr√≥n de acceso</h3>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># C-ORDER (row-major): filas son contiguas</span>
<span class="cm"># Memoria: [f0c0, f0c1, f0c2, f1c0, f1c1, f1c2, ...]</span>
c_matrix = np.zeros((<span class="num">1000</span>, <span class="num">1000</span>), order=<span class="st">'C'</span>)  <span class="cm"># default de NumPy</span>

<span class="cm"># FORTRAN-ORDER (column-major): columnas son contiguas</span>
<span class="cm"># Memoria: [f0c0, f1c0, f2c0, f0c1, f1c1, f2c1, ...]</span>
f_matrix = np.zeros((<span class="num">1000</span>, <span class="num">1000</span>), order=<span class="st">'F'</span>)
<span class="cm"># Equivalente en Numba: np.zeros((1000, 1000)[::-1]).T</span>

<span class="cm"># ¬øCU√ÅNDO USAR CADA UNO?</span>

<span class="cm"># Caso 1: Procesar datos tabulares (filas = registros, columnas = features)</span>
<span class="cm"># Si accedes fila por fila ‚Üí C-order ‚úÖ</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_registros</span>(data):  <span class="cm"># data es (n_registros, n_features) C-order</span>
    n = data.shape[<span class="num">0</span>]
    result = np.empty(n)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        <span class="cm"># Acceder a data[i, :] es contiguo en C-order ‚úÖ</span>
        result[i] = np.sum(data[i, :])
    <span class="kw">return</span> result

<span class="cm"># Caso 2: Series temporales (filas = tiempo, columnas = sensores)</span>
<span class="cm"># Si accedes columna por columna ‚Üí Fortran-order ‚úÖ</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_sensores</span>(data):  <span class="cm"># data es (n_tiempos, n_sensores) F-order</span>
    n_sensors = data.shape[<span class="num">1</span>]
    result = np.empty(n_sensors)
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(n_sensors):
        <span class="cm"># Acceder a data[:, j] es contiguo en F-order ‚úÖ</span>
        result[j] = np.mean(data[:, j])
    <span class="kw">return</span> result</code></pre>

    <h4>Verificar el layout de tus arrays</h4>
<pre><code>arr = np.zeros((<span class="num">100</span>, <span class="num">100</span>))
<span class="fn">print</span>(arr.flags)
<span class="cm">#   C_CONTIGUOUS : True    ‚Üê C-order</span>
<span class="cm">#   F_CONTIGUOUS : False</span>

<span class="cm"># Strides te dicen cu√°ntos bytes hay entre elementos consecutivos</span>
<span class="fn">print</span>(arr.strides)
<span class="cm"># (800, 8) ‚Üí saltar una fila = 800 bytes, saltar una columna = 8 bytes</span>
<span class="cm"># El stride m√°s peque√±o (8) es la dimensi√≥n contigua ‚Üí √∫ltima dim (columnas)</span>

<span class="cm"># ‚ö†Ô∏è CUIDADO: operaciones que destruyen la contig√ºidad</span>
sliced = arr[::2, :]  <span class="cm"># Slice con step</span>
<span class="fn">print</span>(sliced.flags.c_contiguous)  <span class="cm"># False ‚Äî ya no es contiguo</span>
<span class="fn">print</span>(sliced.strides)             <span class="cm"># (1600, 8) ‚Äî stride duplicado</span>

transposed = arr.T
<span class="fn">print</span>(transposed.flags.c_contiguous)  <span class="cm"># False</span>
<span class="fn">print</span>(transposed.flags.f_contiguous)  <span class="cm"># True ‚Äî ahora es F-order</span></code></pre>

    <h4>Forzar contig√ºidad cuando la necesitas</h4>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_seguro</span>(data):
    <span class="cm"># Si data puede llegar no-contiguo (ej: un slice), haz copia contigua</span>
    <span class="kw">if not</span> data.flags.c_contiguous:
        data = np.ascontiguousarray(data)  <span class="cm"># Copia, pero ahora es C-contiguo</span>

    <span class="cm"># NOTA: np.ascontiguousarray no est√° en Numba nopython mode</span>
    <span class="cm"># Alternativa dentro de @njit:</span>
    data_copy = data.copy()  <span class="cm"># .copy() devuelve C-contiguous por defecto</span>
    <span class="kw">return</span> data_copy</code></pre>

    <div class="tip">
      En las signatures de Numba puedes <strong>exigir</strong> un layout espec√≠fico:<br>
      <code>types.Array(types.float64, 2, 'C')</code> ‚Üí solo acepta C-contiguous<br>
      <code>types.Array(types.float64, 2, 'A')</code> ‚Üí acepta cualquier layout<br>
      Si declaras <code>'C'</code> y Numba sabe que el array es contiguo, puede generar c√≥digo m√°s agresivamente optimizado.
    </div>

    <!-- ============ PRE-ALOCACI√ìN ============ -->
    <h3>üè≠ Pre-alocaci√≥n: nunca aloquemos dentro del loop caliente</h3>

    <p>Cada <code>np.zeros()</code>, <code>np.empty()</code>, o <code>np.array()</code> dentro de un loop pide memoria al sistema operativo. Esto implica: syscall al kernel, posible page fault, invalidaci√≥n de cache, y trabajo para el garbage collector. En un loop que se ejecuta millones de veces, esto es devastador.</p>

<pre><code><span class="cm"># ‚ùå TERRIBLE: aloca memoria en CADA iteraci√≥n</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_terrible</span>(dataset, n_iter):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        temp = np.zeros(<span class="num">1000</span>)      <span class="cm"># ‚ùå Alocaci√≥n + inicializaci√≥n √ó n_iter veces</span>
        result = np.empty(<span class="num">1000</span>)    <span class="cm"># ‚ùå Otra alocaci√≥n √ó n_iter veces</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1000</span>):
            temp[j] = dataset[i, j] ** <span class="num">2</span>
            result[j] = np.sqrt(temp[j])
    <span class="kw">return</span> result

<span class="cm"># ‚úÖ CORRECTO: pre-aloca FUERA del loop, reutiliza buffers</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_optimo</span>(dataset, n_iter):
    temp = np.empty(<span class="num">1000</span>)        <span class="cm"># ‚úÖ Se aloca UNA sola vez</span>
    result = np.empty(<span class="num">1000</span>)      <span class="cm"># ‚úÖ Se aloca UNA sola vez</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1000</span>):
            temp[j] = dataset[i, j] ** <span class="num">2</span>  <span class="cm"># Reutiliza el buffer</span>
            result[j] = np.sqrt(temp[j])
    <span class="kw">return</span> result
<span class="cm"># Con n_iter=100000: terrible ~8.2s vs √≥ptimo ~1.1s ‚Üí 7.5x m√°s r√°pido</span></code></pre>

    <h4>Allocation hoisting: Numba lo intenta, pero no siempre puede</h4>
<pre><code><span class="cm"># Numba con parallel=True INTENTA hoistear alocaciones autom√°ticamente:</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">auto_hoist</span>(n):
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        temp = np.zeros((<span class="num">50</span>, <span class="num">50</span>))  <span class="cm"># Numba intenta sacar esto fuera del loop</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">50</span>):
            temp[j, j] = i

<span class="cm"># Internamente, Numba transforma esto a:</span>
<span class="cm"># temp = np.empty((50, 50))        ‚Üê alocaci√≥n hoisted fuera del loop</span>
<span class="cm"># for i in prange(n):</span>
<span class="cm">#     temp[:] = 0                   ‚Üê solo la inicializaci√≥n queda dentro</span>
<span class="cm">#     for j in range(50):</span>
<span class="cm">#         temp[j, j] = i</span>

<span class="cm"># PERO: solo funciona con np.empty y np.zeros</span>
<span class="cm"># Alocaciones m√°s complejas o condicionales NO se hoistean</span>
<span class="cm"># REGLA: no conf√≠es en el hoisting autom√°tico. Pre-aloca t√∫ mismo.</span></code></pre>

    <!-- ============ EVITAR TEMPORALES ============ -->
    <h3>üö´ Eliminar arrays temporales innecesarios</h3>

    <p>Una de las ventajas m√°s grandes de Numba sobre NumPy puro es que puedes eliminar arrays temporales. En NumPy vectorizado, cada operaci√≥n intermedia crea un array temporal:</p>

<pre><code><span class="cm"># Con NumPy puro ‚Äî CADA operaci√≥n crea un array temporal en RAM:</span>
<span class="kw">def</span> <span class="fn">numpy_style</span>(x):
    temp1 = np.sin(x)           <span class="cm"># temp1: 80MB si x tiene 10M float64</span>
    temp2 = temp1 ** <span class="num">2</span>          <span class="cm"># temp2: otros 80MB</span>
    temp3 = np.cos(x)           <span class="cm"># temp3: otros 80MB</span>
    temp4 = temp3 ** <span class="num">2</span>          <span class="cm"># temp4: otros 80MB</span>
    result = temp1 + temp3      <span class="cm"># result: otros 80MB</span>
    <span class="kw">return</span> result
    <span class="cm"># Total: ~400MB de temporales para un input de 80MB</span>
    <span class="cm"># Cada temporal destruye la cache y causa page faults</span>

<span class="cm"># Con Numba ‚Äî CERO temporales, todo en registros/cache:</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">numba_fused</span>(x):
    n = <span class="fn">len</span>(x)
    result = np.empty(n)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        <span class="cm"># Cada valor se calcula y se guarda INMEDIATAMENTE</span>
        <span class="cm"># sin crear arrays intermedios</span>
        s = np.sin(x[i])
        c = np.cos(x[i])
        result[i] = s ** <span class="num">2</span> + c ** <span class="num">2</span>
        <span class="cm"># s y c viven en registros de CPU, no en RAM</span>
    <span class="kw">return</span> result
    <span class="cm"># Total: 0 bytes de temporales. Solo input + output.</span>

<span class="cm"># Con 10M elementos:</span>
<span class="cm"># numpy_style:  ~650ms (memory Bandwidth (Ancho de banda) limited)</span>
<span class="cm"># numba_fused:  ~180ms (compute limited ‚Äî que es lo que queremos)</span></code></pre>

    <div class="perf">
      <strong>Patr√≥n fundamental:</strong> Fusiona operaciones que NumPy har√≠a en pasos separados en un solo loop. Esto mantiene los datos en cache L1/L2 durante todo el c√°lculo. Es la raz√≥n #1 por la que Numba puede superar a NumPy incluso en operaciones vectorizadas.
    </div>

    <!-- ============ CHUNKING ============ -->
    <h3>üß© Chunking: procesamiento por bloques que caben en cache</h3>

    <p>Cuando tu dataset es enorme (cientos de MB o GB), ni siquiera un loop fila-por-fila es √≥ptimo. El truco es procesar en <strong>bloques que quepan en cache L2/L3</strong>.</p>

<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_naive</span>(matrix):
    <span class="cm">"""Suma de todos los elementos ‚Äî naive."""</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j]
    <span class="kw">return</span> total
<span class="cm"># Para matrices peque√±as, esto est√° bien.</span>
<span class="cm"># Para matrices de 10000x10000+, la cache L2 (256KB) no puede</span>
<span class="cm"># contener una fila entera si es muy ancha.</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">sum_blocked</span>(matrix, block_size=<span class="num">64</span>):
    <span class="cm">"""Suma con blocking: procesa sub-matrices que caben en cache L2."""</span>
    total = <span class="num">0.0</span>
    rows, cols = matrix.shape

    <span class="cm"># Recorre por bloques de block_size √ó block_size</span>
    <span class="kw">for</span> bi <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, rows, block_size):
        <span class="kw">for</span> bj <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, cols, block_size):
            <span class="cm"># Este bloque (64√ó64 float64 = 32KB) cabe en L1 cache</span>
            bi_end = <span class="fn">min</span>(bi + block_size, rows)
            bj_end = <span class="fn">min</span>(bj + block_size, cols)
            <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(bi, bi_end):
                <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(bj, bj_end):
                    total += matrix[i, j]
    <span class="kw">return</span> total

<span class="cm"># Ejemplo cl√°sico donde blocking es CR√çTICO: multiplicaci√≥n de matrices</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">matmul_blocked</span>(A, B, block_size=<span class="num">64</span>):
    <span class="cm">"""Multiplicaci√≥n de matrices con cache blocking.
    Para matrices grandes, puede ser 3-5x m√°s r√°pido que el naive."""</span>
    M, K = A.shape
    K2, N = B.shape
    C = np.zeros((M, N))

    <span class="kw">for</span> bi <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, M, block_size):
        <span class="kw">for</span> bj <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, N, block_size):
            <span class="kw">for</span> bk <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, K, block_size):
                <span class="cm"># Multiplicar sub-bloques que caben en cache</span>
                bi_end = <span class="fn">min</span>(bi + block_size, M)
                bj_end = <span class="fn">min</span>(bj + block_size, N)
                bk_end = <span class="fn">min</span>(bk + block_size, K)
                <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(bi, bi_end):
                    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(bj, bj_end):
                        temp = <span class="num">0.0</span>
                        <span class="kw">for</span> k <span class="kw">in</span> <span class="fn">range</span>(bk, bk_end):
                            temp += A[i, k] * B[k, j]
                        C[i, j] += temp
    <span class="kw">return</span> C</code></pre>

    <h4>¬øC√≥mo elegir el tama√±o del bloque?</h4>
    <table>
      <tr><th>Cache objetivo</th><th>Tama√±o t√≠pico</th><th>block_size para float64</th><th>C√°lculo</th></tr>
      <tr><td>L1 (32KB)</td><td>32,768 bytes</td><td><strong>64√ó64</strong></td><td>64√ó64√ó8 = 32,768 bytes</td></tr>
      <tr><td>L2 (256KB)</td><td>262,144 bytes</td><td><strong>128√ó128</strong></td><td>128√ó128√ó8 = 131,072 (dejas espacio para otros datos)</td></tr>
      <tr><td>L3 (8MB)</td><td>8,388,608 bytes</td><td><strong>512√ó512</strong></td><td>Para datasets gigantes con poca reutilizaci√≥n</td></tr>
    </table>

    <div class="info">
      La regla general: el bloque completo (3 sub-matrices para matmul: bloque de A + bloque de B + bloque de C) debe caber en el cache que est√°s apuntando. Para L1 targeting con matmul: <code>3 √ó block_size¬≤ √ó 8 bytes < 32KB</code>, as√≠ que <code>block_size ‚âà 36</code>. En la pr√°ctica, <strong>potencias de 2 entre 32 y 128</strong> suelen funcionar bien. Experimenta con benchmarking.
    </div>

    <!-- ============ PATRONES DE ACCESO ============ -->
    <h3>üîÅ Patrones de acceso: de terrible a √≥ptimo</h3>

<pre><code><span class="cm"># Escenario: Transponer y procesar una matriz grande</span>
<span class="cm"># Datos: 5000√ó5000 de float64 (200MB)</span>

<span class="cm"># ‚ùå NIVEL 1 ‚Äî TERRIBLE: acceso aleatorio</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">terrible</span>(matrix, indices):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> idx <span class="kw">in</span> indices:  <span class="cm"># indices aleatorios ‚Üí cache miss garantizado</span>
        total += matrix.flat[idx]
    <span class="kw">return</span> total

<span class="cm"># ‚ùå NIVEL 2 ‚Äî MALO: stride grande (acceso por columnas en C-order)</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">malo</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
            total += matrix[i, j]  <span class="cm"># stride = n_cols √ó 8 bytes</span>
    <span class="kw">return</span> total

<span class="cm"># üÜó NIVEL 3 ‚Äî DECENTE: acceso secuencial contiguo</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">decente</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">0</span>]):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j]  <span class="cm"># stride = 8 bytes (contiguo)</span>
    <span class="kw">return</span> total

<span class="cm"># ‚úÖ NIVEL 4 ‚Äî BUENO: contiguo + sin temporales + parallel</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">bueno</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(matrix.shape[<span class="num">0</span>]):
        row_sum = <span class="num">0.0</span>  <span class="cm"># acumulador local ‚Üí vive en registro</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            row_sum += matrix[i, j]
        total += row_sum  <span class="cm"># reducci√≥n paralela segura</span>
    <span class="kw">return</span> total

<span class="cm"># ‚ö° NIVEL 5 ‚Äî √ìPTIMO: blocking + parallel + fastmath + SVML</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">optimo</span>(matrix, block_size=<span class="num">64</span>):
    total = <span class="num">0.0</span>
    rows, cols = matrix.shape
    <span class="kw">for</span> bi <span class="kw">in</span> prange(<span class="num">0</span>, rows, block_size):  <span class="cm"># bloques en paralelo</span>
        block_sum = <span class="num">0.0</span>
        bi_end = <span class="fn">min</span>(bi + block_size, rows)
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(bi, bi_end):
            <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(cols):
                block_sum += matrix[i, j]
        total += block_sum
    <span class="kw">return</span> total

<span class="cm"># Rendimiento relativo (5000√ó5000 float64):</span>
<span class="cm"># terrible: ~850ms   (baseline)</span>
<span class="cm"># malo:     ~340ms   (2.5x m√°s r√°pido)</span>
<span class="cm"># decente:  ~48ms    (17x m√°s r√°pido)</span>
<span class="cm"># bueno:    ~12ms    (70x m√°s r√°pido)</span>
<span class="cm"># √≥ptimo:   ~6ms     (140x m√°s r√°pido)</span></code></pre>

    <!-- ============ STRUCT OF ARRAYS ============ -->
    <h3>üìê Structure of Arrays (SoA) vs Array of Structures (AoS)</h3>

    <p>Este es un patr√≥n de dise√±o de datos que puede cambiar el rendimiento dram√°ticamente. Supongamos que tienes 1 mill√≥n de part√≠culas con posici√≥n (x, y, z) y velocidad (vx, vy, vz):</p>

<pre><code><span class="cm"># ‚ùå ARRAY OF STRUCTURES (AoS) ‚Äî Natural pero lento para procesamiento masivo</span>
<span class="cm"># Cada part√≠cula es una "fila" con todos sus atributos juntos</span>
<span class="cm"># Memoria: [x0,y0,z0,vx0,vy0,vz0, x1,y1,z1,vx1,vy1,vz1, ...]</span>
particles_aos = np.zeros((<span class="num">1_000_000</span>, <span class="num">6</span>))  <span class="cm"># columnas: x,y,z,vx,vy,vz</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">update_positions_aos</span>(particles, dt):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(particles.shape[<span class="num">0</span>]):
        <span class="cm"># Para actualizar x, la CPU trae una cache line que contiene</span>
        <span class="cm"># [x0, y0, z0, vx0, vy0, vz0, x1, y1] ‚Äî solo usa x0 y vx0</span>
        <span class="cm"># 6 de cada 8 valores en la cache line son BASURA para esta operaci√≥n</span>
        particles[i, <span class="num">0</span>] += particles[i, <span class="num">3</span>] * dt  <span class="cm"># x += vx * dt</span>
        particles[i, <span class="num">1</span>] += particles[i, <span class="num">4</span>] * dt  <span class="cm"># y += vy * dt</span>
        particles[i, <span class="num">2</span>] += particles[i, <span class="num">5</span>] * dt  <span class="cm"># z += vz * dt</span>

<span class="cm"># ‚úÖ STRUCTURE OF ARRAYS (SoA) ‚Äî Cada atributo en su propio array</span>
<span class="cm"># Memoria de x: [x0, x1, x2, x3, x4, x5, x6, x7, ...]</span>
<span class="cm"># Memoria de vx: [vx0, vx1, vx2, vx3, vx4, vx5, vx6, vx7, ...]</span>
n = <span class="num">1_000_000</span>
x  = np.zeros(n)
y  = np.zeros(n)
z  = np.zeros(n)
vx = np.random.randn(n)
vy = np.random.randn(n)
vz = np.random.randn(n)

<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">update_positions_soa</span>(x, y, z, vx, vy, vz, dt):
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(x)):
        <span class="cm"># La CPU trae [x0, x1, x2, x3, x4, x5, x6, x7]</span>
        <span class="cm"># TODOS los 8 valores en la cache line son √∫tiles</span>
        <span class="cm"># Adem√°s, LLVM puede usar instrucciones SIMD (procesar 4 a la vez)</span>
        x[i] += vx[i] * dt
        y[i] += vy[i] * dt
        z[i] += vz[i] * dt

<span class="cm"># Benchmarks con 1M part√≠culas:</span>
<span class="cm"># AoS: ~4.8ms</span>
<span class="cm"># SoA: ~0.6ms ‚Üí 8x m√°s r√°pido con los mismos c√°lculos</span></code></pre>

    <div class="perf">
      <strong>¬øCu√°ndo usar SoA?</strong> Siempre que proceses <strong>un atributo a la vez</strong> sobre muchas entidades (actualizar todas las posiciones, calcular todas las energ√≠as, etc.). <strong>¬øCu√°ndo usar AoS?</strong> Cuando proceses <strong>todos los atributos de una entidad</strong> juntos y esa entidad sea peque√±a (cabe en pocas cache lines).
    </div>

    <!-- ============ RESUMEN VISUAL ============ -->
    <h3>üìã Checklist de optimizaci√≥n de memoria</h3>
    <table>
      <tr><th>#</th><th>Verificar</th><th>C√≥mo</th></tr>
      <tr><td>1</td><td>¬øLoop interno recorre dimensi√≥n contigua?</td><td>√öltima dim en C-order, primera en F-order</td></tr>
      <tr><td>2</td><td>¬øAloco arrays dentro de loops calientes?</td><td>Mover <code>np.empty/zeros</code> fuera del loop</td></tr>
      <tr><td>3</td><td>¬øCreo arrays temporales innecesarios?</td><td>Fusionar operaciones en un solo loop</td></tr>
      <tr><td>4</td><td>¬øMis arrays son contiguos?</td><td>Verificar <code>arr.flags.c_contiguous</code></td></tr>
      <tr><td>5</td><td>¬øMi dataset cabe en cache?</td><td>Si no ‚Üí usar blocking con tama√±o < L2</td></tr>
      <tr><td>6</td><td>¬øEstructura de datos apropiada?</td><td>SoA si proceso un atributo a la vez sobre muchas entidades</td></tr>
      <tr><td>7</td><td>¬øAcceso aleatorio a arrays grandes?</td><td>Reordenar datos o usar √≠ndices sorted</td></tr>
      <tr><td>8</td><td>¬øUso <code>parallel=True</code> en ops de arrays?</td><td>Divide el trabajo en cache-friendly chunks entre cores</td></tr>
    </table>

  </div>
</div>

<!-- ==================== M√ìDULO BENCHMARKING ==================== -->
<a href="#m-bench">üî¨ Benchmarking: Medir Sin Mentirte</a>
<div class="module" id="m-bench">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">üî¨</span> Benchmarking: Medir Sin Mentirte</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Sin benchmarking correcto, todas las optimizaciones anteriores son adivinanzas. La mayor√≠a de la gente benchmarkea Numba mal, obtiene n√∫meros incorrectos, y toma decisiones equivocadas. Esta secci√≥n te ense√±a a medir de verdad.</p>

    <!-- ============ ERROR #1: COMPILACI√ìN ============ -->
    <h3>üö® Error #1: Medir el tiempo de compilaci√≥n</h3>

    <p>El error m√°s com√∫n y m√°s grave. La primera llamada a una funci√≥n Numba incluye el tiempo de compilaci√≥n (0.1s - 5s). Si mides eso, tus benchmarks no significan nada.</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit
<span class="kw">import</span> time

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">mi_funcion</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))

data = np.random.rand(<span class="num">1_000_000</span>)

<span class="cm"># ‚ùå MAL: incluye compilaci√≥n</span>
start = time.time()
mi_funcion(data)
<span class="fn">print</span>(f<span class="st">"Tiempo: {time.time() - start:.4f}s"</span>)
<span class="cm"># Resultado: "Tiempo: 0.8523s" ‚Üê ¬°MENTIRA! 0.85s son de compilaci√≥n</span>

<span class="cm"># ‚úÖ BIEN: calentar primero, luego medir</span>
mi_funcion(data)  <span class="cm"># ‚Üê WARMUP: primera llamada compila (descartar)</span>

start = time.time()
<span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">100</span>):
    mi_funcion(data)
elapsed = (time.time() - start) / <span class="num">100</span>
<span class="fn">print</span>(f<span class="st">"Tiempo real: {elapsed*1000:.2f}ms"</span>)
<span class="cm"># Resultado: "Tiempo real: 1.23ms" ‚Üê La verdad</span></code></pre>

    <div class="warn">
      <strong>SIEMPRE</strong> haz al menos una llamada de warmup antes de medir. Si est√°s midiendo diferentes funciones para comparar, haz warmup de TODAS antes de empezar a medir CUALQUIERA.
    </div>

    <!-- ============ TEMPLATE DE BENCHMARK ============ -->
    <h3>üß™ Template de benchmarking profesional</h3>

    <p>Este es el patr√≥n que debes usar siempre. C√≥pialo y ad√°ptalo:</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit, prange
<span class="kw">import</span> time
<span class="kw">import</span> sys

<span class="kw">def</span> <span class="fn">benchmark</span>(func, *args, n_warmup=<span class="num">3</span>, n_runs=<span class="num">50</span>, label=<span class="st">""</span>):
    <span class="cm">"""Benchmark profesional: warmup + m√∫ltiples runs + estad√≠sticas."""</span>

    <span class="cm"># 1. WARMUP: compilar + estabilizar cache</span>
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_warmup):
        func(*args)

    <span class="cm"># 2. MEDIR: m√∫ltiples ejecuciones</span>
    times = []
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_runs):
        start = time.perf_counter()  <span class="cm"># ‚Üê perf_counter, NO time.time()</span>
        result = func(*args)
        elapsed = time.perf_counter() - start
        times.append(elapsed)

    <span class="cm"># 3. ANALIZAR: no solo la media</span>
    times = np.array(times)
    <span class="fn">print</span>(f<span class="st">"{'‚îÄ'*50}"</span>)
    <span class="fn">print</span>(f<span class="st">"  {label or func.__name__}"</span>)
    <span class="fn">print</span>(f<span class="st">"{'‚îÄ'*50}"</span>)
    <span class="fn">print</span>(f<span class="st">"  Mediana:   {np.median(times)*1000:10.3f} ms"</span>)  <span class="cm"># ‚Üê MEDIANA, no media</span>
    <span class="fn">print</span>(f<span class="st">"  Media:     {np.mean(times)*1000:10.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"  Min:       {np.min(times)*1000:10.3f} ms"</span>)  <span class="cm"># ‚Üê mejor caso real</span>
    <span class="fn">print</span>(f<span class="st">"  Max:       {np.max(times)*1000:10.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"  Std:       {np.std(times)*1000:10.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"  Runs:      {n_runs}"</span>)
    <span class="fn">print</span>()
    <span class="kw">return</span> np.median(times)

<span class="cm"># USO:</span>
data = np.random.rand(<span class="num">10_000_000</span>)

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">version_a</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))

<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">version_b</span>(x):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(x)):
        total += np.sqrt(x[i])
    <span class="kw">return</span> total

t_a = benchmark(version_a, data, label=<span class="st">"@njit b√°sico"</span>)
t_b = benchmark(version_b, data, label=<span class="st">"@njit parallel+fastmath"</span>)
t_np = benchmark(np.sum, np.sqrt(data), label=<span class="st">"NumPy puro"</span>)

<span class="fn">print</span>(f<span class="st">"Speedup B vs A: {t_a/t_b:.1f}x"</span>)
<span class="fn">print</span>(f<span class="st">"Speedup B vs NumPy: {t_np/t_b:.1f}x"</span>)</code></pre>

    <h4>¬øPor qu√© mediana y no media?</h4>
    <p>Porque los outliers de tiempo alto (causados por garbage collection, interrupciones del OS, context switches) distorsionan la media. La mediana te dice "en la mitad de los casos tard√≥ menos que X" ‚Äî mucho m√°s representativo del rendimiento real.</p>

    <h4>¬øPor qu√© <code>time.perf_counter()</code> y no <code>time.time()</code>?</h4>
    <p><code>perf_counter</code> usa el reloj monot√≥nico de mayor resoluci√≥n disponible en el sistema (nanosegundos). <code>time.time()</code> puede tener resoluci√≥n de milisegundos y es afectado por ajustes del reloj del sistema.</p>

    <!-- ============ MEDIR COMPILACI√ìN ============ -->
    <h3>‚è±Ô∏è Medir el tiempo de compilaci√≥n por separado</h3>

    <p>A veces necesitas saber cu√°nto tarda Numba en compilar (para decidir si usar <code>cache=True</code>):</p>

<pre><code><span class="kw">def</span> <span class="fn">benchmark_compilation</span>(func, *args):
    <span class="cm">"""Mide el tiempo de compilaci√≥n vs ejecuci√≥n por separado."""</span>

    <span class="cm"># Forzar recompilaci√≥n</span>
    func.recompile()  <span class="cm"># Limpia todas las especializaciones</span>

    <span class="cm"># Medir primera llamada (compilaci√≥n + ejecuci√≥n)</span>
    start = time.perf_counter()
    func(*args)
    first_call = time.perf_counter() - start

    <span class="cm"># Medir segunda llamada (solo ejecuci√≥n)</span>
    start = time.perf_counter()
    func(*args)
    second_call = time.perf_counter() - start

    compilation = first_call - second_call

    <span class="fn">print</span>(f<span class="st">"Compilaci√≥n:  {compilation*1000:.1f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"Ejecuci√≥n:    {second_call*1000:.3f} ms"</span>)
    <span class="fn">print</span>(f<span class="st">"Ratio:        compilaci√≥n es {compilation/second_call:.0f}x la ejecuci√≥n"</span>)

    <span class="cm"># Si la ratio es > 1000, cache=True es casi obligatorio</span>
    <span class="kw">if</span> compilation / max(second_call, <span class="num">1e-9</span>) > <span class="num">1000</span>:
        <span class="fn">print</span>(<span class="st">"  ‚Üí RECOMENDACI√ìN: usa cache=True"</span>)</code></pre>

    <!-- ============ SCALING TEST ============ -->
    <h3>üìà Scaling test: ¬øtu optimizaci√≥n escala?</h3>

    <p>Un benchmark con un solo tama√±o de datos es incompleto. Las optimizaciones se comportan diferente seg√∫n la escala:</p>

<pre><code><span class="kw">def</span> <span class="fn">scaling_test</span>(funcs, sizes, labels=<span class="num">None</span>, gen_data=<span class="num">None</span>):
    <span class="cm">"""Compara funciones en m√∫ltiples tama√±os de datos.
    Revela si una optimizaci√≥n escala o tiene overhead fijo."""</span>

    <span class="kw">if</span> gen_data <span class="kw">is</span> <span class="num">None</span>:
        gen_data = <span class="kw">lambda</span> n: np.random.rand(n)
    <span class="kw">if</span> labels <span class="kw">is</span> <span class="num">None</span>:
        labels = [f.__name__ <span class="kw">for</span> f <span class="kw">in</span> funcs]

    <span class="cm"># Warmup todas las funciones con el tama√±o m√°s peque√±o</span>
    small_data = gen_data(sizes[<span class="num">0</span>])
    <span class="kw">for</span> func <span class="kw">in</span> funcs:
        func(small_data)

    <span class="fn">print</span>(f<span class="st">"\n{'Tama√±o':>12}"</span>, end=<span class="st">""</span>)
    <span class="kw">for</span> label <span class="kw">in</span> labels:
        <span class="fn">print</span>(f<span class="st">"{label:>15}"</span>, end=<span class="st">""</span>)
    <span class="fn">print</span>(f<span class="st">"{'Speedup':>12}"</span>)
    <span class="fn">print</span>(<span class="st">"‚îÄ"</span> * (<span class="num">12</span> + <span class="num">15</span> * <span class="fn">len</span>(funcs) + <span class="num">12</span>))

    <span class="kw">for</span> size <span class="kw">in</span> sizes:
        data = gen_data(size)
        times = []
        <span class="kw">for</span> func <span class="kw">in</span> funcs:
            t = []
            <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">20</span>):
                start = time.perf_counter()
                func(data)
                t.append(time.perf_counter() - start)
            times.append(np.median(t))

        <span class="fn">print</span>(f<span class="st">"{size:>12,}"</span>, end=<span class="st">""</span>)
        <span class="kw">for</span> t <span class="kw">in</span> times:
            <span class="fn">print</span>(f<span class="st">"{t*1000:>14.3f}ms"</span>, end=<span class="st">""</span>)
        speedup = times[<span class="num">0</span>] / times[<span class="num">-1</span>] <span class="kw">if</span> times[<span class="num">-1</span>] > <span class="num">0</span> <span class="kw">else</span> float(<span class="st">'inf'</span>)
        <span class="fn">print</span>(f<span class="st">"{speedup:>11.1f}x"</span>)

<span class="cm"># Uso:</span>
scaling_test(
    funcs=[version_a, version_b],
    sizes=[<span class="num">1_000</span>, <span class="num">10_000</span>, <span class="num">100_000</span>, <span class="num">1_000_000</span>, <span class="num">10_000_000</span>],
    labels=[<span class="st">"njit"</span>, <span class="st">"parallel+fast"</span>]
)

<span class="cm"># Output t√≠pico:</span>
<span class="cm">#      Tama√±o           njit  parallel+fast     Speedup</span>
<span class="cm"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span class="cm">#        1,000        0.002ms        0.045ms       0.0x  ‚Üê parallel PIERDE en datos chicos</span>
<span class="cm">#       10,000        0.012ms        0.048ms       0.3x  ‚Üê todav√≠a pierde</span>
<span class="cm">#      100,000        0.108ms        0.065ms       1.7x  ‚Üê empieza a ganar</span>
<span class="cm">#    1,000,000        1.052ms        0.342ms       3.1x  ‚Üê gana claramente</span>
<span class="cm">#   10,000,000       10.891ms        2.187ms       5.0x  ‚Üê domina</span></code></pre>

    <div class="info">
      Este test revela algo crucial: <code>parallel=True</code> tiene un overhead fijo de ~0.04ms por crear el thread pool. Con datos peque√±os (<10K), ese overhead domina y la versi√≥n paralela es M√ÅS LENTA. Con datos grandes, el overhead es insignificante y la paralelizaci√≥n domina. <strong>Sin un scaling test, nunca ver√≠as esto.</strong>
    </div>

    <!-- ============ Throughput (Rendimiento real) ============ -->
    <h3>üìä Medir Throughput (Rendimiento real), no solo tiempo</h3>

    <p>El tiempo absoluto no te dice si tu c√≥digo est√° cerca del l√≠mite te√≥rico. El Throughput (Rendimiento real) (GB/s procesados) s√≠:</p>

<pre><code><span class="kw">def</span> <span class="fn">benchmark_Throughput (Rendimiento real)</span>(func, data, n_runs=<span class="num">50</span>, label=<span class="st">""</span>):
    <span class="cm">"""Mide Throughput (Rendimiento real) en GB/s para saber si est√°s cerca del l√≠mite de memoria."""</span>

    <span class="cm"># Warmup</span>
    func(data)
    func(data)

    <span class="cm"># Medir</span>
    times = []
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_runs):
        start = time.perf_counter()
        func(data)
        times.append(time.perf_counter() - start)

    median_time = np.median(times)
    data_bytes = data.nbytes
    Throughput (Rendimiento real)_gbs = (data_bytes / median_time) / <span class="num">1e9</span>

    <span class="fn">print</span>(f<span class="st">"{label:30s} | {median_time*1000:8.3f} ms | {Throughput (Rendimiento real)_gbs:6.1f} GB/s"</span>)

    <span class="cm"># Referencia: Bandwidth (Ancho de banda) de RAM t√≠pica</span>
    <span class="cm"># DDR4-3200: ~25 GB/s por canal, ~50 GB/s dual channel</span>
    <span class="cm"># DDR5-5600: ~45 GB/s por canal, ~90 GB/s dual channel</span>
    <span class="cm"># Si tu Throughput (Rendimiento real) est√° cerca de estos n√∫meros, est√°s</span>
    <span class="cm"># MEMORY BOUND y optimizar la CPU no ayudar√° m√°s.</span>
    <span class="cm"># La √∫nica forma de ir m√°s r√°pido es reducir los datos</span>
    <span class="cm"># que necesitas leer (mejor layout, menos temporales, etc.)</span>

<span class="cm"># Uso:</span>
data = np.random.rand(<span class="num">50_000_000</span>)  <span class="cm"># ~400MB</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">just_sum</span>(x):
    <span class="kw">return</span> np.sum(x)

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">par_sum</span>(x):
    <span class="kw">return</span> np.sum(x)

benchmark_Throughput (Rendimiento real)(just_sum, data, label=<span class="st">"np.sum @njit"</span>)
benchmark_Throughput (Rendimiento real)(par_sum, data, label=<span class="st">"np.sum @njit parallel"</span>)
benchmark_Throughput (Rendimiento real)(np.sum, data, label=<span class="st">"np.sum NumPy puro"</span>)

<span class="cm"># Output t√≠pico:</span>
<span class="cm"># np.sum @njit                  |   17.234 ms |  23.2 GB/s</span>
<span class="cm"># np.sum @njit parallel         |    4.891 ms |  81.8 GB/s  ‚Üê cerca del l√≠mite de RAM</span>
<span class="cm"># np.sum NumPy puro             |   18.103 ms |  22.1 GB/s</span>
<span class="cm">#</span>
<span class="cm"># La versi√≥n parallel a 81.8 GB/s est√° CERCA del l√≠mite de DDR5 (~90 GB/s)</span>
<span class="cm"># No va a ir m√°s r√°pido sin importar qu√© hagas en c√≥digo.</span>
<span class="cm"># ¬°Est√°s memory-bound! La CPU est√° esperando datos de RAM.</span></code></pre>

    <div class="perf">
      <strong>Si tu Throughput (Rendimiento real) se acerca al Bandwidth (Ancho de banda) de tu RAM, est√°s memory-bound.  La mayor√≠a de los programadores optimizan loops cuando el cuello de botella es el bus de memoria.</strong> Ninguna optimizaci√≥n de CPU (fastmath, SVML, mejor algoritmo) te va a ayudar. La √∫nica salida es reducir la cantidad de datos que lees: fusionar operaciones (un solo pass en vez de m√∫ltiples), usar tipos m√°s peque√±os (float32 en vez de float64 duplica tu Throughput (Rendimiento real) efectivo), o reestructurar datos (SoA).
    </div>

    <!-- ============ DIAGNOSTICS ============ -->
    <h3>üîç Verificar qu√© hizo Numba realmente</h3>

    <p>No asumas que Numba hizo lo que esperabas. <strong>Verifica.</strong></p>

<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">mi_funcion</span>(x):
    n = x.shape[<span class="num">0</span>]
    a = np.sin(x)
    b = np.cos(a * a)
    acc = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(n - <span class="num">2</span>):
        <span class="kw">for</span> j <span class="kw">in</span> prange(n - <span class="num">1</span>):
            acc += b[i] + b[j + <span class="num">1</span>]
    <span class="kw">return</span> acc

<span class="cm"># Forzar compilaci√≥n</span>
mi_funcion(np.arange(<span class="num">10</span>, dtype=np.float64))

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 1: inspect_types() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Muestra qu√© tipo infiri√≥ Numba para CADA variable</span>
mi_funcion.inspect_types()
<span class="cm"># Busca sorpresas: ¬øuna variable es float64 cuando esperabas int?</span>
<span class="cm"># ¬øUn array es 'A' (any layout) cuando deber√≠a ser 'C'?</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 2: parallel_diagnostics() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Muestra qu√© se paraleliz√≥, qu√© se fusion√≥, y qu√© NO</span>
mi_funcion.parallel_diagnostics(level=<span class="num">4</span>)
<span class="cm"># Nivel 1: resumen b√°sico</span>
<span class="cm"># Nivel 2: + info de fusi√≥n de loops</span>
<span class="cm"># Nivel 3: + estructura antes/despu√©s de optimizaci√≥n</span>
<span class="cm"># Nivel 4: + hoisting de c√≥digo + instrucciones</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 3: inspect_llvm() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Para expertos: el c√≥digo LLVM IR generado</span>
llvm_ir = mi_funcion.inspect_llvm(mi_funcion.signatures[<span class="num">0</span>])
<span class="cm"># Buscar: "vector.body" ‚Üí LLVM vectoriz√≥ el loop (SIMD)</span>
<span class="cm"># Buscar: "llvm.sqrt.v4f64" ‚Üí est√° usando SIMD de 4 doubles</span>
<span class="cm"># Buscar: "svml" ‚Üí est√° usando Intel SVML</span>
<span class="kw">if</span> <span class="st">'svml'</span> <span class="kw">in</span> llvm_ir.lower():
    <span class="fn">print</span>(<span class="st">"‚úÖ SVML activo"</span>)
<span class="kw">else</span>:
    <span class="fn">print</span>(<span class="st">"‚ö†Ô∏è SVML NO detectado ‚Äî instala intel-cmplr-lib-rt"</span>)

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 4: inspect_asm() ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Para ultra-expertos: el assembly nativo generado</span>
asm = mi_funcion.inspect_asm(mi_funcion.signatures[<span class="num">0</span>])
<span class="cm"># Buscar: "vmulpd" / "vaddpd" ‚Üí instrucciones AVX (256-bit SIMD)</span>
<span class="cm"># Buscar: "zmm" ‚Üí instrucciones AVX-512 (512-bit SIMD)</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ HERRAMIENTA 5: debug de vectorizaci√≥n LLVM ‚îÄ‚îÄ‚îÄ</span>
<span class="kw">import</span> llvmlite.binding <span class="kw">as</span> llvm
llvm.set_option(<span class="st">''</span>, <span class="st">'--debug-only=loop-vectorize'</span>)
<span class="cm"># NOTA: requiere LLVM con assertions (build de desarrollo)</span>
<span class="cm"># Mensajes comunes:</span>
<span class="cm"># "LV: Vectorization is possible but not beneficial" ‚Üí el loop es muy corto</span>
<span class="cm"># "LV: Can't vectorize due to memory conflicts" ‚Üí acceso no contiguo</span>
<span class="cm"># "LV: Not vectorizing: loop did not meet vectorization requirements" ‚Üí dependencia</span></code></pre>

    <!-- ============ COMPARAR CONTRA NUMPY ============ -->
    <h3>‚öñÔ∏è Comparar contra NumPy de forma justa</h3>

<pre><code><span class="cm"># ERROR MUY COM√öN: comparar operaciones diferentes</span>

data = np.random.rand(<span class="num">1_000_000</span>)

<span class="cm"># Esto NO es una comparaci√≥n justa:</span>
<span class="cm"># NumPy:</span>
result_np = np.sum(np.sqrt(data))   <span class="cm"># crea array temporal de sqrt</span>

<span class="cm"># Numba:</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">fused</span>(x):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(x)):
        total += np.sqrt(x[i])  <span class="cm"># sin array temporal</span>
    <span class="kw">return</span> total
<span class="cm"># La versi√≥n Numba es m√°s r√°pida EN PARTE porque evita el temporal,</span>
<span class="cm"># no solo por la compilaci√≥n JIT.</span>

<span class="cm"># COMPARACI√ìN JUSTA: misma operaci√≥n, mismo patr√≥n de memoria</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">numba_same_as_numpy</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))  <span class="cm"># misma operaci√≥n que NumPy</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">numba_fused</span>(x):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(x)):
        total += np.sqrt(x[i])
    <span class="kw">return</span> total

<span class="cm"># Ahora puedes decir:</span>
<span class="cm"># "Numba sin fusi√≥n (misma operaci√≥n que NumPy): 1.2x m√°s r√°pido"</span>
<span class="cm"># "Numba CON fusi√≥n de loops: 3.5x m√°s r√°pido"</span>
<span class="cm"># "De ese 3.5x, 1.2x viene de JIT y 2.9x viene de eliminar temporales"</span></code></pre>

    <!-- ============ FLOAT32 ============ -->
    <h3>üî¢ Truco nuclear: float32 en vez de float64</h3>

<pre><code><span class="cm"># Cambiar de float64 a float32 tiene TRES beneficios simult√°neos:</span>
<span class="cm"># 1. La mitad de bytes ‚Üí el doble de datos caben en cache</span>
<span class="cm"># 2. La mitad de bytes ‚Üí el doble de Throughput (Rendimiento real) de memoria</span>
<span class="cm"># 3. SIMD procesa el doble de elementos por instrucci√≥n</span>
<span class="cm">#    (8 float32 vs 4 float64 en AVX-256)</span>

data64 = np.random.rand(<span class="num">10_000_000</span>).astype(np.float64)  <span class="cm"># 80 MB</span>
data32 = data64.astype(np.float32)                        <span class="cm"># 40 MB</span>

<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process</span>(x):
    result = np.empty_like(x)
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(x)):
        result[i] = np.sin(x[i]) ** <span class="num">2</span> + np.cos(x[i]) ** <span class="num">2</span>
    <span class="kw">return</span> result

benchmark(process, data64, label=<span class="st">"float64 (80MB)"</span>)
benchmark(process, data32, label=<span class="st">"float32 (40MB)"</span>)

<span class="cm"># Resultado t√≠pico:</span>
<span class="cm"># float64:  12.3ms</span>
<span class="cm"># float32:   4.1ms ‚Üí 3x m√°s r√°pido con ~7 d√≠gitos de precisi√≥n (vs ~15)</span>

<span class="cm"># ¬øCu√°ndo es aceptable float32?</span>
<span class="cm"># ‚úÖ Machine Learning (pesos, activaciones, gradientes)</span>
<span class="cm"># ‚úÖ Procesamiento de se√±ales e im√°genes</span>
<span class="cm"># ‚úÖ Simulaciones donde 7 d√≠gitos bastan</span>
<span class="cm"># ‚úÖ Visualizaci√≥n y gr√°ficos</span>
<span class="cm"># ‚ùå C√°lculos financieros precisos</span>
<span class="cm"># ‚ùå √Ålgebra lineal con matrices mal condicionadas</span>
<span class="cm"># ‚ùå Acumulaci√≥n de sumas muy largas (error se acumula)</span></code></pre>

    <!-- ============ CHECKLIST FINAL ============ -->
    <h3>üìã Checklist de benchmarking</h3>
    <table>
      <tr><th>#</th><th>Verificar</th><th>Si no lo haces...</th></tr>
      <tr><td>1</td><td>¬øHice warmup antes de medir?</td><td>Mides compilaci√≥n, no ejecuci√≥n</td></tr>
      <tr><td>2</td><td>¬øUso <code>time.perf_counter()</code>?</td><td>Resoluci√≥n insuficiente para funciones r√°pidas</td></tr>
      <tr><td>3</td><td>¬øM√∫ltiples ejecuciones + mediana?</td><td>GC y OS noise distorsionan un solo run</td></tr>
      <tr><td>4</td><td>¬øProb√© con m√∫ltiples tama√±os de datos?</td><td>Tu "optimizaci√≥n" puede ser m√°s lenta para datos reales</td></tr>
      <tr><td>5</td><td>¬øVerifiqu√© Throughput (Rendimiento real) en GB/s?</td><td>Podr√≠as estar optimizando CPU cuando est√°s memory-bound</td></tr>
      <tr><td>6</td><td>¬øCompar√© operaciones equivalentes?</td><td>Speedup falso por comparar cosas diferentes</td></tr>
      <tr><td>7</td><td>¬øRevis√© <code>parallel_diagnostics()</code>?</td><td>parallel=True podr√≠a no estar haciendo nada</td></tr>
      <tr><td>8</td><td>¬øVerifiqu√© que SVML est√° activo?</td><td>Podr√≠as estar a 3x del rendimiento √≥ptimo sin saberlo</td></tr>
      <tr><td>9</td><td>¬øProb√© float32 vs float64?</td><td>Posible 2-3x gratis si la precisi√≥n no es cr√≠tica</td></tr>
    </table>

  </div>
</div>

        
<a href="#m-threading">üßµ Threading y Paralelismo para Datos Masivos</a>
<!--
  ============================================================
  M√ìDULO: Threading y Paralelismo para Datos Masivos
  ============================================================
-->

<!-- ==================== M√ìDULO THREADING ==================== -->
<div class="module" id="m-threading">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">üßµ</span> Threading y Paralelismo: El Multiplicador de Fuerza</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Ya sabes c√≥mo hacer que un solo core trabaje al m√°ximo (memoria, cache, layout). Ahora multiplicamos eso por <strong>todos los cores</strong>. Pero el paralelismo mal aplicado no solo no ayuda ‚Äî puede ser m√°s lento que secuencial. Esta secci√≥n te ense√±a a paralelizar sin errores y a saber exactamente cu√°ndo conviene y cu√°ndo no.</p>

    <!-- ============ MAPA MENTAL ============ -->
    <h3>üó∫Ô∏è Las 3 formas de paralelizar en Numba</h3>

    <table>
      <tr><th>Mecanismo</th><th>C√≥mo funciona</th><th>Cu√°ndo usar</th><th>Overhead</th></tr>
      <tr>
        <td><code>parallel=True</code><br>(auto-parallelizaci√≥n)</td>
        <td>Numba detecta operaciones con sem√°ntica paralela y las fusiona en kernels multi-thread</td>
        <td>Operaciones sobre arrays completos: <code>np.sin(x) + np.cos(y)</code>, reducciones</td>
        <td>Bajo (~0.05ms)</td>
      </tr>
      <tr>
        <td><code>prange</code><br>(loops paralelos expl√≠citos)</td>
        <td>T√∫ indicas qu√© loop paralelizar. Numba reparte iteraciones entre threads</td>
        <td>Loops donde cada iteraci√≥n es independiente y hace trabajo significativo</td>
        <td>Bajo (~0.05ms)</td>
      </tr>
      <tr>
        <td><code>nogil=True</code> +<br><code>threading</code> manual</td>
        <td>T√∫ manejas los threads de Python. Numba libera el GIL para que ejecuten en paralelo</td>
        <td>Pipelines complejos, control total de scheduling, combinar Numba con I/O</td>
        <td>M√≠nimo (t√∫ controlas)</td>
      </tr>
    </table>

    <div class="info">
      <code>parallel=True</code> y <code>prange</code> usan el <strong>mismo thread pool</strong> interno de Numba. No compiten entre s√≠ ‚Äî si tienes un <code>prange</code> dentro de una funci√≥n <code>parallel=True</code>, el <code>prange</code> se ejecuta en paralelo y las operaciones de array de esa funci√≥n tambi√©n se paralelizan (pero los loops <code>prange</code> internos se serializan para evitar over-subscription).
    </div>

    <!-- ============ THREADING LAYERS ============ -->
    <h3>‚öôÔ∏è Threading layers: el motor bajo el cap√≥</h3>

    <p>Numba no implementa su propio sistema de threads. Delega a una librer√≠a externa, y la elecci√≥n de cu√°l usar importa:</p>

    <table>
      <tr><th>Layer</th><th>Backend</th><th>Fork-safe</th><th>Thread-safe</th><th>Dynamic scheduling</th><th>Instalar</th></tr>
      <tr><td><strong>tbb</strong></td><td>Intel TBB</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td><code>conda install tbb</code></td></tr>
      <tr><td><strong>omp</strong></td><td>OpenMP</td><td>‚ùå (Linux)</td><td>‚úÖ</td><td>‚ùå</td><td>Ya incluido en la mayor√≠a de sistemas</td></tr>
      <tr><td><strong>workqueue</strong></td><td>Built-in Numba</td><td>‚úÖ</td><td>‚ùå</td><td>‚ùå</td><td>Siempre disponible</td></tr>
    </table>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> config, threading_layer

<span class="cm"># Ver qu√© layer se seleccion√≥</span>
<span class="cm"># (necesitas ejecutar algo parallel primero)</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">_trigger</span>(x):
    <span class="kw">return</span> x + <span class="num">1</span>
_trigger(np.array([<span class="num">1.0</span>]))

<span class="fn">print</span>(f<span class="st">"Threading layer: {threading_layer()}"</span>)

<span class="cm"># Forzar un layer espec√≠fico (ANTES de cualquier compilaci√≥n parallel)</span>
<span class="cm"># Opci√≥n 1: variable de entorno</span>
<span class="cm"># $ NUMBA_THREADING_LAYER=tbb python mi_script.py</span>

<span class="cm"># Opci√≥n 2: program√°ticamente (antes de cualquier @njit parallel)</span>
config.THREADING_LAYER = <span class="st">'tbb'</span>

<span class="cm"># Opci√≥n 3: elegir por seguridad</span>
config.THREADING_LAYER = <span class="st">'safe'</span>        <span class="cm"># fork + thread safe (requiere TBB)</span>
config.THREADING_LAYER = <span class="st">'threadsafe'</span>  <span class="cm"># solo thread safe</span>
config.THREADING_LAYER = <span class="st">'forksafe'</span>    <span class="cm"># solo fork safe</span></code></pre>

    <div class="perf">
      <strong>Recomendaci√≥n para datos masivos:</strong> Instala TBB (<code>conda install tbb</code>). Es el √∫nico backend que soporta <strong>dynamic scheduling</strong> (repartir trabajo din√°micamente cuando hay iteraciones con costo variable). Tambi√©n es el √∫nico que es fork-safe Y thread-safe, lo que importa si usas <code>multiprocessing</code> adem√°s de Numba.
    </div>

    <!-- ============ CONTROL DE THREADS ============ -->
    <h3>üéõÔ∏è Controlar el n√∫mero de threads</h3>

<pre><code><span class="kw">from</span> numba <span class="kw">import</span> (njit, prange, config,
                    set_num_threads, get_num_threads, get_thread_id)

<span class="cm"># Ver cu√°ntos threads tiene Numba</span>
<span class="fn">print</span>(f<span class="st">"Threads disponibles: {config.NUMBA_NUM_THREADS}"</span>)
<span class="fn">print</span>(f<span class="st">"Threads activos:     {get_num_threads()}"</span>)

<span class="cm"># ‚îÄ‚îÄ‚îÄ ESCENARIO: 4 procesos Python, 8 cores f√≠sicos ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Sin ajustar: cada proceso usa 8 threads = 32 threads totales = oversubscription</span>
<span class="cm"># Con ajuste: cada proceso usa 2 threads = 8 threads totales = perfecto</span>

<span class="cm"># Opci√≥n A: Variable de entorno (antes de lanzar Python)</span>
<span class="cm"># $ NUMBA_NUM_THREADS=2 python worker.py</span>

<span class="cm"># Opci√≥n B: En runtime (m√°s flexible)</span>
set_num_threads(<span class="num">2</span>)  <span class="cm"># Ahora parallel usa solo 2 threads</span>

<span class="cm"># Se puede cambiar din√°micamente:</span>
set_num_threads(<span class="num">8</span>)  <span class="cm"># Volver a usar todos para una tarea pesada</span>
set_num_threads(<span class="num">2</span>)  <span class="cm"># Reducir de nuevo</span>

<span class="cm"># ¬°Funciona DENTRO de funciones @njit tambi√©n!</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">adaptive_parallel</span>(data, use_all_cores):
    <span class="kw">if</span> use_all_cores:
        set_num_threads(config.NUMBA_NUM_THREADS)
    <span class="kw">else</span>:
        set_num_threads(<span class="num">2</span>)

    result = np.empty(<span class="fn">len</span>(data))
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(data)):
        result[i] = np.sqrt(data[i])
    <span class="kw">return</span> result

<span class="cm"># Identificar threads individuales (√∫til para debugging)</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">show_threads</span>(n):
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        tid = get_thread_id()  <span class="cm"># 0 a get_num_threads()-1</span>
        <span class="cm"># No uses print aqu√≠ en producci√≥n (I/O serializa los threads)</span>
    <span class="kw">return</span> get_num_threads()</code></pre>

    <div class="warn">
      <code>set_num_threads(n)</code> solo puede bajar el n√∫mero, nunca subir m√°s all√° de <code>NUMBA_NUM_THREADS</code> (que se fija al importar Numba). Si necesitas un m√°ximo alto, no restrinjas <code>NUMBA_NUM_THREADS</code> con variable de entorno ‚Äî usa <code>set_num_threads()</code> din√°micamente.
    </div>

    <!-- ============ PRANGE EN PROFUNDIDAD ============ -->
    <h3>üîÑ prange en profundidad: scheduling y chunking</h3>

    <p>Por defecto, <code>prange</code> usa <strong>static scheduling</strong>: divide las N iteraciones en partes iguales entre los threads. Si cada iteraci√≥n tarda lo mismo, es perfecto. Si no, algunos threads terminan antes y esperan ociosos.</p>

<pre><code><span class="cm"># ‚îÄ‚îÄ‚îÄ PROBLEMA: iteraciones con costo variable ‚îÄ‚îÄ‚îÄ</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">costo_variable</span>(limits):
    <span class="cm">"""Cada iteraci√≥n tiene un costo muy diferente."""</span>
    results = np.empty(<span class="fn">len</span>(limits))
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(limits)):
        <span class="cm"># Collatz: el costo var√≠a enormemente seg√∫n el n√∫mero</span>
        n = limits[i]
        count = <span class="num">0</span>
        <span class="kw">while</span> n > <span class="num">1</span>:
            <span class="kw">if</span> n % <span class="num">2</span> == <span class="num">0</span>:
                n //= <span class="num">2</span>
            <span class="kw">else</span>:
                n = n * <span class="num">3</span> + <span class="num">1</span>
            count += <span class="num">1</span>
        results[i] = count
    <span class="kw">return</span> results

<span class="cm"># Con static scheduling: thread 0 puede terminar en 1ms</span>
<span class="cm"># mientras thread 3 tarda 50ms ‚Üí los otros 3 threads esperan 49ms</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ SOLUCI√ìN: dynamic scheduling con chunksize ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Requiere TBB como threading layer</span>
<span class="kw">from</span> numba <span class="kw">import</span> parallel_chunksize

<span class="cm"># Chunk size peque√±o = mejor balanceo de carga, m√°s overhead de scheduling</span>
<span class="cm"># Chunk size grande = peor balanceo, menos overhead</span>
<span class="cm"># Regla pr√°ctica: chunk_size ‚âà n_iteraciones / (n_threads √ó 10)</span>

limits = np.random.randint(<span class="num">1</span>, <span class="num">1_000_000</span>, size=<span class="num">100_000</span>)

<span class="cm"># Static (default): desbalanceado</span>
result1 = costo_variable(limits)

<span class="cm"># Dynamic con chunk size optimizado</span>
<span class="kw">with</span> parallel_chunksize(<span class="num">100</span>):   <span class="cm"># 100K iteraciones / (8 threads √ó 10) ‚âà 1250</span>
    result2 = costo_variable(limits)  <span class="cm"># pero chunks m√°s peque√±os ‚Üí mejor balance</span>

<span class="cm"># Tambi√©n se puede hacer program√°ticamente</span>
<span class="kw">from</span> numba <span class="kw">import</span> set_parallel_chunksize, get_parallel_chunksize

old = set_parallel_chunksize(<span class="num">100</span>)
result3 = costo_variable(limits)
set_parallel_chunksize(old)  <span class="cm"># restaurar</span></code></pre>

    <div class="info">
      <strong>Dynamic scheduling solo funciona con TBB.</strong> Con OpenMP o workqueue, el chunksize se ignora silenciosamente. Verifica: <code>from numba import threading_layer; print(threading_layer())</code> debe mostrar <code>tbb</code>.
    </div>

    <!-- ============ NESTED PARALLELISM ============ -->
    <h3>ü™Ü Paralelismo anidado: las reglas que nadie te dice</h3>

<pre><code><span class="cm"># REGLA CR√çTICA: Numba NO soporta prange anidados en paralelo.</span>
<span class="cm"># El loop externo se ejecuta en paralelo, el interno se SERIALIZA.</span>

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">nested_prange</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(matrix.shape[<span class="num">0</span>]):        <span class="cm"># ‚úÖ PARALELO</span>
        <span class="kw">for</span> j <span class="kw">in</span> prange(matrix.shape[<span class="num">1</span>]):    <span class="cm"># ‚ùå SERIALIZADO (tratado como range)</span>
            total += matrix[i, j]
    <span class="kw">return</span> total

<span class="cm"># ¬øC√≥mo saber cu√°l se paraleliz√≥? ‚Üí parallel_diagnostics</span>
<span class="cm"># Mostrar√° algo como:</span>
<span class="cm"># +--3 (parallel)          ‚Üê loop externo</span>
<span class="cm">#    +--2 (serial)         ‚Üê loop interno serializado</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ CONSECUENCIA PR√ÅCTICA ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Si tienes un loop externo de 4 iteraciones y uno interno de 1000,</span>
<span class="cm"># solo el de 4 se paraleliza ‚Üí ¬°solo 4 threads trabajan!</span>

<span class="cm"># ‚ùå MAL: pocas iteraciones en el loop paralelo</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">mal_anidado</span>(data_3d):  <span class="cm"># shape: (4, 1000, 1000)</span>
    result = np.zeros(data_3d.shape[<span class="num">0</span>])
    <span class="kw">for</span> c <span class="kw">in</span> prange(<span class="num">4</span>):              <span class="cm"># Solo 4 iteraciones paralelas</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1000</span>):         <span class="cm"># 1000 iteraciones seriales</span>
            <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1000</span>):     <span class="cm"># 1000 iteraciones seriales</span>
                result[c] += data_3d[c, i, j]
    <span class="kw">return</span> result

<span class="cm"># ‚úÖ BIEN: aplanar para maximizar iteraciones paralelas</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">bien_aplanado</span>(data_3d):  <span class="cm"># shape: (4, 1000, 1000)</span>
    C, H, W = data_3d.shape
    flat_size = C * H           <span class="cm"># = 4000 iteraciones paralelas</span>
    result = np.zeros(C)
    <span class="kw">for</span> idx <span class="kw">in</span> prange(flat_size):
        c = idx // H
        i = idx % H
        row_sum = <span class="num">0.0</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(W):
            row_sum += data_3d[c, i, j]
        result[c] += row_sum    <span class="cm"># ¬°Cuidado! Esto es una reducci√≥n en result[c]</span>
    <span class="kw">return</span> result</code></pre>

    <!-- ============ NOGIL + THREADING ============ -->
    <h3>üîì nogil + threading manual: control total</h3>

    <p><code>prange</code> es conveniente pero limitado: un solo patr√≥n de loop paralelo, sin control de scheduling, sin poder mezclar con I/O. Para pipelines complejos, necesitas <code>nogil=True</code> con threading de Python.</p>

<pre><code><span class="kw">import</span> threading
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="cm"># ‚îÄ‚îÄ‚îÄ PATR√ìN 1: Divide & Conquer manual ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># √ötil cuando necesitas control total de qu√© chunk va a qu√© thread</span>

<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_chunk</span>(data, result, start, end):
    <span class="cm">"""Procesa un rango [start, end) del array.
    nogil=True permite que m√∫ltiples threads ejecuten esto simult√°neamente."""</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(start, end):
        <span class="cm"># Operaci√≥n costosa por elemento</span>
        val = data[i]
        <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">100</span>):   <span class="cm"># simular trabajo pesado</span>
            val = np.sin(val) * np.cos(val) + <span class="num">0.001</span>
        result[i] = val

<span class="kw">def</span> <span class="fn">parallel_process</span>(data, n_threads=<span class="num">None</span>):
    <span class="cm">"""Procesa data en paralelo con n_threads threads."""</span>
    <span class="kw">import</span> os
    <span class="kw">if</span> n_threads <span class="kw">is</span> <span class="num">None</span>:
        n_threads = os.cpu_count()

    n = <span class="fn">len</span>(data)
    result = np.empty_like(data)
    chunk_size = (n + n_threads - <span class="num">1</span>) // n_threads  <span class="cm"># ceil division</span>

    threads = []
    <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_threads):
        start = t * chunk_size
        end = <span class="fn">min</span>(start + chunk_size, n)
        <span class="kw">if</span> start >= n:
            <span class="kw">break</span>
        thread = threading.Thread(
            target=process_chunk,
            args=(data, result, start, end)
        )
        threads.append(thread)
        thread.start()

    <span class="kw">for</span> t <span class="kw">in</span> threads:
        t.join()

    <span class="kw">return</span> result

data = np.random.rand(<span class="num">1_000_000</span>)
result = parallel_process(data, n_threads=<span class="num">8</span>)</code></pre>

    <h4>Patr√≥n 2: Pipeline productor-consumidor</h4>
<pre><code><span class="kw">import</span> threading
<span class="kw">from</span> queue <span class="kw">import</span> Queue

<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">heavy_transform</span>(chunk, output):
    <span class="cm">"""Transformaci√≥n pesada sobre un chunk de datos."""</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(chunk)):
        val = chunk[i]
        <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">50</span>):
            val = np.sqrt(np.abs(val) + <span class="num">1.0</span>)
        output[i] = val

<span class="kw">def</span> <span class="fn">pipeline_process</span>(data, chunk_size=<span class="num">100_000</span>, n_workers=<span class="num">4</span>):
    <span class="cm">"""Pipeline: un thread carga chunks, varios workers los procesan.
    Ideal cuando combinas I/O con c√≥mputo pesado."""</span>

    n = <span class="fn">len</span>(data)
    result = np.empty_like(data)
    task_queue = Queue(maxsize=n_workers * <span class="num">2</span>)  <span class="cm"># buffer limitado</span>
    done_event = threading.Event()

    <span class="kw">def</span> <span class="fn">worker</span>():
        <span class="kw">while not</span> done_event.is_set() <span class="kw">or not</span> task_queue.empty():
            <span class="kw">try</span>:
                start, end = task_queue.get(timeout=<span class="num">0.1</span>)
            <span class="kw">except</span>:
                <span class="kw">continue</span>
            <span class="cm"># heavy_transform tiene nogil=True</span>
            <span class="cm"># ‚Üí se ejecuta sin GIL ‚Üí verdadero paralelismo</span>
            heavy_transform(data[start:end], result[start:end])
            task_queue.task_done()

    <span class="cm"># Lanzar workers</span>
    workers = []
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_workers):
        t = threading.Thread(target=worker, daemon=<span class="num">True</span>)
        t.start()
        workers.append(t)

    <span class="cm"># Producir tareas (podr√≠a ser lectura de disco aqu√≠)</span>
    <span class="kw">for</span> start <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, n, chunk_size):
        end = <span class="fn">min</span>(start + chunk_size, n)
        task_queue.put((start, end))

    task_queue.join()   <span class="cm"># Esperar que todos los chunks se procesen</span>
    done_event.set()    <span class="cm"># Se√±alar a workers que terminen</span>
    <span class="kw">for</span> t <span class="kw">in</span> workers:
        t.join()

    <span class="kw">return</span> result</code></pre>

    <h4>Patr√≥n 3: concurrent.futures (la forma m√°s limpia)</h4>
<pre><code><span class="kw">from</span> concurrent.futures <span class="kw">import</span> ThreadPoolExecutor
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">compute_stats</span>(chunk):
    <span class="cm">"""Calcula media y varianza de un chunk (nogil para paralelismo real)."""</span>
    n = <span class="fn">len</span>(chunk)
    mean = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        mean += chunk[i]
    mean /= n

    var = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        diff = chunk[i] - mean
        var += diff * diff
    var /= (n - <span class="num">1</span>)

    <span class="kw">return</span> mean, var

<span class="kw">def</span> <span class="fn">parallel_stats_pipeline</span>(data, n_chunks=<span class="num">16</span>, n_workers=<span class="num">8</span>):
    <span class="cm">"""Calcula estad√≠sticas globales procesando chunks en paralelo."""</span>
    chunk_size = (<span class="fn">len</span>(data) + n_chunks - <span class="num">1</span>) // n_chunks
    chunks = [data[i*chunk_size : (i+<span class="num">1</span>)*chunk_size] <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_chunks)]

    <span class="cm"># ThreadPoolExecutor + nogil = paralelismo real en Python</span>
    <span class="kw">with</span> ThreadPoolExecutor(max_workers=n_workers) <span class="kw">as</span> pool:
        futures = [pool.submit(compute_stats, chunk) <span class="kw">for</span> chunk <span class="kw">in</span> chunks]
        results = [f.result() <span class="kw">for</span> f <span class="kw">in</span> futures]

    <span class="cm"># Combinar resultados parciales</span>
    total_mean = np.mean([r[<span class="num">0</span>] <span class="kw">for</span> r <span class="kw">in</span> results])
    <span class="kw">return</span> total_mean

data = np.random.randn(<span class="num">100_000_000</span>)  <span class="cm"># 800MB</span>
result = parallel_stats_pipeline(data)</code></pre>

    <!-- ============ PRANGE vs NOGIL vs PARALLEL ============ -->
    <h3>‚öîÔ∏è ¬øprange vs nogil+threading vs parallel=True? Cu√°ndo usar cada uno</h3>

<pre><code><span class="cm"># ‚îÄ‚îÄ‚îÄ ESCENARIO 1: Operaciones vectoriales sobre arrays grandes ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># ‚Üí parallel=True (auto-detecci√≥n + fusi√≥n de kernels)</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">vector_ops</span>(x, y):
    <span class="cm"># Numba fusiona todo esto en UN SOLO kernel paralelo</span>
    a = np.sin(x) ** <span class="num">2</span>
    b = np.cos(y) ** <span class="num">2</span>
    <span class="kw">return</span> a + b
<span class="cm"># ‚úÖ parallel=True es ideal: detecta las operaciones, las fusiona,</span>
<span class="cm">#    y las reparte entre threads autom√°ticamente</span>
<span class="cm"># ‚ùå prange ser√≠a innecesariamente verboso para esto</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ ESCENARIO 2: Loop con l√≥gica compleja por iteraci√≥n ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># ‚Üí prange (t√∫ controlas el loop expl√≠citamente)</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">complex_per_row</span>(matrix):
    n = matrix.shape[<span class="num">0</span>]
    result = np.empty(n)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        <span class="cm"># L√≥gica compleja que parallel=True no puede auto-detectar</span>
        row = matrix[i, :]
        sorted_indices = np.argsort(row)  <span class="cm"># no es paralelizable autom√°ticamente</span>
        median_idx = sorted_indices[<span class="fn">len</span>(row) // <span class="num">2</span>]
        result[i] = row[median_idx]
    <span class="kw">return</span> result
<span class="cm"># ‚úÖ prange es ideal: cada fila es independiente pero la l√≥gica es compleja</span>
<span class="cm"># ‚ùå parallel=True no podr√≠a auto-detectar el paralelismo aqu√≠</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ ESCENARIO 3: Pipeline I/O + c√≥mputo ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># ‚Üí nogil + threading (control total, mezclar I/O con c√≥mputo)</span>
<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">transform_chunk</span>(chunk, output):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(chunk)):
        output[i] = np.sqrt(chunk[i]) * np.log(chunk[i] + <span class="num">1</span>)

<span class="kw">def</span> <span class="fn">io_pipeline</span>(filenames):
    <span class="cm">"""Lee archivos mientras procesa los anteriores.
    prange NO puede hacer esto: necesitas solapar I/O con c√≥mputo."""</span>
    <span class="kw">with</span> ThreadPoolExecutor(max_workers=<span class="num">6</span>) <span class="kw">as</span> pool:
        futures = []
        <span class="kw">for</span> fname <span class="kw">in</span> filenames:
            data = np.load(fname)               <span class="cm"># I/O (con GIL, pero no bloquea Numba)</span>
            output = np.empty_like(data)
            fut = pool.submit(transform_chunk, data, output)  <span class="cm"># C√≥mputo sin GIL</span>
            futures.append((fname, fut, output))
        <span class="cm"># Mientras un thread procesa, otro puede estar leyendo el siguiente archivo</span>
        <span class="kw">return</span> [(fn, out) <span class="kw">for</span> fn, fut, out <span class="kw">in</span> futures <span class="kw">if</span> fut.result() <span class="kw">is</span> <span class="num">None</span> <span class="kw">or</span> <span class="num">True</span>]
<span class="cm"># ‚úÖ nogil+threading es ideal: solapas I/O con c√≥mputo</span>
<span class="cm"># ‚ùå prange no puede mezclar I/O con c√≥mputo</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ ESCENARIO 4: M√∫ltiples funciones independientes ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># ‚Üí nogil + threading (ejecutar funciones DIFERENTES en paralelo)</span>
<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">task_a</span>(data, output):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        output[i] = np.sin(data[i])

<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">task_b</span>(data, output):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        output[i] = np.cos(data[i])

<span class="kw">def</span> <span class="fn">run_tasks_parallel</span>(data_a, data_b):
    out_a = np.empty_like(data_a)
    out_b = np.empty_like(data_b)
    t1 = threading.Thread(target=task_a, args=(data_a, out_a))
    t2 = threading.Thread(target=task_b, args=(data_b, out_b))
    t1.start(); t2.start()
    t1.join();  t2.join()
    <span class="kw">return</span> out_a, out_b
<span class="cm"># ‚úÖ Dos funciones DIFERENTES ejecut√°ndose simult√°neamente</span>
<span class="cm"># ‚ùå prange solo paraleliza iteraciones del MISMO loop</span></code></pre>

    <h3>üìä Tabla de decisi√≥n</h3>
    <table>
      <tr><th>Situaci√≥n</th><th>Mecanismo</th><th>Por qu√©</th></tr>
      <tr><td>Operaciones NumPy sobre arrays grandes</td><td><code>parallel=True</code></td><td>Auto-detecta y fusiona</td></tr>
      <tr><td>Loop con iteraciones independientes</td><td><code>prange</code></td><td>Expl√≠cito, simple, eficiente</td></tr>
      <tr><td>Iteraciones con costo muy variable</td><td><code>prange</code> + <code>parallel_chunksize</code> + TBB</td><td>Dynamic scheduling evita desbalance</td></tr>
      <tr><td>Pipeline I/O + c√≥mputo</td><td><code>nogil</code> + <code>ThreadPoolExecutor</code></td><td>Solapa I/O con c√≥mputo</td></tr>
      <tr><td>Funciones diferentes en paralelo</td><td><code>nogil</code> + <code>threading</code></td><td>prange solo paraleliza un loop</td></tr>
      <tr><td>M√∫ltiples procesos Python + Numba</td><td><code>set_num_threads</code></td><td>Evitar oversubscription</td></tr>
      <tr><td>Dataset &gt; RAM</td><td><code>nogil</code> + pipeline de chunks</td><td>Procesar en streaming sin cargar todo</td></tr>
    </table>

    <!-- ============ OVERSUBSCRIPTION ============ -->
    <h3>üí• Oversubscription: el enemigo silencioso</h3>

    <p>Oversubscription ocurre cuando lanzas m√°s threads que cores f√≠sicos. Los threads compiten por CPU, el OS hace context switches constantemente, y la cache se invalida en cada switch. El resultado: <strong>m√°s threads = m√°s lento</strong>.</p>

<pre><code><span class="kw">import</span> os
<span class="kw">from</span> numba <span class="kw">import</span> config, set_num_threads

<span class="cm"># ‚îÄ‚îÄ‚îÄ DETECTAR oversubscription ‚îÄ‚îÄ‚îÄ</span>
physical_cores = os.cpu_count()  <span class="cm"># ojo: incluye hyperthreading</span>
<span class="cm"># En un i7-12700 con 12 cores / 20 threads:</span>
<span class="cm"># os.cpu_count() = 20 (incluye HT)</span>
<span class="cm"># Numba usar√° 20 threads por defecto ‚Üí sub√≥ptimo para c√≥mputo puro</span>

<span class="cm"># Para c√≥mputo num√©rico puro, usa cores F√çSICOS, no l√≥gicos</span>
<span class="cm"># Hyperthreading NO ayuda con c√≥digo num√©rico intensivo</span>
<span class="cm"># (dos threads en el mismo core comparten las unidades de c√≥mputo)</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ PREVENIR oversubscription ‚îÄ‚îÄ‚îÄ</span>

<span class="cm"># Caso 1: Script single-process</span>
<span class="cm"># Regla: threads = cores f√≠sicos (no l√≥gicos)</span>
set_num_threads(physical_cores // <span class="num">2</span>)  <span class="cm"># Aprox cores f√≠sicos si HT est√° activo</span>

<span class="cm"># Caso 2: M√∫ltiples procesos (ej: 4 workers)</span>
n_workers = <span class="num">4</span>
threads_per_worker = max(<span class="num">1</span>, physical_cores // (<span class="num">2</span> * n_workers))
set_num_threads(threads_per_worker)

<span class="cm"># Caso 3: Numba + otra librer√≠a paralela (NumPy con MKL, scikit-learn, etc.)</span>
<span class="cm"># ¬°MKL y Numba AMBOS lanzan threads! Si no coordinan ‚Üí oversubscription</span>
<span class="cm"># Soluci√≥n: limitar threads de ambos</span>
os.environ[<span class="st">'MKL_NUM_THREADS'</span>] = <span class="st">'4'</span>          <span class="cm"># Limitar MKL</span>
os.environ[<span class="st">'OMP_NUM_THREADS'</span>] = <span class="st">'4'</span>          <span class="cm"># Limitar OpenMP</span>
os.environ[<span class="st">'NUMBA_NUM_THREADS'</span>] = <span class="st">'4'</span>        <span class="cm"># Limitar Numba</span>
<span class="cm"># IMPORTANTE: estas env vars deben setearse ANTES de importar las librer√≠as</span></code></pre>

    <!-- ============ CASO REAL: CHUNK-PROCESS-COMBINE ============ -->
    <h3>üè≠ Caso real completo: Chunk-Process-Combine para datasets gigantes</h3>

    <p>El patr√≥n definitivo para procesar datos que no caben en cache (o incluso en RAM). Es la base de c√≥mo funcionan internamente Spark, Dask, y Polars.</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit, prange, set_num_threads, config
<span class="kw">import</span> time

<span class="cm"># ‚îÄ‚îÄ‚îÄ PASO 1: Funci√≥n de procesamiento por chunk (cache-friendly) ‚îÄ‚îÄ‚îÄ</span>
<span class="dec">@njit</span>(fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_single_chunk</span>(chunk):
    <span class="cm">"""Procesa un chunk que cabe en cache L2/L3.
    Retorna estad√≠sticas parciales: (sum, sum_sq, min, max, count)"""</span>
    n = <span class="fn">len</span>(chunk)
    total = <span class="num">0.0</span>
    total_sq = <span class="num">0.0</span>
    vmin = chunk[<span class="num">0</span>]
    vmax = chunk[<span class="num">0</span>]

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        val = chunk[i]
        <span class="cm"># Todas las operaciones en un solo pass ‚Üí datos en cache</span>
        total += val
        total_sq += val * val
        <span class="kw">if</span> val < vmin:
            vmin = val
        <span class="kw">if</span> val > vmax:
            vmax = val

    <span class="kw">return</span> total, total_sq, vmin, vmax, n

<span class="cm"># ‚îÄ‚îÄ‚îÄ PASO 2: Paralelizar sobre chunks ‚îÄ‚îÄ‚îÄ</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_all_chunks</span>(data, chunk_size):
    <span class="cm">"""Divide data en chunks y los procesa en paralelo."""</span>
    n = <span class="fn">len</span>(data)
    n_chunks = (n + chunk_size - <span class="num">1</span>) // chunk_size

    <span class="cm"># Arrays para resultados parciales (un resultado por chunk)</span>
    sums     = np.empty(n_chunks)
    sums_sq  = np.empty(n_chunks)
    mins     = np.empty(n_chunks)
    maxs     = np.empty(n_chunks)
    counts   = np.empty(n_chunks, dtype=np.int64)

    <span class="kw">for</span> c <span class="kw">in</span> prange(n_chunks):
        start = c * chunk_size
        end = <span class="fn">min</span>(start + chunk_size, n)
        chunk = data[start:end]

        s, sq, mn, mx, cnt = process_single_chunk(chunk)
        sums[c] = s
        sums_sq[c] = sq
        mins[c] = mn
        maxs[c] = mx
        counts[c] = cnt

    <span class="kw">return</span> sums, sums_sq, mins, maxs, counts

<span class="cm"># ‚îÄ‚îÄ‚îÄ PASO 3: Combinar resultados parciales ‚îÄ‚îÄ‚îÄ</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">combine_results</span>(sums, sums_sq, mins, maxs, counts):
    <span class="cm">"""Combina estad√≠sticas parciales en resultado global."""</span>
    total_count = np.sum(counts)
    total_sum = np.sum(sums)
    total_sum_sq = np.sum(sums_sq)

    mean = total_sum / total_count
    variance = (total_sum_sq / total_count) - mean * mean
    global_min = mins[<span class="num">0</span>]
    global_max = maxs[<span class="num">0</span>]
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, <span class="fn">len</span>(mins)):
        <span class="kw">if</span> mins[i] < global_min:
            global_min = mins[i]
        <span class="kw">if</span> maxs[i] > global_max:
            global_max = maxs[i]

    <span class="kw">return</span> mean, np.sqrt(variance), global_min, global_max, total_count

<span class="cm"># ‚îÄ‚îÄ‚îÄ ORQUESTADOR ‚îÄ‚îÄ‚îÄ</span>
<span class="kw">def</span> <span class="fn">massive_stats</span>(data, chunk_size=<span class="num">131072</span>):
    <span class="cm">"""chunk_size=131072 float64 = 1MB ‚Üí cabe c√≥modamente en L2/L3"""</span>
    sums, sums_sq, mins, maxs, counts = process_all_chunks(data, chunk_size)
    <span class="kw">return</span> combine_results(sums, sums_sq, mins, maxs, counts)

<span class="cm"># ‚îÄ‚îÄ‚îÄ BENCHMARK ‚îÄ‚îÄ‚îÄ</span>
data = np.random.randn(<span class="num">100_000_000</span>)  <span class="cm"># 800MB de datos</span>

<span class="cm"># Warmup</span>
massive_stats(data)

start = time.perf_counter()
mean, std, vmin, vmax, count = massive_stats(data)
elapsed = time.perf_counter() - start

<span class="fn">print</span>(f<span class="st">"100M elementos en {elapsed*1000:.1f}ms"</span>)
<span class="fn">print</span>(f<span class="st">"Throughput (Rendimiento real): {data.nbytes / elapsed / 1e9:.1f} GB/s"</span>)
<span class="fn">print</span>(f<span class="st">"Mean={mean:.6f}, Std={std:.6f}, Min={vmin:.4f}, Max={vmax:.4f}"</span>)</code></pre>

    <div class="perf">
      Este patr√≥n Chunk-Process-Combine combina todo lo aprendido: acceso contiguo (cada chunk es contiguo), cache-friendly (chunks caben en L2), sin temporales (todo en un solo pass), paralelo (chunks en prange), y fastmath. Es el patr√≥n que escala desde kilobytes hasta terabytes.
    </div>

    <!-- ============ CHECKLIST ============ -->
    <h3>üìã Checklist de paralelismo</h3>
    <table>
      <tr><th>#</th><th>Verificar</th><th>Si no lo haces...</th></tr>
      <tr><td>1</td><td>¬øSuficientes iteraciones en el prange externo?</td><td>Pocos threads trabajan, los dem√°s esperan</td></tr>
      <tr><td>2</td><td>¬øCada iteraci√≥n es independiente (sin escrituras compartidas)?</td><td>Race conditions, resultados corruptos</td></tr>
      <tr><td>3</td><td>¬øEl threading layer es apropiado?</td><td>fork+thread safety, dynamic scheduling no disponible</td></tr>
      <tr><td>4</td><td>¬øCoordin√© threads con otras librer√≠as (MKL, OpenMP)?</td><td>Oversubscription, peor rendimiento</td></tr>
      <tr><td>5</td><td>¬øProb√© con scaling test (1K ‚Üí 10M)?</td><td>Overhead de threading domina con datos peque√±os</td></tr>
      <tr><td>6</td><td>¬øUso <code>parallel_diagnostics()</code> para verificar?</td><td>Numba puede haber serializado tu loop sin avisar</td></tr>
      <tr><td>7</td><td>¬øChunks de prange caben en cache L2/L3?</td><td>Cache thrashing entre threads, peor que secuencial</td></tr>
      <tr><td>8</td><td>¬øIteraciones de costo variable? ‚Üí chunksize din√°mico</td><td>Threads r√°pidos esperan a los lentos</td></tr>
      <tr><td>9</td><td>¬øNecesito mezclar I/O + c√≥mputo?</td><td>prange no puede, usa nogil + threading</td></tr>
    </table>

  </div>
</div>


<a href="#m-cases">üèÜ Casos End-to-End: De "R√°pido" a "Rid√≠culamente R√°pido"</a>
<!--
  ============================================================
  M√ìDULO: Casos de Estudio End-to-End ‚Äî De "R√°pido" a "Rid√≠culamente R√°pido"
  ============================================================
-->

<!-- ==================== M√ìDULO CASOS DE ESTUDIO ==================== -->
<div class="module" id="m-cases">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">üèÜ</span> Casos de Estudio End-to-End: De "R√°pido" a "Rid√≠culamente R√°pido"</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Esta secci√≥n es diferente. No es teor√≠a ‚Äî son <strong>4 problemas reales</strong> resueltos paso a paso, desde la versi√≥n naive hasta una versi√≥n que exprime hasta el √∫ltimo nanosegundo. Cada caso sigue el mismo proceso disciplinado: <strong>medir ‚Üí identificar bottleneck ‚Üí optimizar ‚Üí medir de nuevo ‚Üí iterar</strong>. Al final, entender√°s visceralmente la diferencia entre "r√°pido" y "rid√≠culamente r√°pido".</p>

    <!-- ============================================ -->
    <!-- ============ CASO 1: AGGREGACI√ìN ========== -->
    <!-- ============================================ -->
    <h3 style="color: #f0883e; border-top: 3px solid #f0883e; padding-top: 12px;">
      üìä Caso 1: Aggregaci√≥n sobre 100 millones de filas
    </h3>
    <p><strong>El problema:</strong> Tienes un dataset de 100M de transacciones financieras. Cada transacci√≥n tiene un <code>customer_id</code> (0 a 999,999) y un <code>amount</code>. Necesitas calcular la suma, media, m√≠nimo, m√°ximo y conteo por cliente.</p>

    <h4>V0 ‚Äî NumPy puro (baseline)</h4>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> time

<span class="cm"># Generar datos: 100M transacciones</span>
N = <span class="num">100_000_000</span>
N_CUSTOMERS = <span class="num">1_000_000</span>
customer_ids = np.random.randint(<span class="num">0</span>, N_CUSTOMERS, size=N, dtype=np.int32)
amounts = np.random.exponential(<span class="num">100.0</span>, size=N).astype(np.float64)

<span class="cm"># V0: NumPy puro ‚Äî simple pero lento</span>
<span class="kw">def</span> <span class="fn">aggregate_numpy</span>(ids, amounts, n_customers):
    sums   = np.zeros(n_customers)
    counts = np.zeros(n_customers, dtype=np.int64)
    mins   = np.full(n_customers, np.inf)
    maxs   = np.full(n_customers, -np.inf)

    <span class="cm"># np.add.at es la forma "correcta" en NumPy, pero es MUY lenta</span>
    np.add.at(sums, ids, amounts)
    np.add.at(counts, ids, <span class="num">1</span>)
    np.minimum.at(mins, ids, amounts)
    np.maximum.at(maxs, ids, amounts)

    means = np.where(counts > <span class="num">0</span>, sums / counts, <span class="num">0.0</span>)
    <span class="kw">return</span> sums, means, mins, maxs, counts

<span class="cm"># Benchmark V0:</span>
<span class="cm"># ~45 segundos para 100M filas</span>
<span class="cm"># Bottleneck: np.add.at hace scatter operations ‚Äî cada una es un cache miss aleatorio</span></code></pre>

    <div class="warn">
      <strong>Bottleneck identificado:</strong> <code>np.add.at</code> accede a posiciones aleatorias del array resultado (scatter). Cada acceso salta a un <code>customer_id</code> diferente ‚Üí cache misses masivos. Adem√°s, hace 4 passes completos sobre los 100M de datos.
    </div>

    <h4>V1 ‚Äî @njit con un solo pass (eliminar temporales + fusionar operaciones)</h4>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">aggregate_v1</span>(ids, amounts, n_customers):
    <span class="cm">"""Un solo pass sobre los datos: 4 operaciones fusionadas."""</span>
    sums   = np.zeros(n_customers)
    counts = np.zeros(n_customers, dtype=np.int64)
    mins   = np.full(n_customers, np.inf)
    maxs   = np.full(n_customers, -np.inf)

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(ids)):
        cid = ids[i]
        amt = amounts[i]
        sums[cid] += amt
        counts[cid] += <span class="num">1</span>
        <span class="kw">if</span> amt < mins[cid]:
            mins[cid] = amt
        <span class="kw">if</span> amt > maxs[cid]:
            maxs[cid] = amt

    means = np.empty(n_customers)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_customers):
        <span class="kw">if</span> counts[i] > <span class="num">0</span>:
            means[i] = sums[i] / counts[i]
        <span class="kw">else</span>:
            means[i] = <span class="num">0.0</span>

    <span class="kw">return</span> sums, means, mins, maxs, counts

<span class="cm"># Benchmark V1:</span>
<span class="cm"># ~3.2 segundos ‚Äî 14x m√°s r√°pido que NumPy</span>
<span class="cm"># ¬øPor qu√©? Un solo pass = lee los 100M de datos UNA vez en vez de 4</span>
<span class="cm"># Pero: el scatter (sums[cid] +=) sigue siendo cache-unfriendly</span></code></pre>

    <h4>V2 ‚Äî Sort + scan (convertir scatter aleatorio en acceso secuencial)</h4>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">aggregate_v2</span>(ids, amounts, n_customers):
    <span class="cm">"""Sort por customer_id ‚Üí acceso secuencial al resultado."""</span>
    <span class="cm"># Paso 1: Ordenar por customer_id</span>
    order = np.argsort(ids)  <span class="cm"># O(N log N) pero mejora localidad</span>

    <span class="cm"># Paso 2: Scan secuencial sobre datos ordenados</span>
    sums   = np.zeros(n_customers)
    counts = np.zeros(n_customers, dtype=np.int64)
    mins   = np.full(n_customers, np.inf)
    maxs   = np.full(n_customers, -np.inf)

    <span class="kw">for</span> idx <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(order)):
        i = order[idx]
        cid = ids[i]
        amt = amounts[i]
        <span class="cm"># Ahora todos los customer_id iguales est√°n JUNTOS</span>
        <span class="cm"># ‚Üí sums[cid] est√° en cache porque es el mismo cid que antes</span>
        sums[cid] += amt
        counts[cid] += <span class="num">1</span>
        <span class="kw">if</span> amt < mins[cid]:
            mins[cid] = amt
        <span class="kw">if</span> amt > maxs[cid]:
            maxs[cid] = amt

    means = np.empty(n_customers)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_customers):
        means[i] = sums[i] / counts[i] <span class="kw">if</span> counts[i] > <span class="num">0</span> <span class="kw">else</span> <span class="num">0.0</span>

    <span class="kw">return</span> sums, means, mins, maxs, counts

<span class="cm"># Benchmark V2:</span>
<span class="cm"># ~5.8 segundos (sort domina: ~4s sort + ~1.8s scan)</span>
<span class="cm"># ¬°M√°s lento! El sort es caro para 100M. Pero el scan es 2x m√°s r√°pido.</span>
<span class="cm"># Lecci√≥n: sort-then-scan solo gana cuando N_CUSTOMERS << N</span>
<span class="cm"># y la distribuci√≥n es muy dispersa. Aqu√≠ hay 100 transacciones/cliente ‚Üí V1 gana.</span></code></pre>

    <h4>V3 ‚Äî Histogram approach paralelo (el truco definitivo)</h4>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> njit, prange, get_num_threads

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">aggregate_v3</span>(ids, amounts, n_customers):
    <span class="cm">"""Histograma paralelo: cada thread tiene su propia copia,
    luego se reducen. Elimina race conditions sin locks."""</span>
    n_threads = get_num_threads()
    n = <span class="fn">len</span>(ids)

    <span class="cm"># Cada thread tiene sus propios acumuladores (sin competencia)</span>
    local_sums   = np.zeros((n_threads, n_customers))
    local_counts = np.zeros((n_threads, n_customers), dtype=np.int64)
    local_mins   = np.full((n_threads, n_customers), np.inf)
    local_maxs   = np.full((n_threads, n_customers), -np.inf)

    <span class="cm"># Fase 1: Cada thread procesa su chunk</span>
    chunk_size = (n + n_threads - <span class="num">1</span>) // n_threads
    <span class="kw">for</span> t <span class="kw">in</span> prange(n_threads):
        start = t * chunk_size
        end = <span class="fn">min</span>(start + chunk_size, n)
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(start, end):
            cid = ids[i]
            amt = amounts[i]
            local_sums[t, cid] += amt
            local_counts[t, cid] += <span class="num">1</span>
            <span class="kw">if</span> amt < local_mins[t, cid]:
                local_mins[t, cid] = amt
            <span class="kw">if</span> amt > local_maxs[t, cid]:
                local_maxs[t, cid] = amt

    <span class="cm"># Fase 2: Reducir resultados parciales (paralelo sobre clientes)</span>
    sums   = np.zeros(n_customers)
    counts = np.zeros(n_customers, dtype=np.int64)
    mins   = np.full(n_customers, np.inf)
    maxs   = np.full(n_customers, -np.inf)
    means  = np.zeros(n_customers)

    <span class="kw">for</span> c <span class="kw">in</span> prange(n_customers):
        <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_threads):
            sums[c] += local_sums[t, c]
            counts[c] += local_counts[t, c]
            <span class="kw">if</span> local_mins[t, c] < mins[c]:
                mins[c] = local_mins[t, c]
            <span class="kw">if</span> local_maxs[t, c] > maxs[c]:
                maxs[c] = local_maxs[t, c]
        <span class="kw">if</span> counts[c] > <span class="num">0</span>:
            means[c] = sums[c] / counts[c]

    <span class="kw">return</span> sums, means, mins, maxs, counts

<span class="cm"># Benchmark V3 (8 cores):</span>
<span class="cm"># ~0.9 segundos ‚Äî 50x vs NumPy, 3.5x vs V1</span>
<span class="cm"># PERO: usa n_threads √ó n_customers √ó 4 arrays = mucha memoria</span>
<span class="cm"># Con 8 threads √ó 1M clientes √ó 4 arrays √ó 8 bytes = 256 MB de buffers</span></code></pre>

    <h4>V4 ‚Äî Chunked histogram paralelo (memoria controlada)</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">aggregate_v4</span>(ids, amounts, n_customers):
    <span class="cm">"""Versi√≥n final: parallel histograms con chunks que caben en L3.
    Equilibrio perfecto entre velocidad y memoria."""</span>
    n_threads = get_num_threads()
    n = <span class="fn">len</span>(ids)

    <span class="cm"># Si n_customers es peque√±o (< 100K), cada thread puede tener copia completa</span>
    <span class="cm"># Si es grande (> 100K), procesamos por bloques de clientes</span>
    CUSTOMER_BLOCK = <span class="fn">min</span>(n_customers, <span class="num">65536</span>)  <span class="cm"># 64K clientes √ó 8 bytes = 512KB ‚Üí cabe en L2</span>

    sums   = np.zeros(n_customers)
    counts = np.zeros(n_customers, dtype=np.int64)
    mins   = np.full(n_customers, np.inf)
    maxs   = np.full(n_customers, -np.inf)

    <span class="cm"># Buffer local por thread (solo CUSTOMER_BLOCK de tama√±o)</span>
    local_sums   = np.zeros((n_threads, CUSTOMER_BLOCK))
    local_counts = np.zeros((n_threads, CUSTOMER_BLOCK), dtype=np.int64)
    local_mins   = np.empty((n_threads, CUSTOMER_BLOCK))
    local_maxs   = np.empty((n_threads, CUSTOMER_BLOCK))

    <span class="cm"># Procesar por bloques de customer_ids</span>
    <span class="kw">for</span> cblock_start <span class="kw">in</span> <span class="fn">range</span>(<span class="num">0</span>, n_customers, CUSTOMER_BLOCK):
        cblock_end = <span class="fn">min</span>(cblock_start + CUSTOMER_BLOCK, n_customers)
        cblock_size = cblock_end - cblock_start

        <span class="cm"># Reset buffers locales</span>
        <span class="kw">for</span> t <span class="kw">in</span> prange(n_threads):
            <span class="kw">for</span> c <span class="kw">in</span> <span class="fn">range</span>(cblock_size):
                local_sums[t, c] = <span class="num">0.0</span>
                local_counts[t, c] = <span class="num">0</span>
                local_mins[t, c] = np.inf
                local_maxs[t, c] = -np.inf

        <span class="cm"># Scatter paralelo sobre datos (cada thread su chunk de datos)</span>
        chunk_size = (n + n_threads - <span class="num">1</span>) // n_threads
        <span class="kw">for</span> t <span class="kw">in</span> prange(n_threads):
            start = t * chunk_size
            end = <span class="fn">min</span>(start + chunk_size, n)
            <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(start, end):
                cid = ids[i]
                <span class="kw">if</span> cid >= cblock_start <span class="kw">and</span> cid < cblock_end:
                    local_cid = cid - cblock_start
                    amt = amounts[i]
                    local_sums[t, local_cid] += amt
                    local_counts[t, local_cid] += <span class="num">1</span>
                    <span class="kw">if</span> amt < local_mins[t, local_cid]:
                        local_mins[t, local_cid] = amt
                    <span class="kw">if</span> amt > local_maxs[t, local_cid]:
                        local_maxs[t, local_cid] = amt

        <span class="cm"># Reduce paralelo</span>
        <span class="kw">for</span> c <span class="kw">in</span> prange(cblock_size):
            gc = cblock_start + c
            <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_threads):
                sums[gc] += local_sums[t, c]
                counts[gc] += local_counts[t, c]
                <span class="kw">if</span> local_mins[t, c] < mins[gc]:
                    mins[gc] = local_mins[t, c]
                <span class="kw">if</span> local_maxs[t, c] > maxs[gc]:
                    maxs[gc] = local_maxs[t, c]

    <span class="cm"># Calcular medias</span>
    means = np.empty(n_customers)
    <span class="kw">for</span> c <span class="kw">in</span> prange(n_customers):
        means[c] = sums[c] / counts[c] <span class="kw">if</span> counts[c] > <span class="num">0</span> <span class="kw">else</span> <span class="num">0.0</span>

    <span class="kw">return</span> sums, means, mins, maxs, counts

<span class="cm"># Benchmark V4 (8 cores):</span>
<span class="cm"># ~1.1 segundos (ligeramente m√°s lento que V3 por la condici√≥n if)</span>
<span class="cm"># PERO: usa solo n_threads √ó 64K √ó 4 √ó 8 = 16MB de buffers (vs 256MB de V3)</span>
<span class="cm"># Para 10M+ clientes, V3 se queda sin memoria y V4 sigue funcionando</span></code></pre>

    <h4>Tabla resumen: la evoluci√≥n</h4>
    <table>
      <tr><th>Versi√≥n</th><th>T√©cnica</th><th>Tiempo (100M filas)</th><th>Speedup vs V0</th><th>Memoria extra</th></tr>
      <tr><td>V0</td><td>NumPy np.add.at</td><td>~45,000 ms</td><td>1x</td><td>M√≠nima</td></tr>
      <tr><td>V1</td><td>@njit single pass</td><td>~3,200 ms</td><td>14x</td><td>M√≠nima</td></tr>
      <tr><td>V2</td><td>Sort + scan</td><td>~5,800 ms</td><td>8x</td><td>Sort buffer</td></tr>
      <tr><td>V3</td><td>Parallel histograms</td><td>~900 ms</td><td>50x</td><td>256 MB</td></tr>
      <tr><td>V4</td><td>Chunked parallel hist</td><td>~1,100 ms</td><td>41x</td><td>16 MB</td></tr>
    </table>
    <div class="perf">
      <strong>Lecci√≥n:</strong> El salto de 45s a 3.2s viene de fusionar 4 passes en 1 (eliminar relecturas de RAM). El salto de 3.2s a 0.9s viene de paralelizar con histogramas thread-local (eliminar race conditions sin locks). V4 es la versi√≥n production-ready: r√°pida Y con memoria controlada.
    </div>

    <!-- ============================================ -->
    <!-- ============ CASO 2: SLIDING WINDOW ======= -->
    <!-- ============================================ -->
    <h3 style="color: #58a6ff; border-top: 3px solid #58a6ff; padding-top: 12px;">
      üìà Caso 2: Sliding window sobre series temporales masivas
    </h3>
    <p><strong>El problema:</strong> Tienes 5,000 series temporales de 500,000 puntos cada una (ej: cotizaciones de 2 a√±os a resoluci√≥n de segundo). Para cada serie necesitas calcular una media m√≥vil exponencial (EMA), bandas de Bollinger, y el Z-score rolling ‚Äî todo de una sola pasada.</p>

    <h4>V0 ‚Äî Python puro (baseline para entender la l√≥gica)</h4>
<pre><code><span class="kw">def</span> <span class="fn">indicators_python</span>(prices, window):
    <span class="cm">"""Calcula EMA, Bollinger Bands y Z-score. Python puro."""</span>
    n_series, n_points = prices.shape
    ema    = np.empty_like(prices)
    upper  = np.empty_like(prices)
    lower  = np.empty_like(prices)
    zscore = np.empty_like(prices)

    alpha = <span class="num">2.0</span> / (window + <span class="num">1</span>)

    <span class="kw">for</span> s <span class="kw">in</span> <span class="fn">range</span>(n_series):
        <span class="cm"># EMA inicial</span>
        ema[s, <span class="num">0</span>] = prices[s, <span class="num">0</span>]
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, n_points):
            ema[s, i] = alpha * prices[s, i] + (<span class="num">1</span> - alpha) * ema[s, i-<span class="num">1</span>]

        <span class="cm"># Rolling mean y std para Bollinger</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_points):
            start = max(<span class="num">0</span>, i - window + <span class="num">1</span>)
            segment = prices[s, start:i+<span class="num">1</span>]
            mean = np.mean(segment)
            std = np.std(segment)
            upper[s, i] = mean + <span class="num">2</span> * std
            lower[s, i] = mean - <span class="num">2</span> * std
            zscore[s, i] = (prices[s, i] - mean) / std <span class="kw">if</span> std > <span class="num">0</span> <span class="kw">else</span> <span class="num">0</span>

    <span class="kw">return</span> ema, upper, lower, zscore

<span class="cm"># Con 5000 series √ó 500K puntos: HORAS (demasiado lento para medir)</span>
<span class="cm"># Bottleneck: O(N√óW) por serie para rolling mean/std (recalcula todo el window)</span></code></pre>

    <h4>V1 ‚Äî @njit con algoritmo O(N) para rolling stats (Welford online)</h4>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">indicators_v1</span>(prices, window):
    <span class="cm">"""Una serie a la vez. Rolling stats en O(N) con sumas acumuladas."""</span>
    n_series, n_points = prices.shape
    ema    = np.empty_like(prices)
    upper  = np.empty_like(prices)
    lower  = np.empty_like(prices)
    zscore = np.empty_like(prices)

    alpha = <span class="num">2.0</span> / (window + <span class="num">1</span>)

    <span class="kw">for</span> s <span class="kw">in</span> <span class="fn">range</span>(n_series):
        <span class="cm"># EMA: O(N) ‚Äî dependencia secuencial (NO paralelizable por punto)</span>
        ema[s, <span class="num">0</span>] = prices[s, <span class="num">0</span>]
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, n_points):
            ema[s, i] = alpha * prices[s, i] + (<span class="num">1</span> - alpha) * ema[s, i - <span class="num">1</span>]

        <span class="cm"># Rolling mean y variance con sumas deslizantes: O(N)</span>
        <span class="cm"># En vez de recalcular la media de scratch, mantenemos sum y sum_sq</span>
        win_sum = <span class="num">0.0</span>
        win_sum_sq = <span class="num">0.0</span>

        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_points):
            <span class="cm"># A√±adir nuevo elemento</span>
            win_sum += prices[s, i]
            win_sum_sq += prices[s, i] * prices[s, i]

            <span class="cm"># Remover elemento que sale de la ventana</span>
            <span class="kw">if</span> i >= window:
                old = prices[s, i - window]
                win_sum -= old
                win_sum_sq -= old * old

            <span class="cm"># Calcular estad√≠sticas</span>
            count = <span class="fn">min</span>(i + <span class="num">1</span>, window)
            mean = win_sum / count
            var = (win_sum_sq / count) - mean * mean
            <span class="kw">if</span> var < <span class="num">0.0</span>:  <span class="cm"># protecci√≥n contra error num√©rico</span>
                var = <span class="num">0.0</span>
            std = np.sqrt(var)

            upper[s, i] = mean + <span class="num">2.0</span> * std
            lower[s, i] = mean - <span class="num">2.0</span> * std
            zscore[s, i] = (prices[s, i] - mean) / std <span class="kw">if</span> std > <span class="num">1e-15</span> <span class="kw">else</span> <span class="num">0.0</span>

    <span class="kw">return</span> ema, upper, lower, zscore

<span class="cm"># Benchmark V1 (5000 √ó 500K):</span>
<span class="cm"># ~18 segundos</span>
<span class="cm"># Mejora algor√≠tmica: de O(N√óW) a O(N) ‚Üí enormemente m√°s r√°pido</span>
<span class="cm"># Pero: secuencial sobre las 5000 series</span></code></pre>

    <h4>V2 ‚Äî Paralelizar sobre series + pre-alocar + cache-friendly</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">indicators_v2</span>(prices, window):
    <span class="cm">"""Paralelo sobre series. Cada serie se procesa en un thread."""</span>
    n_series, n_points = prices.shape
    ema    = np.empty_like(prices)
    upper  = np.empty_like(prices)
    lower  = np.empty_like(prices)
    zscore = np.empty_like(prices)

    alpha = <span class="num">2.0</span> / (window + <span class="num">1</span>)
    one_minus_alpha = <span class="num">1.0</span> - alpha

    <span class="cm"># Paralelizar sobre series (cada una es independiente)</span>
    <span class="kw">for</span> s <span class="kw">in</span> prange(n_series):
        <span class="cm"># ‚îÄ‚îÄ EMA ‚îÄ‚îÄ</span>
        ema[s, <span class="num">0</span>] = prices[s, <span class="num">0</span>]
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, n_points):
            ema[s, i] = alpha * prices[s, i] + one_minus_alpha * ema[s, i - <span class="num">1</span>]

        <span class="cm"># ‚îÄ‚îÄ Rolling stats O(N) ‚îÄ‚îÄ</span>
        win_sum = <span class="num">0.0</span>
        win_sum_sq = <span class="num">0.0</span>

        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_points):
            p = prices[s, i]
            win_sum += p
            win_sum_sq += p * p

            <span class="kw">if</span> i >= window:
                old = prices[s, i - window]
                win_sum -= old
                win_sum_sq -= old * old

            count = <span class="fn">min</span>(i + <span class="num">1</span>, window)
            inv_count = <span class="num">1.0</span> / count         <span class="cm"># multiplicar es m√°s r√°pido que dividir</span>
            mean = win_sum * inv_count
            var = win_sum_sq * inv_count - mean * mean
            <span class="kw">if</span> var < <span class="num">0.0</span>:
                var = <span class="num">0.0</span>
            std = np.sqrt(var)

            upper[s, i] = mean + <span class="num">2.0</span> * std
            lower[s, i] = mean - <span class="num">2.0</span> * std
            zscore[s, i] = (p - mean) / std <span class="kw">if</span> std > <span class="num">1e-15</span> <span class="kw">else</span> <span class="num">0.0</span>

    <span class="kw">return</span> ema, upper, lower, zscore

<span class="cm"># Benchmark V2 (8 cores):</span>
<span class="cm"># ~2.8 segundos ‚Äî 6.4x vs V1</span>
<span class="cm"># 5000 series / 8 threads = 625 series/thread ‚Üí excelente granularidad</span></code></pre>

    <h4>V3 ‚Äî Transponer datos + float32 (memoria y SIMD al m√°ximo)</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">indicators_v3</span>(prices_T, window):
    <span class="cm">"""Datos transpuestos: shape (n_points, n_series).
    Ahora el loop interno recorre series contiguas en memoria ‚Üí SIMD."""</span>
    n_points, n_series = prices_T.shape
    ema_T    = np.empty_like(prices_T)
    upper_T  = np.empty_like(prices_T)
    lower_T  = np.empty_like(prices_T)
    zscore_T = np.empty_like(prices_T)

    alpha = np.float32(<span class="num">2.0</span>) / np.float32(window + <span class="num">1</span>)
    one_minus_alpha = np.float32(<span class="num">1.0</span>) - alpha

    <span class="cm"># EMA: loop por tiempo, dentro vectorizado sobre series</span>
    <span class="kw">for</span> s <span class="kw">in</span> prange(n_series):
        ema_T[<span class="num">0</span>, s] = prices_T[<span class="num">0</span>, s]
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, n_points):
        <span class="kw">for</span> s <span class="kw">in</span> prange(n_series):
            ema_T[i, s] = alpha * prices_T[i, s] + one_minus_alpha * ema_T[i-<span class="num">1</span>, s]

    <span class="cm"># Rolling stats: paralelo sobre series, secuencial por tiempo</span>
    <span class="kw">for</span> s <span class="kw">in</span> prange(n_series):
        win_sum = np.float32(<span class="num">0.0</span>)
        win_sum_sq = np.float32(<span class="num">0.0</span>)

        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_points):
            p = prices_T[i, s]
            win_sum += p
            win_sum_sq += p * p

            <span class="kw">if</span> i >= window:
                old = prices_T[i - window, s]
                win_sum -= old
                win_sum_sq -= old * old

            count = <span class="fn">min</span>(i + <span class="num">1</span>, window)
            inv_c = np.float32(<span class="num">1.0</span>) / np.float32(count)
            mean = win_sum * inv_c
            var = win_sum_sq * inv_c - mean * mean
            <span class="kw">if</span> var < np.float32(<span class="num">0.0</span>):
                var = np.float32(<span class="num">0.0</span>)
            std = np.sqrt(var)

            upper_T[i, s] = mean + np.float32(<span class="num">2.0</span>) * std
            lower_T[i, s] = mean - np.float32(<span class="num">2.0</span>) * std
            zscore_T[i, s] = (p - mean) / std <span class="kw">if</span> std > np.float32(<span class="num">1e-7</span>) <span class="kw">else</span> np.float32(<span class="num">0.0</span>)

    <span class="kw">return</span> ema_T, upper_T, lower_T, zscore_T

<span class="cm"># Uso: transponer a float32 antes</span>
<span class="cm"># prices_T = prices.T.astype(np.float32).copy()  # .copy() para C-contiguous</span>
<span class="cm"># results = indicators_v3(prices_T, 20)</span>

<span class="cm"># Benchmark V3 (8 cores, float32):</span>
<span class="cm"># ~0.9 segundos</span>
<span class="cm"># float32 = mitad de memoria + doble SIMD width (8 floats vs 4 doubles)</span></code></pre>

    <table>
      <tr><th>Versi√≥n</th><th>T√©cnica</th><th>Tiempo</th><th>Speedup acumulado</th></tr>
      <tr><td>V0</td><td>Python puro O(N√óW)</td><td>Horas+</td><td>1x</td></tr>
      <tr><td>V1</td><td>@njit + O(N) rolling</td><td>~18,000 ms</td><td>~200x+</td></tr>
      <tr><td>V2</td><td>+ parallel sobre series</td><td>~2,800 ms</td><td>~1,300x+</td></tr>
      <tr><td>V3</td><td>+ float32 + transponer</td><td>~900 ms</td><td>~4,000x+</td></tr>
    </table>

    <div class="perf">
      <strong>Lecci√≥n:</strong> El salto m√°s grande (200x) viene del <strong>cambio algor√≠tmico</strong> (O(N√óW) ‚Üí O(N)). Luego la paralelizaci√≥n da 6x y float32+layout dan 3x m√°s. Siempre optimiza el algoritmo primero, hardware despu√©s.
    </div>

    <!-- ============================================ -->
    <!-- ============ CASO 3: LOOKUP / JOIN ======== -->
    <!-- ============================================ -->
    <h3 style="color: #3fb950; border-top: 3px solid #3fb950; padding-top: 12px;">
      üîó Caso 3: Join/Lookup masivo ‚Äî Hash table en Numba
    </h3>
    <p><strong>El problema:</strong> Tienes 50M de eventos con un <code>user_id</code> y necesitas "enriquecer" cada evento con datos del perfil del usuario (lookup table de 2M usuarios). Es esencialmente un LEFT JOIN a velocidad de C.</p>

    <h4>V0 ‚Äî NumPy searchsorted (baseline r√°pido)</h4>
<pre><code><span class="cm"># Datos</span>
N_EVENTS = <span class="num">50_000_000</span>
N_USERS = <span class="num">2_000_000</span>

event_user_ids = np.random.randint(<span class="num">0</span>, N_USERS, size=N_EVENTS, dtype=np.int64)
<span class="cm"># Lookup table: user_id ‚Üí (age, score, region)</span>
user_ages   = np.random.randint(<span class="num">18</span>, <span class="num">80</span>, size=N_USERS, dtype=np.int32)
user_scores = np.random.rand(N_USERS).astype(np.float32)
user_regions = np.random.randint(<span class="num">0</span>, <span class="num">50</span>, size=N_USERS, dtype=np.int8)

<span class="cm"># V0: Direct indexing (los user_ids ya son √≠ndices 0..N_USERS-1)</span>
<span class="kw">def</span> <span class="fn">lookup_numpy</span>(event_ids, ages, scores, regions):
    <span class="cm"># Si los IDs son contiguos 0..N, esto es un simple fancy indexing</span>
    e_ages = ages[event_ids]
    e_scores = scores[event_ids]
    e_regions = regions[event_ids]
    <span class="kw">return</span> e_ages, e_scores, e_regions

<span class="cm"># Benchmark V0:</span>
<span class="cm"># ~850 ms (3 passes, cada uno hace 50M random accesses)</span>
<span class="cm"># Bottleneck: 3 passes separados √ó acceso aleatorio = 3√ó cache misses</span></code></pre>

    <h4>V1 ‚Äî Fusionar en un solo pass (eliminar relecturas)</h4>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">lookup_v1</span>(event_ids, ages, scores, regions):
    <span class="cm">"""Un solo pass: por cada evento, buscar todos los atributos a la vez."""</span>
    n = <span class="fn">len</span>(event_ids)
    e_ages    = np.empty(n, dtype=np.int32)
    e_scores  = np.empty(n, dtype=np.float32)
    e_regions = np.empty(n, dtype=np.int8)

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        uid = event_ids[i]
        e_ages[i]    = ages[uid]
        e_scores[i]  = scores[uid]
        e_regions[i] = regions[uid]

    <span class="kw">return</span> e_ages, e_scores, e_regions

<span class="cm"># Benchmark V1: ~420 ms ‚Äî 2x vs V0</span>
<span class="cm"># ¬øPor qu√© solo 2x con 3 passes ‚Üí 1? Porque ages, scores, regions</span>
<span class="cm"># est√°n en direcciones diferentes ‚Üí el acceso a ages[uid] no trae</span>
<span class="cm"># scores[uid] al cache. Cada uno sigue siendo un random access.</span></code></pre>

    <h4>V2 ‚Äî Structure of Arrays ‚Üí Array of Structures (SoA ‚Üí AoS)</h4>
<pre><code><span class="cm"># AQU√ç aplicamos el patr√≥n INVERSO al caso de part√≠culas</span>
<span class="cm"># En el Caso 1, SoA era mejor porque proces√°bamos UN atributo a la vez</span>
<span class="cm"># En lookup, procesamos TODOS los atributos de un usuario a la vez</span>
<span class="cm"># ‚Üí AoS es mejor porque todos los datos del usuario est√°n juntos en cache</span>

<span class="cm"># Empaquetar perfil del usuario en una sola estructura contigua</span>
<span class="cm"># Usar un array 2D donde cada fila es un usuario</span>
<span class="cm"># Para m√°xima localidad, almacenar todo como float32</span>
user_profiles = np.empty((N_USERS, <span class="num">3</span>), dtype=np.float32)
user_profiles[:, <span class="num">0</span>] = user_ages.astype(np.float32)
user_profiles[:, <span class="num">1</span>] = user_scores
user_profiles[:, <span class="num">2</span>] = user_regions.astype(np.float32)
<span class="cm"># Ahora: user_profiles[uid] = [age, score, region] contiguos en 12 bytes</span>
<span class="cm"># ¬°Caben en UNA cache line! Un solo cache miss trae TODO el perfil.</span>

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">lookup_v2</span>(event_ids, profiles):
    <span class="cm">"""Lookup con datos empaquetados: un cache miss trae todo el perfil."""</span>
    n = <span class="fn">len</span>(event_ids)
    results = np.empty((n, <span class="num">3</span>), dtype=np.float32)

    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        uid = event_ids[i]
        results[i, <span class="num">0</span>] = profiles[uid, <span class="num">0</span>]
        results[i, <span class="num">1</span>] = profiles[uid, <span class="num">1</span>]
        results[i, <span class="num">2</span>] = profiles[uid, <span class="num">2</span>]

    <span class="kw">return</span> results

<span class="cm"># Benchmark V2: ~180 ms ‚Äî 4.7x vs V0</span>
<span class="cm"># Los 3 atributos del mismo usuario est√°n en la misma cache line</span>
<span class="cm"># ‚Üí 1 cache miss en vez de 3 por evento</span></code></pre>

    <h4>V3 ‚Äî Prefetch sorting (ordenar eventos para maximizar cache hits)</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">lookup_v3</span>(event_ids, profiles):
    <span class="cm">"""Ordenar eventos por user_id ‚Üí accesos a profiles se vuelven
    semi-secuenciales. Usuarios populares ya est√°n en cache."""</span>
    n = <span class="fn">len</span>(event_ids)

    <span class="cm"># Paso 1: Sort indices por user_id</span>
    order = np.argsort(event_ids)

    <span class="cm"># Paso 2: Lookup en orden sorted ‚Üí localidad temporal masiva</span>
    results = np.empty((n, <span class="num">3</span>), dtype=np.float32)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        orig_idx = order[i]
        uid = event_ids[orig_idx]
        results[orig_idx, <span class="num">0</span>] = profiles[uid, <span class="num">0</span>]
        results[orig_idx, <span class="num">1</span>] = profiles[uid, <span class="num">1</span>]
        results[orig_idx, <span class="num">2</span>] = profiles[uid, <span class="num">2</span>]

    <span class="kw">return</span> results

<span class="cm"># Benchmark V3: ~290 ms (sort ~200ms + lookup ~90ms)</span>
<span class="cm"># El lookup puro es 2x m√°s r√°pido (90 vs 180ms)</span>
<span class="cm"># Pero el sort cuesta 200ms. Solo gana si haces MUCHOS lookups con los mismos IDs</span>
<span class="cm"># En pipelines donde el mismo dataset se procesa repetidamente ‚Üí pre-sort una vez</span></code></pre>

    <table>
      <tr><th>Versi√≥n</th><th>T√©cnica</th><th>Tiempo</th><th>Speedup</th></tr>
      <tr><td>V0</td><td>NumPy fancy indexing √ó 3</td><td>~850 ms</td><td>1x</td></tr>
      <tr><td>V1</td><td>@njit single pass</td><td>~420 ms</td><td>2x</td></tr>
      <tr><td>V2</td><td>AoS packing + parallel</td><td>~180 ms</td><td>4.7x</td></tr>
      <tr><td>V3</td><td>Pre-sort + packed</td><td>~90 ms (lookup only)</td><td>9.4x (amortizado)</td></tr>
    </table>

    <div class="perf">
      <strong>Lecci√≥n:</strong> En lookups aleatorios, el layout de datos importa m√°s que la paralelizaci√≥n. Empaquetar los datos que se acceden juntos (AoS) reduce cache misses 3x. Pre-ordenar las queries convierte acceso aleatorio en semi-secuencial, pero solo rinde si amortizas el sort sobre m√∫ltiples operaciones.
    </div>

    <!-- ============================================ -->
    <!-- ============ CASO 4: PIPELINE ============= -->
    <!-- ============================================ -->
    <h3 style="color: #d2a8ff; border-top: 3px solid #d2a8ff; padding-top: 12px;">
      üîÑ Caso 4: Pipeline multi-etapa ‚Äî ETL en tiempo real
    </h3>
    <p><strong>El problema:</strong> Un pipeline de procesamiento de se√±ales de sensores IoT. Flujo: datos crudos (20M muestras √ó 8 sensores) ‚Üí filtrado de outliers ‚Üí normalizaci√≥n ‚Üí detecci√≥n de anomal√≠as ‚Üí estad√≠sticas por ventana temporal. Cada etapa alimenta la siguiente.</p>

    <h4>V0 ‚Äî Pipeline ingenuo: cada etapa por separado</h4>
<pre><code><span class="cm"># Datos: 20M muestras √ó 8 sensores</span>
N_SAMPLES = <span class="num">20_000_000</span>
N_SENSORS = <span class="num">8</span>
raw_data = np.random.randn(N_SAMPLES, N_SENSORS).astype(np.float32) * <span class="num">100</span>
<span class="cm"># Inyectar outliers</span>
outlier_mask = np.random.rand(N_SAMPLES, N_SENSORS) < <span class="num">0.001</span>
raw_data[outlier_mask] *= <span class="num">100</span>

<span class="cm"># V0: Cada etapa lee y escribe un array completo</span>
<span class="kw">def</span> <span class="fn">pipeline_v0</span>(data, window=<span class="num">1000</span>):
    <span class="cm"># Etapa 1: Filtrar outliers (reemplazar con NaN ‚Üí interpolar)</span>
    cleaned = data.copy()                        <span class="cm"># 640MB copia</span>
    means = np.mean(data, axis=<span class="num">0</span>)
    stds = np.std(data, axis=<span class="num">0</span>)
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(N_SENSORS):
        mask = np.abs(data[:, j] - means[j]) > <span class="num">5</span> * stds[j]
        cleaned[mask, j] = means[j]

    <span class="cm"># Etapa 2: Normalizar a Z-score</span>
    normalized = np.empty_like(cleaned)           <span class="cm"># 640MB m√°s</span>
    <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(N_SENSORS):
        m = np.mean(cleaned[:, j])
        s = np.std(cleaned[:, j])
        normalized[:, j] = (cleaned[:, j] - m) / s

    <span class="cm"># Etapa 3: Detecci√≥n de anomal√≠as (score combinado)</span>
    anomaly_scores = np.sqrt(np.sum(normalized ** <span class="num">2</span>, axis=<span class="num">1</span>))  <span class="cm"># 160MB</span>

    <span class="cm"># Etapa 4: Estad√≠sticas por ventana</span>
    n_windows = N_SAMPLES // window
    window_stats = np.empty((n_windows, <span class="num">4</span>))    <span class="cm"># mean, std, max_anomaly, anomaly_rate</span>
    <span class="kw">for</span> w <span class="kw">in</span> <span class="fn">range</span>(n_windows):
        s = w * window
        e = s + window
        scores = anomaly_scores[s:e]
        window_stats[w, <span class="num">0</span>] = np.mean(scores)
        window_stats[w, <span class="num">1</span>] = np.std(scores)
        window_stats[w, <span class="num">2</span>] = np.max(scores)
        window_stats[w, <span class="num">3</span>] = np.sum(scores > <span class="num">3.0</span>) / window

    <span class="kw">return</span> anomaly_scores, window_stats

<span class="cm"># Benchmark V0:</span>
<span class="cm"># ~12 segundos, pico de memoria ~2.5 GB</span>
<span class="cm"># 4 passes completos sobre 640MB + arrays temporales gigantes</span></code></pre>

    <h4>V1 ‚Äî Pipeline fusionado: una sola pasada sobre los datos</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">pipeline_v1</span>(data, window):
    <span class="cm">"""TODAS las etapas fusionadas en un solo pass por ventana.
    Cada ventana se procesa completa: clean ‚Üí normalize ‚Üí score ‚Üí stats.
    Los datos de una ventana caben en cache: 1000 √ó 8 √ó 4 = 32KB (L1!)"""</span>
    n_samples, n_sensors = data.shape
    n_windows = n_samples // window

    <span class="cm"># Pre-calcular estad√≠sticas globales para cleaning</span>
    <span class="cm"># (necesitamos un pre-pass, pero solo sobre 8 columnas)</span>
    global_means = np.zeros(n_sensors, dtype=np.float32)
    global_stds  = np.zeros(n_sensors, dtype=np.float32)

    <span class="kw">for</span> j <span class="kw">in</span> prange(n_sensors):
        s = np.float32(<span class="num">0.0</span>)
        sq = np.float32(<span class="num">0.0</span>)
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_samples):
            v = data[i, j]
            s += v
            sq += v * v
        inv_n = np.float32(<span class="num">1.0</span>) / np.float32(n_samples)
        global_means[j] = s * inv_n
        var = sq * inv_n - (s * inv_n) ** <span class="num">2</span>
        global_stds[j] = np.sqrt(var) <span class="kw">if</span> var > <span class="num">0</span> <span class="kw">else</span> np.float32(<span class="num">1.0</span>)

    <span class="cm"># Output arrays</span>
    anomaly_scores = np.empty(n_samples, dtype=np.float32)
    window_stats = np.empty((n_windows, <span class="num">4</span>), dtype=np.float32)

    <span class="cm"># Precalcular means/stds de datos limpios para normalizaci√≥n</span>
    <span class="cm"># (en producci√≥n esto vendr√≠a de datos hist√≥ricos, no del batch actual)</span>
    clean_means = global_means.copy()
    clean_stds = global_stds.copy()
    threshold = np.float32(<span class="num">5.0</span>)
    anomaly_thresh = np.float32(<span class="num">3.0</span>)

    <span class="cm"># ‚îÄ‚îÄ LOOP PRINCIPAL: paralelo por ventanas ‚îÄ‚îÄ</span>
    <span class="kw">for</span> w <span class="kw">in</span> prange(n_windows):
        w_start = w * window
        w_end = w_start + window

        <span class="cm"># Acumuladores de la ventana</span>
        score_sum = np.float32(<span class="num">0.0</span>)
        score_sq_sum = np.float32(<span class="num">0.0</span>)
        score_max = np.float32(<span class="num">0.0</span>)
        anomaly_count = <span class="num">0</span>

        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(w_start, w_end):
            <span class="cm"># ‚îÄ‚îÄ Etapa 1+2+3 FUSIONADAS por muestra ‚îÄ‚îÄ</span>
            score_sq_total = np.float32(<span class="num">0.0</span>)

            <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(n_sensors):
                val = data[i, j]

                <span class="cm"># Clean: reemplazar outliers</span>
                diff = val - global_means[j]
                <span class="kw">if</span> diff > threshold * global_stds[j] <span class="kw">or</span> diff < -threshold * global_stds[j]:
                    val = global_means[j]

                <span class="cm"># Normalize</span>
                z = (val - clean_means[j]) / clean_stds[j]

                <span class="cm"># Accumulate anomaly score</span>
                score_sq_total += z * z

            <span class="cm"># Score para esta muestra</span>
            score = np.sqrt(score_sq_total)
            anomaly_scores[i] = score

            <span class="cm"># Etapa 4: acumular stats de ventana</span>
            score_sum += score
            score_sq_sum += score * score
            <span class="kw">if</span> score > score_max:
                score_max = score
            <span class="kw">if</span> score > anomaly_thresh:
                anomaly_count += <span class="num">1</span>

        <span class="cm"># Escribir stats de la ventana</span>
        inv_w = np.float32(<span class="num">1.0</span>) / np.float32(window)
        w_mean = score_sum * inv_w
        w_var = score_sq_sum * inv_w - w_mean * w_mean
        window_stats[w, <span class="num">0</span>] = w_mean
        window_stats[w, <span class="num">1</span>] = np.sqrt(w_var) <span class="kw">if</span> w_var > <span class="num">0</span> <span class="kw">else</span> np.float32(<span class="num">0.0</span>)
        window_stats[w, <span class="num">2</span>] = score_max
        window_stats[w, <span class="num">3</span>] = np.float32(anomaly_count) * inv_w

    <span class="kw">return</span> anomaly_scores, window_stats

<span class="cm"># Benchmark V1:</span>
<span class="cm"># ~0.45 segundos ‚Äî 27x vs V0</span>
<span class="cm"># Pico de memoria: ~200MB (vs 2.5GB de V0)</span>
<span class="cm"># ¬øPOR QU√â tan r√°pido?</span>
<span class="cm"># 1. UN solo pass sobre los datos (no 4)</span>
<span class="cm"># 2. Cada ventana (1000√ó8√ó4 = 32KB) cabe ENTERA en L1 cache</span>
<span class="cm"># 3. CERO arrays temporales intermedios</span>
<span class="cm"># 4. 20K ventanas en prange ‚Üí excelente granularidad paralela</span>
<span class="cm"># 5. fastmath + float32 ‚Üí SIMD procesa 8 valores por instrucci√≥n</span></code></pre>

    <table>
      <tr><th>Versi√≥n</th><th>Tiempo</th><th>Memoria pico</th><th>Speedup</th><th>Passes sobre datos</th></tr>
      <tr><td>V0 (NumPy etapas separadas)</td><td>~12,000 ms</td><td>~2.5 GB</td><td>1x</td><td>4+ passes</td></tr>
      <tr><td>V1 (Fusionado + parallel)</td><td>~450 ms</td><td>~200 MB</td><td>27x</td><td>1 pass + 1 pre-pass</td></tr>
    </table>

    <div class="perf">
      <strong>Lecci√≥n fundamental:</strong> La fusi√≥n de etapas es el arma m√°s poderosa en pipelines. V0 lee 640MB √ó 4 etapas = 2.5GB de RAM. V1 lee 640MB √ó 1 vez. Solo ese cambio da ~4x. A√±ade que cada ventana cabe en L1, no hay temporales, y 20K chunks paralelos, y llegas a 27x. Este patr√≥n ‚Äî fusionar todo lo que se pueda en un solo pass con chunks cache-friendly ‚Äî es c√≥mo se construyen los motores de procesamiento de datos m√°s r√°pidos del mundo.
    </div>

    <!-- ============================================ -->
    <!-- ============ RESUMEN META ================= -->
    <!-- ============================================ -->
    <h3>üß¨ Meta-patrones: los principios que conectan todos los casos</h3>

    <p>Si analizas los 4 casos, emergen principios universales que aplican a CUALQUIER problema de datos masivos:</p>

    <table>
      <tr><th>#</th><th>Principio</th><th>Efecto t√≠pico</th><th>D√≥nde se vio</th></tr>
      <tr><td>1</td><td><strong>Reducir passes sobre los datos</strong> (fusionar operaciones)</td><td>Nx (N = passes eliminados)</td><td>Todos los casos</td></tr>
      <tr><td>2</td><td><strong>Eliminar arrays temporales</strong> (fusionar en un loop)</td><td>2-5x + reducci√≥n de memoria 3-10x</td><td>Caso 1 (V1), Caso 4 (V1)</td></tr>
      <tr><td>3</td><td><strong>Mejorar localidad de cache</strong> (AoS/SoA seg√∫n patr√≥n)</td><td>2-8x</td><td>Caso 3 (V2)</td></tr>
      <tr><td>4</td><td><strong>Paralelizar sobre la dimensi√≥n con m√°s iteraciones</strong></td><td>~Nx (N = cores)</td><td>Caso 2 (series), Caso 4 (ventanas)</td></tr>
      <tr><td>5</td><td><strong>Chunk size = L1/L2 cache</strong></td><td>2-5x vs chunks grandes</td><td>Caso 1 (V4), Caso 4 (V1)</td></tr>
      <tr><td>6</td><td><strong>Cambio algor√≠tmico primero</strong> (O(N¬≤) ‚Üí O(N))</td><td>10-1000x</td><td>Caso 2 (V1): rolling O(N√óW) ‚Üí O(N)</td></tr>
      <tr><td>7</td><td><strong>float32 cuando la precisi√≥n lo permite</strong></td><td>2-3x (memoria + SIMD)</td><td>Caso 2 (V3), Caso 4 (V1)</td></tr>
      <tr><td>8</td><td><strong>Histogramas thread-local ‚Üí reduce</strong> (evitar locks)</td><td>Nx sin race conditions</td><td>Caso 1 (V3)</td></tr>
      <tr><td>9</td><td><strong>Pre-sort para convertir acceso aleatorio en secuencial</strong></td><td>2-3x en lookups (amortizado)</td><td>Caso 3 (V3)</td></tr>
      <tr><td>10</td><td><strong>Medir Throughput (Rendimiento real) (GB/s) para saber si est√°s memory-bound</strong></td><td>Evita optimizar CPU cuando el l√≠mite es RAM</td><td>Todos</td></tr>
    </table>

    <div class="tip">
      <strong>El proceso siempre es el mismo:</strong><br>
      1. Escribir versi√≥n correcta (aunque sea lenta)<br>
      2. Medir (benchmark + Throughput (Rendimiento real))<br>
      3. Identificar bottleneck (¬øCPU-bound? ¬ømemory-bound? ¬øalgor√≠tmico?)<br>
      4. Aplicar el principio correcto de la tabla<br>
      5. Medir de nuevo<br>
      6. Repetir hasta llegar al l√≠mite te√≥rico (Bandwidth (Ancho de banda) de RAM o pico de FLOPS)
    </div>

  </div>
</div>

<a href="#m-antipatterns">‚ò†Ô∏è Anti-Patterns a Escala: Trampas que Cuestan Horas</a>
<!--
  ============================================================
  M√ìDULO: Anti-Patterns a Escala ‚Äî Las Trampas que Cuestan Horas ‚ò†Ô∏è
  ============================================================
-->

<!-- ==================== M√ìDULO ANTI-PATTERNS ==================== -->
<div class="module" id="m-antipatterns">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">‚ò†Ô∏è</span> Anti-Patterns a Escala: Las Trampas Invisibles que Cuestan Horas</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Todo lo anterior te ense√±√≥ a hacer las cosas bien. Esta secci√≥n es diferente ‚Äî te ense√±a a <strong>no destruirte a ti mismo</strong>. Cada anti-pattern aqu√≠ parece razonable, pasa los tests, y funciona con datos peque√±os. Pero a escala masiva, cada uno es una bomba de tiempo que convierte minutos en horas y horas en d√≠as. Los he ordenado del m√°s com√∫n al m√°s sutil.</p>

    <!-- ============ ANTI-PATTERN 1 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #1 ‚Äî Llamar una funci√≥n parallel=True dentro de un loop</h3>
    <p><strong>Severidad: CATASTR√ìFICA</strong> | <strong>Frecuencia: MUY ALTA</strong></p>

    <p>Este es el anti-pattern m√°s devastador y m√°s com√∫n. Debido al overhead de dispatch (~1Œºs √ó millones de llamadas). Parece completamente inocente:</p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: parallel=True DENTRO de un loop Python que itera millones de veces</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_single_row</span>(row):
    <span class="cm">"""Procesa UNA fila con parallel=True."""</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(row)):
        total += np.sqrt(row[i]) * np.sin(row[i])
    <span class="kw">return</span> total

<span class="cm"># La llamada "inocente":</span>
data = np.random.rand(<span class="num">1_000_000</span>, <span class="num">50</span>)  <span class="cm"># 1M filas, 50 columnas</span>
results = np.empty(data.shape[<span class="num">0</span>])

<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(data.shape[<span class="num">0</span>]):          <span class="cm"># 1 MILL√ìN de iteraciones</span>
    results[i] = process_single_row(data[i])  <span class="cm"># ‚ò†Ô∏è cada llamada: dispatch + thread sync</span>

<span class="cm"># Tiempo: ~180 segundos</span>
<span class="cm"># POR QU√â: Cada llamada a process_single_row():</span>
<span class="cm">#   1. Python ‚Üí Numba dispatch: ~1Œºs (boxing/unboxing de argumentos)</span>
<span class="cm">#   2. Thread pool wake up: ~5-20Œºs (despertar N threads dormidos)</span>
<span class="cm">#   3. Work distribution: ~2Œºs (repartir 50 elementos entre 8 threads)</span>
<span class="cm">#   4. Thread sync barrier: ~5-10Œºs (esperar que todos terminen)</span>
<span class="cm">#   5. C√≥mputo real: ~0.1Œºs (50 sqrt+sin es NADA)</span>
<span class="cm"># Total overhead por llamada: ~30Œºs √ó 1M llamadas = 30 SEGUNDOS de puro overhead</span>
<span class="cm"># El c√≥mputo real total es ~0.1 segundos</span></code></pre>

    <h4>‚úÖ Soluci√≥n: mover el loop DENTRO de la funci√≥n paralela</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>, fastmath=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_all_rows</span>(data):
    <span class="cm">"""El loop sobre filas est√° DENTRO ‚Äî UN solo dispatch, UN solo thread sync."""</span>
    n_rows, n_cols = data.shape
    results = np.empty(n_rows)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n_rows):   <span class="cm"># 1M iteraciones paralelas</span>
        total = <span class="num">0.0</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(n_cols):
            total += np.sqrt(data[i, j]) * np.sin(data[i, j])
        results[i] = total
    <span class="kw">return</span> results

results = process_all_rows(data)
<span class="cm"># Tiempo: ~0.3 segundos ‚Äî 600x m√°s r√°pido</span>
<span class="cm"># UN dispatch + 1M iteraciones de prange = overhead despreciable</span></code></pre>

    <div class="warn">
      <strong>Regla de hierro:</strong> Nunca llames una funci√≥n <code>parallel=True</code> o <code>@njit</code> en un loop Python que itera m√°s de ~1000 veces. El overhead del dispatch Python‚ÜíNumba (~1Œºs) √ó N llamadas te destruye. Siempre mueve el loop DENTRO de la funci√≥n Numba.
    </div>

    <!-- ============ ANTI-PATTERN 2 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #2 ‚Äî Alocaciones dentro del loop caliente</h3>
    <p><strong>Severidad: ALTA</strong> | <strong>Frecuencia: MUY ALTA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: np.empty/np.zeros dentro de un loop que itera mucho</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">process_with_temp_bad</span>(data, n_iter):
    n = data.shape[<span class="num">0</span>]
    result = np.zeros(n)
    <span class="kw">for</span> iteration <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        temp = np.empty(n)             <span class="cm"># ‚ò†Ô∏è ALOCACI√ìN: syscall al OS</span>
        temp2 = np.zeros(n)            <span class="cm"># ‚ò†Ô∏è ALOCACI√ìN + memset a 0</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
            temp[i] = np.sin(data[i]) * iteration
            temp2[i] = temp[i] ** <span class="num">2</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
            result[i] += temp2[i]
    <span class="kw">return</span> result

<span class="cm"># Con n=100_000, n_iter=1000:</span>
<span class="cm"># Tiempo: ~4.8 segundos</span>
<span class="cm"># 1000 iteraciones √ó 2 alocaciones √ó 100K √ó 8 bytes = 1.6 GB total de alocaci√≥n</span>
<span class="cm"># Cada np.empty llama a NRT_MemInfo_alloc ‚Üí malloc ‚Üí puede causar page fault</span>
<span class="cm"># Cada np.zeros adem√°s hace memset (llena de ceros 800KB)</span>

<span class="cm"># ‚úÖ CORRECTO: pre-alocar fuera del loop, reutilizar</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">process_with_temp_good</span>(data, n_iter):
    n = data.shape[<span class="num">0</span>]
    result = np.zeros(n)
    temp = np.empty(n)        <span class="cm"># ‚úÖ UNA vez</span>
    temp2 = np.empty(n)       <span class="cm"># ‚úÖ UNA vez (ni siquiera necesita zeros)</span>
    <span class="kw">for</span> iteration <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
            temp[i] = np.sin(data[i]) * iteration
            temp2[i] = temp[i] ** <span class="num">2</span>
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
            result[i] += temp2[i]
    <span class="kw">return</span> result

<span class="cm"># Tiempo: ~1.2 segundos ‚Äî 4x m√°s r√°pido</span>

<span class="cm"># ‚ö° A√öN MEJOR: eliminar temporales completamente fusionando loops</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">process_fused</span>(data, n_iter):
    n = data.shape[<span class="num">0</span>]
    result = np.zeros(n)
    <span class="kw">for</span> iteration <span class="kw">in</span> <span class="fn">range</span>(n_iter):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
            val = np.sin(data[i]) * iteration  <span class="cm"># en registro, no en array</span>
            result[i] += val * val              <span class="cm"># directo, sin temporal</span>
    <span class="kw">return</span> result

<span class="cm"># Tiempo: ~0.8 segundos ‚Äî 6x vs original</span></code></pre>

    <!-- ============ ANTI-PATTERN 3 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #3 ‚Äî parallel=True silenciosamente no hizo nada</h3>
    <p><strong>Severidad: ALTA</strong> | <strong>Frecuencia: ALTA</strong></p>

    <p>Este es el m√°s traicionero: tu c√≥digo compila, ejecuta, produce resultados correctos... pero <code>parallel=True</code> no paraleliz√≥ absolutamente nada. Usas 1 core de 8 sin saberlo. ¬øY sabes por qu√©? porque Numba no avisa cuando falla la paralelizaci√≥n...</p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA 3A: usar range() en vez de prange()</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">trap_range</span>(data):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):    <span class="cm"># ‚ò†Ô∏è range, NO prange ‚Üí secuencial</span>
        total += data[i] ** <span class="num">2</span>
    <span class="kw">return</span> total
<span class="cm"># parallel=True est√° activado pero NO se usa</span>
<span class="cm"># Numba NO da error, NO da warning (en la mayor√≠a de los casos)</span>

<span class="cm"># ‚ò†Ô∏è TRAMPA 3B: dependencia entre iteraciones que impide paralelizar</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">trap_dependency</span>(data):
    result = np.empty(<span class="fn">len</span>(data))
    result[<span class="num">0</span>] = data[<span class="num">0</span>]
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="num">1</span>, <span class="fn">len</span>(data)):
        result[i] = result[i-<span class="num">1</span>] + data[i]  <span class="cm"># ‚ò†Ô∏è depende del valor anterior</span>
    <span class="kw">return</span> result
<span class="cm"># Numba PUEDE serializar el prange sin avisarte</span>
<span class="cm"># El resultado ser√° CORRECTO (ejecuta secuencialmente) pero sin speedup</span>

<span class="cm"># ‚ò†Ô∏è TRAMPA 3C: operaciones sobre arrays sin parallel sem√°ntico</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">trap_no_parallel_ops</span>(data):
    <span class="cm"># np.sort NO tiene sem√°ntica paralela en Numba</span>
    sorted_data = np.sort(data)
    <span class="cm"># np.searchsorted tampoco</span>
    idx = np.searchsorted(sorted_data, <span class="num">0.5</span>)
    <span class="kw">return</span> sorted_data, idx
<span class="cm"># parallel=True no hace nada aqu√≠ ‚Äî estas ops no se paralelizan</span>

<span class="cm"># ‚ò†Ô∏è TRAMPA 3D: prange en el loop INTERNO (se serializa)</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">trap_inner_prange</span>(matrix):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> prange(matrix.shape[<span class="num">0</span>]):         <span class="cm"># ‚úÖ paralelo</span>
        <span class="kw">for</span> j <span class="kw">in</span> prange(matrix.shape[<span class="num">1</span>]):     <span class="cm"># ‚ò†Ô∏è SERIALIZADO silenciosamente</span>
            total += matrix[i, j]
    <span class="kw">return</span> total
<span class="cm"># Si tienes 4 filas y 10,000 columnas ‚Üí solo 4 threads trabajan</span></code></pre>

    <h4>‚úÖ SIEMPRE verificar con parallel_diagnostics()</h4>
<pre><code><span class="cm"># Despu√©s de CADA funci√≥n parallel=True que escribas, haz esto:</span>
trap_range(np.random.rand(<span class="num">100</span>))
trap_range.parallel_diagnostics(level=<span class="num">1</span>)
<span class="cm"># Si NO ves loops marcados como "parallel" ‚Üí no se paraleliz√≥ NADA</span>
<span class="cm"># Si ves "(serial)" donde esperabas "(parallel)" ‚Üí Numba lo serializ√≥</span>

<span class="cm"># Automatizar la verificaci√≥n:</span>
<span class="kw">def</span> <span class="fn">verify_parallel</span>(func, *args):
    <span class="cm">"""Verifica que una funci√≥n parallel=True realmente paraleliza."""</span>
    <span class="kw">import</span> io, sys
    func(*args)  <span class="cm"># compilar</span>

    <span class="cm"># Capturar output de parallel_diagnostics</span>
    old_stdout = sys.stdout
    sys.stdout = buf = io.StringIO()
    <span class="kw">try</span>:
        func.parallel_diagnostics(level=<span class="num">1</span>)
    <span class="kw">except</span>:
        <span class="fn">print</span>(<span class="st">"NO PARALLEL"</span>, file=sys.stderr)
    sys.stdout = old_stdout
    output = buf.getvalue()

    <span class="kw">if</span> <span class="st">'parallel'</span> <span class="kw">not in</span> output.lower():
        <span class="fn">print</span>(f<span class="st">"‚ö†Ô∏è {func.py_func.__name__}: parallel=True NO tiene efecto"</span>)
    <span class="kw">elif</span> <span class="st">'serial'</span> <span class="kw">in</span> output.lower():
        <span class="fn">print</span>(f<span class="st">"‚ö†Ô∏è {func.py_func.__name__}: algunos loops est√°n serializados"</span>)
    <span class="kw">else</span>:
        <span class="fn">print</span>(f<span class="st">"‚úÖ {func.py_func.__name__}: paralizaci√≥n confirmada"</span>)</code></pre>

    <!-- ============ ANTI-PATTERN 4 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #4 ‚Äî Race conditions silenciosas en arrays</h3>
    <p><strong>Severidad: CATASTR√ìFICA</strong> | <strong>Frecuencia: MEDIA</strong></p>

    <p>Numba NO detecta race conditions en arrays en la mayor√≠a de los casos. Tu c√≥digo da resultados diferentes cada vez que lo ejecutas, y puedes no notarlo si la diferencia es peque√±a. Esto en sistemas donde la precisi√≥n y exactitud son cr√≠ticas, ser√≠a un desastre.</p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: m√∫ltiples threads escriben al MISMO elemento</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">race_histogram</span>(data, n_bins):
    <span class="cm">"""Histograma con race condition."""</span>
    bins = np.zeros(n_bins, dtype=np.int64)
    <span class="kw">for</span> i <span class="kw">in</span> prange(<span class="fn">len</span>(data)):
        bin_idx = <span class="fn">int</span>(data[i] * n_bins)
        <span class="kw">if</span> bin_idx >= n_bins:
            bin_idx = n_bins - <span class="num">1</span>
        bins[bin_idx] += <span class="num">1</span>       <span class="cm"># ‚ò†Ô∏è RACE: thread A y B incrementan el mismo bin</span>
    <span class="kw">return</span> bins

data = np.random.rand(<span class="num">10_000_000</span>)
<span class="cm"># Ejecutar 10 veces ‚Äî resultados DIFERENTES cada vez:</span>
<span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(<span class="num">10</span>):
    result = race_histogram(data, <span class="num">100</span>)
    <span class="fn">print</span>(np.sum(result))  <span class="cm"># Deber√≠a ser 10_000_000 SIEMPRE</span>
<span class="cm"># Output: 9_998_374, 9_999_102, 9_997_850, ... ‚Üê ¬°DATOS CORRUPTOS!</span>

<span class="cm"># ‚ò†Ô∏è TRAMPA SUTIL: race en slice de array</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">race_slice</span>(data):
    n = data.shape[<span class="num">0</span>]
    result = np.zeros(<span class="num">4</span>)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n):
        result[:] += data[i]  <span class="cm"># ‚ò†Ô∏è RACE: todos los threads escriben a result[:]</span>
    <span class="kw">return</span> result</code></pre>

    <h4>‚úÖ Soluci√≥n: histogramas thread-local + reduce</h4>
<pre><code><span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">safe_histogram</span>(data, n_bins):
    <span class="cm">"""Histograma sin race condition: cada thread tiene su propio buffer."""</span>
    n_threads = get_num_threads()
    n = <span class="fn">len</span>(data)

    <span class="cm"># Cada thread tiene su propia copia</span>
    local_bins = np.zeros((n_threads, n_bins), dtype=np.int64)

    chunk = (n + n_threads - <span class="num">1</span>) // n_threads
    <span class="kw">for</span> t <span class="kw">in</span> prange(n_threads):
        start = t * chunk
        end = <span class="fn">min</span>(start + chunk, n)
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(start, end):
            bin_idx = <span class="fn">int</span>(data[i] * n_bins)
            <span class="kw">if</span> bin_idx >= n_bins:
                bin_idx = n_bins - <span class="num">1</span>
            local_bins[t, bin_idx] += <span class="num">1</span>  <span class="cm"># ‚úÖ cada thread su propio array</span>

    <span class="cm"># Reducir</span>
    bins = np.zeros(n_bins, dtype=np.int64)
    <span class="kw">for</span> b <span class="kw">in</span> prange(n_bins):
        <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_threads):
            bins[b] += local_bins[t, b]

    <span class="kw">return</span> bins
<span class="cm"># np.sum(safe_histogram(data, 100)) == 10_000_000 SIEMPRE</span></code></pre>

    <div class="warn">
      <strong>Nota:</strong> Las reducciones ESCALARES con <code>+=</code> sobre una variable simple S√ç son seguras en prange (Numba las detecta y maneja autom√°ticamente). Las reducciones sobre ARRAYS o slices de arrays NO son seguras ‚Äî Numba no las detecta.
    </div>

    <!-- ============ ANTI-PATTERN 5 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #5 ‚Äî Recompilaci√≥n infinita: el asesino silencioso</h3>
    <p><strong>Severidad: ALTA</strong> | <strong>Frecuencia: MEDIA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: llamar la misma funci√≥n con TIPOS DIFERENTES</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">flexible_func</span>(x):
    <span class="kw">return</span> np.sum(x ** <span class="num">2</span>)

<span class="cm"># Cada tipo nuevo = NUEVA COMPILACI√ìN (0.1-2 segundos cada una)</span>
flexible_func(np.array([<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>]))               <span class="cm"># int64 ‚Üí compila</span>
flexible_func(np.array([<span class="num">1.0</span>, <span class="num">2.0</span>]))               <span class="cm"># float64 ‚Üí compila OTRA VEZ</span>
flexible_func(np.array([<span class="num">1.0</span>, <span class="num">2.0</span>], dtype=np.float32))  <span class="cm"># float32 ‚Üí compila OTRA VEZ</span>
flexible_func(np.array([[<span class="num">1.0</span>, <span class="num">2.0</span>]]))             <span class="cm"># 2D float64 ‚Üí compila OTRA VEZ</span>

<span class="cm"># En un pipeline real donde los datos llegan como int a veces y float otras,</span>
<span class="cm"># puedes estar recompilando sin saberlo</span>

<span class="cm"># ‚ò†Ô∏è CASO PEOR: generar tipos distintos en un loop</span>
<span class="kw">for</span> dtype <span class="kw">in</span> [np.int32, np.int64, np.float32, np.float64]:
    data = np.random.rand(<span class="num">1000</span>).astype(dtype)
    flexible_func(data)  <span class="cm"># ‚ò†Ô∏è 4 compilaciones = ~4 segundos de overhead</span>

<span class="cm"># ‚úÖ SOLUCI√ìN: estandarizar tipos ANTES de llamar a Numba</span>
data_clean = data.astype(np.float64)  <span class="cm"># SIEMPRE float64</span>
flexible_func(data_clean)              <span class="cm"># Usa la versi√≥n ya compilada</span>

<span class="cm"># ‚úÖ SOLUCI√ìN alternativa: usar signature expl√≠cita</span>
<span class="dec">@njit</span>(<span class="st">'float64(float64[:])'</span>)
<span class="kw">def</span> <span class="fn">fixed_func</span>(x):
    <span class="kw">return</span> np.sum(x ** <span class="num">2</span>)
<span class="cm"># Ahora si pasas int32, Numba da ERROR en vez de recompilar silenciosamente</span>

<span class="cm"># Verificar cu√°ntas compilaciones tiene tu funci√≥n:</span>
<span class="fn">print</span>(f<span class="st">"Especializaciones: {len(flexible_func.signatures)}"</span>)
<span class="cm"># Si este n√∫mero crece con el tiempo ‚Üí tienes un problema</span></code></pre>

    <!-- ============ ANTI-PATTERN 6 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #6 ‚Äî Numba donde NumPy ya es √≥ptimo</h3>
    <p><strong>Severidad: MEDIA</strong> | <strong>Frecuencia: MUY ALTA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: reescribir algo que NumPy ya hace en C/BLAS optimizado</span>

<span class="cm"># Multiplicaci√≥n de matrices: NumPy llama a BLAS (MKL/OpenBLAS)</span>
<span class="cm"># que est√° optimizado por ingenieros de Intel con d√©cadas de trabajo</span>
A = np.random.rand(<span class="num">1000</span>, <span class="num">1000</span>)
B = np.random.rand(<span class="num">1000</span>, <span class="num">1000</span>)

<span class="cm"># ‚ùå Intentar hacer matmul en Numba</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">matmul_numba</span>(A, B):
    M, K = A.shape
    K2, N = B.shape
    C = np.zeros((M, N))
    <span class="kw">for</span> i <span class="kw">in</span> prange(M):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(N):
            <span class="kw">for</span> k <span class="kw">in</span> <span class="fn">range</span>(K):
                C[i, j] += A[i, k] * B[k, j]
    <span class="kw">return</span> C

<span class="cm"># Benchmark:</span>
<span class="cm"># np.dot(A, B):     ~15 ms (BLAS, AVX-512, cache blocking perfecto)</span>
<span class="cm"># matmul_numba:     ~450 ms (30x M√ÅS LENTO que NumPy)</span>

<span class="cm"># OTROS CASOS donde NumPy gana:</span>
<span class="cm"># np.linalg.solve() ‚Üí LAPACK optimizado</span>
<span class="cm"># np.fft.fft() ‚Üí FFTW/MKL optimizado</span>
<span class="cm"># np.linalg.svd() ‚Üí LAPACK optimizado</span>
<span class="cm"># Estas operaciones tienen D√âCADAS de optimizaci√≥n. No las reescribas.</span></code></pre>

    <h4>¬øCu√°ndo Numba S√ç gana a NumPy?</h4>
<pre><code><span class="cm"># Numba gana cuando:</span>
<span class="cm"># ‚úÖ Fusionar m√∫ltiples operaciones NumPy en un solo loop</span>
<span class="cm"># ‚úÖ L√≥gica condicional (if/else) dentro de loops</span>
<span class="cm"># ‚úÖ Acceso a elementos individuales en loops</span>
<span class="cm"># ‚úÖ Algoritmos con estado (EMA, filtro Kalman, etc.)</span>
<span class="cm"># ‚úÖ Operaciones no vectorizables nativamente</span>

<span class="cm"># NumPy gana cuando:</span>
<span class="cm"># ‚ùå √Ålgebra lineal densa (BLAS/LAPACK)</span>
<span class="cm"># ‚ùå FFT</span>
<span class="cm"># ‚ùå Una sola operaci√≥n vectorizada sobre un array grande</span>
<span class="cm"># ‚ùå Operaciones que NumPy ya ejecuta en C compilado</span></code></pre>

    <!-- ============ ANTI-PATTERN 7 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #7 ‚Äî Cache invalidado sin saberlo</h3>
    <p><strong>Severidad: MEDIA</strong> | <strong>Frecuencia: MEDIA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: cache=True no detecta cambios en funciones importadas</span>

<span class="cm"># archivo: utils.py</span>
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">helper</span>(x):
    <span class="kw">return</span> x ** <span class="num">2</span>       <span class="cm"># Versi√≥n 1</span>

<span class="cm"># archivo: main.py</span>
<span class="kw">from</span> utils <span class="kw">import</span> helper

<span class="dec">@njit</span>(cache=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">main_func</span>(data):
    result = np.empty_like(data)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        result[i] = helper(data[i])
    <span class="kw">return</span> result

<span class="cm"># Ahora cambias utils.py:</span>
<span class="cm"># def helper(x): return x ** 3   # Versi√≥n 2</span>
<span class="cm"># </span>
<span class="cm"># main_func sigue usando el CACHE VIEJO con x**2</span>
<span class="cm"># ¬°Tu c√≥digo da resultados incorrectos sin error!</span>

<span class="cm"># ‚úÖ SOLUCI√ìN: limpiar cache despu√©s de cambios</span>
<span class="cm"># $ find . -name "__pycache__" -type d -exec rm -rf {} +</span>
<span class="cm"># $ find . -name "*.nbi" -delete && find . -name "*.nbc" -delete</span>

<span class="cm"># ‚ò†Ô∏è OTRA TRAMPA: globals tratados como constantes</span>
THRESHOLD = <span class="num">0.5</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">uses_global</span>(data):
    count = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        <span class="kw">if</span> data[i] > THRESHOLD:  <span class="cm"># Numba usa el valor de compilaci√≥n</span>
            count += <span class="num">1</span>
    <span class="kw">return</span> count

uses_global(np.random.rand(<span class="num">100</span>))  <span class="cm"># compila con THRESHOLD=0.5</span>
THRESHOLD = <span class="num">0.9</span>                    <span class="cm"># ¬°Cambiar esto NO tiene efecto!</span>
uses_global(np.random.rand(<span class="num">100</span>))  <span class="cm"># sigue usando 0.5</span>

<span class="cm"># ‚úÖ SOLUCI√ìN: pasar como argumento</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">uses_argument</span>(data, threshold):  <span class="cm"># ‚úÖ argumento, no global</span>
    count = <span class="num">0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        <span class="kw">if</span> data[i] > threshold:
            count += <span class="num">1</span>
    <span class="kw">return</span> count</code></pre>

    <!-- ============ ANTI-PATTERN 8 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #8 ‚Äî Boundscheck desactivado + √≠ndice fuera de rango = corrupci√≥n silenciosa</h3>
    <p><strong>Severidad: CATASTR√ìFICA</strong> | <strong>Frecuencia: MEDIA</strong></p>

<pre><code><span class="cm"># En Numba, boundscheck est√° DESACTIVADO por defecto (por rendimiento)</span>
<span class="cm"># Esto significa que acceder fuera de rango NO da error</span>
<span class="cm"># ‚Äî lee o escribe basura en memoria adyacente</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">silent_corruption</span>(data, indices):
    result = np.zeros(<span class="num">10</span>)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(indices)):
        idx = indices[i]
        result[idx] += data[i]  <span class="cm"># Si idx >= 10 ‚Üí escribe en memoria AJENA</span>
    <span class="kw">return</span> result                <span class="cm"># result puede tener valores corruptos</span>

<span class="cm"># Con datos reales esto puede:</span>
<span class="cm"># - Corromper otros arrays en memoria</span>
<span class="cm"># - Causar segfault HORAS despu√©s (cuando accedes al array corrupto)</span>
<span class="cm"># - Dar resultados sutilmente incorrectos que pasan validaci√≥n</span>

<span class="cm"># ‚úÖ DURANTE DESARROLLO: activar boundscheck</span>
<span class="dec">@njit</span>(boundscheck=<span class="num">True</span>)  <span class="cm"># o NUMBA_BOUNDSCHECK=1 como env var</span>
<span class="kw">def</span> <span class="fn">safe_during_dev</span>(data, indices):
    result = np.zeros(<span class="num">10</span>)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(indices)):
        idx = indices[i]
        result[idx] += data[i]  <span class="cm"># Ahora lanza IndexError si idx >= 10</span>
    <span class="kw">return</span> result

<span class="cm"># ‚úÖ EN PRODUCCI√ìN: validar indices manualmente (sin boundscheck overhead)</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">safe_in_production</span>(data, indices, n_bins):
    result = np.zeros(n_bins)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(indices)):
        idx = indices[i]
        <span class="kw">if</span> <span class="num">0</span> <= idx < n_bins:     <span class="cm"># ‚úÖ Guard manual, m√≠nimo overhead</span>
            result[idx] += data[i]
    <span class="kw">return</span> result</code></pre>

    <!-- ============ ANTI-PATTERN 9 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #9 ‚Äî Oversubscription: m√°s threads ‚â† m√°s r√°pido</h3>
    <p><strong>Severidad: ALTA</strong> | <strong>Frecuencia: ALTA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: Numba parallel + NumPy con MKL + scikit-learn paralelo</span>
<span class="cm"># Cada uno lanza SUS PROPIOS threads sin coordinarse</span>

<span class="cm"># Escenario real:</span>
<span class="cm"># - Tu CPU tiene 8 cores f√≠sicos (16 con HyperThreading)</span>
<span class="cm"># - Numba lanza 16 threads (ve 16 "cores" l√≥gicos)</span>
<span class="cm"># - NumPy con MKL lanza 16 threads para np.dot()</span>
<span class="cm"># - scikit-learn lanza 16 threads para cross_val_score()</span>
<span class="cm"># Total: 48 threads compitiendo por 8 cores f√≠sicos</span>
<span class="cm"># ‚Üí Context switches masivos, cache invalidation, 3x M√ÅS LENTO</span>

<span class="cm"># ‚ò†Ô∏è Ejemplo concreto:</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">numba_step</span>(data):
    <span class="kw">return</span> np.sum(data ** <span class="num">2</span>)  <span class="cm"># Numba usa 16 threads</span>

<span class="kw">def</span> <span class="fn">pipeline</span>(data):
    step1 = numba_step(data)          <span class="cm"># 16 threads Numba</span>
    step2 = np.dot(data.T, data)      <span class="cm"># 16 threads MKL (SIMULT√ÅNEOS si async)</span>
    <span class="kw">return</span> step1, step2

<span class="cm"># ‚úÖ SOLUCI√ìN: coordinar threads entre librer√≠as</span>
<span class="kw">import</span> os
os.environ[<span class="st">'MKL_NUM_THREADS'</span>] = <span class="st">'4'</span>       <span class="cm"># ANTES de importar numpy</span>
os.environ[<span class="st">'OMP_NUM_THREADS'</span>] = <span class="st">'4'</span>       <span class="cm"># ANTES de importar numpy</span>
os.environ[<span class="st">'NUMBA_NUM_THREADS'</span>] = <span class="st">'4'</span>     <span class="cm"># ANTES de importar numba</span>

<span class="cm"># O din√°micamente:</span>
<span class="kw">from</span> numba <span class="kw">import</span> set_num_threads
set_num_threads(<span class="num">4</span>)   <span class="cm"># Cuando Numba est√° activo</span>
set_num_threads(<span class="num">1</span>)   <span class="cm"># Cuando dejas que MKL use todos los cores</span></code></pre>

    <!-- ============ ANTI-PATTERN 10 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #10 ‚Äî Usar typed.Dict o typed.List como sustituto de arrays</h3>
    <p><strong>Severidad: MEDIA</strong> | <strong>Frecuencia: MEDIA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: usar Dict para algo que deber√≠a ser un array</span>
<span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.typed <span class="kw">import</span> Dict

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">count_with_dict</span>(data, n_bins):
    <span class="cm">"""Contar ocurrencias usando un dict ‚Äî LENTO."""</span>
    counts = Dict.empty(key_type=types.int64, value_type=types.int64)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        key = <span class="fn">int</span>(data[i] * n_bins)
        <span class="kw">if</span> key <span class="kw">in</span> counts:
            counts[key] += <span class="num">1</span>       <span class="cm"># ‚ò†Ô∏è Hash lookup + hash insert cada vez</span>
        <span class="kw">else</span>:
            counts[key] = <span class="num">1</span>
    <span class="kw">return</span> counts

<span class="cm"># ‚úÖ CORRECTO: usar array directo (si las keys son √≠ndices enteros)</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">count_with_array</span>(data, n_bins):
    counts = np.zeros(n_bins, dtype=np.int64)
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        idx = <span class="fn">int</span>(data[i] * n_bins)
        <span class="kw">if</span> idx >= n_bins:
            idx = n_bins - <span class="num">1</span>
        counts[idx] += <span class="num">1</span>          <span class="cm"># ‚úÖ Acceso directo O(1), sin hash</span>
    <span class="kw">return</span> counts

<span class="cm"># Benchmark con 10M elementos:</span>
<span class="cm"># Dict version:  ~2.1 segundos</span>
<span class="cm"># Array version: ~0.05 segundos ‚Üí 42x m√°s r√°pido</span>

<span class="cm"># REGLA: Si tus keys son enteros consecutivos 0..N ‚Üí usa un array</span>
<span class="cm"># Usa Dict SOLO cuando las keys son no-consecutivas o strings</span></code></pre>

    <!-- ============ ANTI-PATTERN 11 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #11 ‚Äî parallel=True con datos peque√±os</h3>
    <p><strong>Severidad: MEDIA</strong> | <strong>Frecuencia: ALTA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPA: paralelizar operaciones sobre arrays peque√±os</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">overkill</span>(small_array):
    <span class="kw">return</span> np.sum(np.sqrt(small_array))

data = np.random.rand(<span class="num">100</span>)  <span class="cm"># 100 elementos = 800 bytes</span>

<span class="cm"># Benchmark:</span>
<span class="cm"># @njit (sin parallel):   0.3 Œºs</span>
<span class="cm"># @njit(parallel=True):   45 Œºs ‚Üí 150x M√ÅS LENTO</span>
<span class="cm"># ¬°El overhead de coordinar 8 threads es mayor que todo el c√≥mputo!</span>

<span class="cm"># ‚úÖ REGLA PR√ÅCTICA: parallel=True solo rinde con:</span>
<span class="cm"># - Arrays de > 10,000 elementos para operaciones simples</span>
<span class="cm"># - Arrays de > 1,000 elementos para operaciones costosas (sin, cos, etc.)</span>
<span class="cm"># - Siempre verificar con un scaling test (m√≥dulo de benchmarking)</span>

<span class="cm"># ‚úÖ Si no sabes el tama√±o de antemano, decide en runtime:</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">smart_serial</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">smart_parallel</span>(x):
    <span class="kw">return</span> np.sum(np.sqrt(x))

<span class="kw">def</span> <span class="fn">smart_dispatch</span>(x, threshold=<span class="num">10_000</span>):
    <span class="kw">if</span> <span class="fn">len</span>(x) < threshold:
        <span class="kw">return</span> smart_serial(x)
    <span class="kw">return</span> smart_parallel(x)</code></pre>

    <!-- ============ ANTI-PATTERN 12 ============ -->
    <h3 style="color: #f85149;">‚ò†Ô∏è #12 ‚Äî Acceso no contiguo disfrazado</h3>
    <p><strong>Severidad: ALTA</strong> | <strong>Frecuencia: MEDIA</strong></p>

<pre><code><span class="cm"># ‚ò†Ô∏è TRAMPAS que destruyen la contig√ºidad sin que lo notes:</span>

<span class="cm"># 1. Slices con step</span>
data = np.random.rand(<span class="num">1000000</span>)
every_other = data[::2]       <span class="cm"># stride = 16 bytes, NO contiguo</span>

<span class="cm"># 2. Transponer sin copiar</span>
matrix = np.random.rand(<span class="num">1000</span>, <span class="num">1000</span>)
transposed = matrix.T          <span class="cm"># NO es C-contiguous, es F-contiguous</span>
<span class="cm"># Si tu loop interno recorre filas del transpuesto ‚Üí cache disaster</span>

<span class="cm"># 3. Fancy indexing devuelve copia contigua... pero el loop puede no serlo</span>
indices = np.array([<span class="num">0</span>, <span class="num">500</span>, <span class="num">1</span>, <span class="num">501</span>, <span class="num">2</span>, <span class="num">502</span>])
selected = matrix[indices]     <span class="cm"># selected ES contiguo, pero los accesos no</span>

<span class="cm"># 4. Reshape que cambia strides</span>
flat = np.random.rand(<span class="num">1000000</span>)
matrix_view = flat.reshape(<span class="num">1000</span>, <span class="num">1000</span>)  <span class="cm"># OK, C-contiguous</span>
col_major = matrix_view.T                    <span class="cm"># ‚ò†Ô∏è F-contiguous</span>

<span class="cm"># ‚úÖ SIEMPRE verificar antes de procesar:</span>
<span class="fn">print</span>(f<span class="st">"C-contiguo: {matrix_view.flags.c_contiguous}"</span>)
<span class="fn">print</span>(f<span class="st">"F-contiguo: {matrix_view.flags.f_contiguous}"</span>)
<span class="fn">print</span>(f<span class="st">"Strides: {matrix_view.strides}"</span>)

<span class="cm"># ‚úÖ Forzar contig√ºidad cuando sea necesario:</span>
safe_data = np.ascontiguousarray(col_major)  <span class="cm"># copia a C-contiguous</span></code></pre>

<span class="cm"># ‚úÖ Identifica que slices y transposes destruyen performance</span>
    <!-- ============ RESUMEN ============ -->
    <h3>üìä Clasificaci√≥n de severidad</h3>

    <table>
      <tr><th>Anti-Pattern</th><th>S√≠ntoma</th><th>Costo a escala</th><th>Detecci√≥n</th></tr>
      <tr><td>#1 parallel en loop externo</td><td>Lentitud inexplicable</td><td>100-1000x m√°s lento</td><td>Profiler muestra overhead en dispatch</td></tr>
      <tr><td>#2 Alocaciones en loop</td><td>Uso de memoria creciente</td><td>2-10x m√°s lento</td><td>NUMBA_DEBUG_NRT=1</td></tr>
      <tr><td>#3 parallel=True inactivo</td><td>Usa 1 core en vez de 8</td><td>8x m√°s lento (en 8 cores)</td><td>parallel_diagnostics()</td></tr>
      <tr><td>#4 Race conditions</td><td>Resultados variables</td><td>Datos corruptos</td><td>Ejecutar 10x y comparar sumas</td></tr>
      <tr><td>#5 Recompilaci√≥n</td><td>Pausas peri√≥dicas</td><td>Segundos por tipo nuevo</td><td>len(func.signatures)</td></tr>
      <tr><td>#6 Numba donde NumPy gana</td><td>M√°s lento que sin Numba</td><td>Hasta 30x m√°s lento</td><td>Benchmark vs NumPy puro</td></tr>
      <tr><td>#7 Cache invalidado</td><td>Resultados incorrectos</td><td>Bugs silenciosos</td><td>Limpiar cache, re-test</td></tr>
      <tr><td>#8 Sin boundscheck</td><td>Segfaults aleatorios</td><td>Corrupci√≥n de memoria</td><td>NUMBA_BOUNDSCHECK=1</td></tr>
      <tr><td>#9 Oversubscription</td><td>M√°s threads = m√°s lento</td><td>2-5x m√°s lento</td><td>htop, top</td></tr>
      <tr><td>#10 Dict como array</td><td>Lentitud en lookups</td><td>10-50x m√°s lento</td><td>Benchmark vs array</td></tr>
      <tr><td>#11 parallel con datos chicos</td><td>Overhead domina</td><td>10-150x m√°s lento</td><td>Scaling test</td></tr>
      <tr><td>#12 Acceso no contiguo</td><td>Throughput (Rendimiento real) bajo</td><td>3-10x m√°s lento</td><td>arr.flags.c_contiguous</td></tr>
    </table>

    <div class="tip">
      <strong>Workflow defensivo para proyectos a escala:</strong><br>
      1. Despu√©s de cada <code>@njit(parallel=True)</code> ‚Üí <code>parallel_diagnostics(level=1)</code><br>
      2. Durante desarrollo ‚Üí <code>NUMBA_BOUNDSCHECK=1</code><br>
      3. Antes de producci√≥n ‚Üí scaling test (1K ‚Üí 10M)<br>
      4. Monitorear ‚Üí <code>len(func.signatures)</code> no debe crecer con el tiempo<br>
      5. Benchmark ‚Üí siempre comparar contra NumPy puro como baseline<br>
      6. Race conditions ‚Üí ejecutar 10x, verificar que sumas son id√©nticas
    </div>

  </div>
</div>
  
<!--
  ============================================================
  M√ìDULO EXTRA: Gu√≠a Exhaustiva de Decoradores
  ============================================================
<!-- ==================== M√ìDULO 10 ==================== -->
<a href="#m-decorators">‚òÖ Gu√≠a Exhaustiva: ¬øQu√© decorador uso?</a>

<!-- ==================== M√ìDULO DECORADORES ==================== -->
<div class="module" id="m-decorators">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">‚òÖ</span> Gu√≠a Exhaustiva: ¬øQu√© decorador uso y cu√°ndo?</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Esta secci√≥n resuelve la pregunta m√°s importante al usar Numba: <strong>¬øqu√© decorador necesito para MI caso?</strong> Cada decorador existe para resolver un problema distinto. Usar el incorrecto no solo no ayuda ‚Äî puede hacer tu c√≥digo m√°s lento, incorrecto, o directamente roto.</p>

    <!-- ============ MAPA DE DECISI√ìN ============ -->
    <h3>üó∫Ô∏è Mapa de decisi√≥n r√°pido</h3>
    <p>Antes de leer todo, usa esta gu√≠a r√°pida:</p>
    <table>
      <tr><th>Tu situaci√≥n</th><th>Decorador correcto</th></tr>
      <tr><td>Tengo una funci√≥n con loops num√©ricos y quiero que vaya r√°pido</td><td><code>@njit</code></td></tr>
      <tr><td>Tengo una operaci√≥n escalar que quiero aplicar a todo un array</td><td><code>@vectorize</code></td></tr>
      <tr><td>Tengo una operaci√≥n que recibe/devuelve sub-arrays (no escalares)</td><td><code>@guvectorize</code></td></tr>
      <tr><td>Tengo un c√°lculo donde cada elemento depende de sus vecinos</td><td><code>@stencil</code></td></tr>
      <tr><td>Necesito una clase con estado y m√©todos r√°pidos</td><td><code>@jitclass</code></td></tr>
      <tr><td>Necesito que una funci√≥n Python sea llamable desde C/Fortran</td><td><code>@cfunc</code></td></tr>
      <tr><td>Quiero que una funci√≥n de terceros funcione dentro de @njit</td><td><code>@overload</code></td></tr>
    </table>

    <!-- ============================================ -->
    <!-- ============ @njit / @jit ================= -->
    <!-- ============================================ -->
    <h3 style="color: #58a6ff; border-color: #58a6ff;">1. <code>@njit</code> / <code>@jit</code> ‚Äî La navaja suiza</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Toma tu funci√≥n Python, analiza el bytecode, infiere los tipos de todas las variables, y genera c√≥digo m√°quina nativo v√≠a LLVM. Es como si un compilador de C tomara tu Python y lo convirtiera en un ejecutable. Desde Numba 0.59, <code>@jit</code> y <code>@njit</code> son id√©nticos.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Funciones con loops num√©ricos sobre arrays NumPy</li>
      <li>C√°lculos matem√°ticos puros (sin I/O, sin strings complejos)</li>
      <li>Funciones auxiliares peque√±as que ser√°n llamadas desde otro c√≥digo Numba</li>
      <li>Cualquier funci√≥n donde "escribir√≠as un loop en C"</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Funciones que hacen I/O (leer archivos, requests HTTP, print extensivo)</li>
      <li>C√≥digo que manipula strings complejos, listas Python, o DataFrames de Pandas</li>
      <li>Funciones que ya son 100% operaciones vectorizadas de NumPy (Numba no las acelera mucho m√°s)</li>
      <li>Scripts cortos que se ejecutan una sola vez (la compilaci√≥n tarda m√°s que la ejecuci√≥n)</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Filtro de part√≠culas en una simulaci√≥n f√≠sica</strong></p>
<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">actualizar_particulas</span>(posiciones, velocidades, masas, dt, gravedad):
    <span class="cm">"""Simulaci√≥n N-body simplificada: actualiza posiciones por un paso dt."""</span>
    n = posiciones.shape[<span class="num">0</span>]
    fuerzas = np.zeros_like(posiciones)

    <span class="cm"># Calcular fuerzas gravitacionales entre pares</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(i + <span class="num">1</span>, n):
            dx = posiciones[j, <span class="num">0</span>] - posiciones[i, <span class="num">0</span>]
            dy = posiciones[j, <span class="num">1</span>] - posiciones[i, <span class="num">1</span>]
            dist_sq = dx * dx + dy * dy + <span class="num">1e-10</span>  <span class="cm"># evitar div/0</span>
            dist = np.sqrt(dist_sq)
            fuerza = gravedad * masas[i] * masas[j] / dist_sq
            fx = fuerza * dx / dist
            fy = fuerza * dy / dist
            fuerzas[i, <span class="num">0</span>] += fx
            fuerzas[i, <span class="num">1</span>] += fy
            fuerzas[j, <span class="num">0</span>] -= fx
            fuerzas[j, <span class="num">1</span>] -= fy

    <span class="cm"># Integrar: actualizar velocidades y posiciones</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        velocidades[i, <span class="num">0</span>] += (fuerzas[i, <span class="num">0</span>] / masas[i]) * dt
        velocidades[i, <span class="num">1</span>] += (fuerzas[i, <span class="num">1</span>] / masas[i]) * dt
        posiciones[i, <span class="num">0</span>] += velocidades[i, <span class="num">0</span>] * dt
        posiciones[i, <span class="num">1</span>] += velocidades[i, <span class="num">1</span>] * dt

<span class="cm"># Uso</span>
n_particulas = <span class="num">1000</span>
pos = np.random.rand(n_particulas, <span class="num">2</span>) * <span class="num">100</span>
vel = np.zeros((n_particulas, <span class="num">2</span>))
masas = np.random.rand(n_particulas) * <span class="num">10</span> + <span class="num">1</span>
actualizar_particulas(pos, vel, masas, <span class="num">0.01</span>, <span class="num">9.8</span>)
<span class="cm"># Sin Numba: ~15 segundos | Con @njit: ~0.02 segundos</span></code></pre>

    <p><strong>Ejemplo 2: B√∫squeda en historial de precios (datos financieros)</strong></p>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">max_drawdown</span>(precios):
    <span class="cm">"""Calcula la m√°xima ca√≠da desde un pico en una serie de precios.
    M√©trica fundamental en finanzas para medir riesgo."""</span>
    n = <span class="fn">len</span>(precios)
    pico = precios[<span class="num">0</span>]
    max_dd = <span class="num">0.0</span>

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="num">1</span>, n):
        <span class="kw">if</span> precios[i] > pico:
            pico = precios[i]
        dd = (pico - precios[i]) / pico
        <span class="kw">if</span> dd > max_dd:
            max_dd = dd

    <span class="kw">return</span> max_dd

<span class="cm"># 10 a√±os de datos diarios</span>
precios = np.cumsum(np.random.randn(<span class="num">2520</span>)) + <span class="num">100</span>
resultado = max_drawdown(precios)  <span class="cm"># instant√°neo</span></code></pre>

    <p><strong>Ejemplo 3: Procesamiento de se√±al (correlaci√≥n cruzada manual)</strong></p>
<pre><code><span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">cross_correlation</span>(signal, template):
    <span class="cm">"""Encuentra d√≥nde un template aparece en una se√±al larga."""</span>
    n_signal = <span class="fn">len</span>(signal)
    n_template = <span class="fn">len</span>(template)
    n_output = n_signal - n_template + <span class="num">1</span>
    result = np.empty(n_output)

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_output):
        total = <span class="num">0.0</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(n_template):
            total += signal[i + j] * template[j]
        result[i] = total

    <span class="kw">return</span> result</code></pre>

    <h4>üö´ Errores comunes con @njit</h4>
<pre><code><span class="cm"># ERROR 1: Usar listas Python normales</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">malo_lista</span>(n):
    resultado = []                  <span class="cm"># ‚ùå Lista Python, NO funciona</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        resultado.append(i * <span class="num">2</span>)
    <span class="kw">return</span> resultado

<span class="cm"># SOLUCI√ìN: Usa array NumPy o numba.typed.List</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">bueno_array</span>(n):
    resultado = np.empty(n, dtype=np.int64)  <span class="cm"># ‚úÖ Pre-aloca array</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n):
        resultado[i] = i * <span class="num">2</span>
    <span class="kw">return</span> resultado

<span class="cm"># ERROR 2: Llamar funciones no soportadas</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">malo_pandas</span>(df):
    <span class="kw">return</span> df.groupby(<span class="st">'col'</span>).mean()  <span class="cm"># ‚ùå Pandas NO funciona en Numba</span>

<span class="cm"># ERROR 3: Usar @njit en funciones que YA son r√°pidas con NumPy</span>
<span class="dec">@njit</span>   <span class="cm"># ‚ùå Innecesario: NumPy ya hace esto en C internamente</span>
<span class="kw">def</span> <span class="fn">innecesario</span>(a, b):
    <span class="kw">return</span> np.dot(a, b)  <span class="cm"># np.dot ya llama a BLAS optimizado</span>

<span class="cm"># ERROR 4: Olvidar decorar funciones auxiliares</span>
<span class="kw">def</span> <span class="fn">helper</span>(x):        <span class="cm"># ‚ùå Sin @njit, Numba sale al int√©rprete</span>
    <span class="kw">return</span> x ** <span class="num">2</span>

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">principal</span>(arr):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(arr)):
        arr[i] = helper(arr[i])  <span class="cm"># Lent√≠simo: sale de nativo a Python en cada iteraci√≥n</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @vectorize =================== -->
    <!-- ============================================ -->
    <h3 style="color: #3fb950; border-color: #3fb950;">2. <code>@vectorize</code> ‚Äî F√°brica de ufuncs escalares</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>T√∫ escribes una funci√≥n que opera sobre <strong>un solo par de valores escalares</strong>. Numba genera autom√°ticamente el loop que aplica esa operaci√≥n a arrays completos, con soporte autom√°tico de broadcasting, reduce, accumulate ‚Äî como un ufunc nativo de NumPy escrito en C, pero sin escribir C.</p>

    <h4>‚úÖ Cu√°ndo usarlo (en vez de @njit)</h4>
    <ul>
      <li>Tu operaci√≥n es naturalmente <strong>escalar ‚Üí escalar</strong> (recibe n√∫meros, devuelve un n√∫mero)</li>
      <li>Necesitas <strong>broadcasting</strong> autom√°tico entre arrays de distintas formas</li>
      <li>Necesitas <strong>reduce</strong> o <strong>accumulate</strong> sobre tu operaci√≥n</li>
      <li>Quieres paralelizar f√°cilmente con <code>target='parallel'</code></li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Tu funci√≥n necesita ver m√°s de un elemento a la vez (usa @guvectorize)</li>
      <li>Tu funci√≥n tiene estado interno o acumuladores (usa @njit con loop)</li>
      <li>Solo necesitas aplicar una operaci√≥n a un array sin broadcasting/reduce (un @njit simple es m√°s directo)</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: C√°lculo de impuestos con tramos progresivos</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> vectorize, float64

<span class="dec">@vectorize</span>([float64(float64)])
<span class="kw">def</span> <span class="fn">calcular_impuesto</span>(ingreso):
    <span class="cm">"""Impuesto progresivo: aplica a CADA empleado de un array."""</span>
    <span class="kw">if</span> ingreso <= <span class="num">12000</span>:
        <span class="kw">return</span> ingreso * <span class="num">0.0</span>
    <span class="kw">elif</span> ingreso <= <span class="num">30000</span>:
        <span class="kw">return</span> (ingreso - <span class="num">12000</span>) * <span class="num">0.15</span>
    <span class="kw">elif</span> ingreso <= <span class="num">60000</span>:
        <span class="kw">return</span> <span class="num">2700</span> + (ingreso - <span class="num">30000</span>) * <span class="num">0.25</span>
    <span class="kw">else</span>:
        <span class="kw">return</span> <span class="num">10200</span> + (ingreso - <span class="num">60000</span>) * <span class="num">0.35</span>

<span class="cm"># Funciona sobre arrays completos autom√°ticamente</span>
salarios = np.array([<span class="num">8000</span>, <span class="num">25000</span>, <span class="num">45000</span>, <span class="num">120000</span>])
impuestos = calcular_impuesto(salarios)
<span class="cm"># ‚Üí array([0., 1950., 6450., 31200.])</span>

<span class="cm"># ¬°Broadcasting gratis! Funciona con matrices tambi√©n</span>
salarios_departamentos = np.random.rand(<span class="num">50</span>, <span class="num">200</span>) * <span class="num">80000</span>
impuestos_todos = calcular_impuesto(salarios_departamentos)  <span class="cm"># shape (50, 200)</span></code></pre>

    <p><strong>Ejemplo 2: Funci√≥n de activaci√≥n personalizada para ML</strong></p>
<pre><code><span class="kw">import</span> math

<span class="dec">@vectorize</span>([float64(float64)], target=<span class="st">'parallel'</span>)
<span class="kw">def</span> <span class="fn">swish</span>(x):
    <span class="cm">"""Swish activation: x * sigmoid(x). Mejor que ReLU en muchos casos."""</span>
    <span class="kw">return</span> x / (<span class="num">1.0</span> + math.exp(-x))

<span class="cm"># Aplicar a toda una capa de red neuronal (millones de valores)</span>
activaciones = np.random.randn(<span class="num">512</span>, <span class="num">1024</span>)
resultado = swish(activaciones)  <span class="cm"># Paralelo en todos los cores</span></code></pre>

    <p><strong>Ejemplo 3: Distancia con reduce (¬°imposible con @njit!)</strong></p>
<pre><code><span class="dec">@vectorize</span>([float64(float64, float64)])
<span class="kw">def</span> <span class="fn">max_custom</span>(a, b):
    <span class="cm">"""Un max que funciona como ufunc con reduce."""</span>
    <span class="kw">if</span> a >= b:
        <span class="kw">return</span> a
    <span class="kw">return</span> b

data = np.array([<span class="num">3.0</span>, <span class="num">7.0</span>, <span class="num">2.0</span>, <span class="num">9.0</span>, <span class="num">1.0</span>])
max_custom.reduce(data)  <span class="cm"># ‚Üí 9.0 (reduce funciona autom√°ticamente)</span>

<span class="cm"># Con una matriz: m√°ximo por filas o columnas</span>
matrix = np.random.rand(<span class="num">100</span>, <span class="num">50</span>)
max_custom.reduce(matrix, axis=<span class="num">1</span>)     <span class="cm"># m√°ximo de cada fila</span>
max_custom.accumulate(matrix, axis=<span class="num">0</span>) <span class="cm"># m√°ximo acumulado por columnas</span></code></pre>

    <h4>üö´ Errores comunes con @vectorize</h4>
<pre><code><span class="cm"># ERROR 1: Intentar acceder a √≠ndices del array</span>
<span class="dec">@vectorize</span>([float64(float64[:])])  <span class="cm"># ‚ùå No puedes pasar arrays</span>
<span class="kw">def</span> <span class="fn">malo</span>(arr):
    <span class="kw">return</span> arr[<span class="num">0</span>] + arr[<span class="num">1</span>]
<span class="cm"># vectorize solo recibe ESCALARES. Para arrays usa @guvectorize</span>

<span class="cm"># ERROR 2: Orden incorrecto de signatures (tipos m√°s espec√≠ficos primero)</span>
<span class="dec">@vectorize</span>([
    float64(float64, float64),  <span class="cm"># ‚ùå float64 antes que int ‚Üí int nunca se usa</span>
    int64(int64, int64),
])
<span class="kw">def</span> <span class="fn">malo_orden</span>(a, b):
    <span class="kw">return</span> a + b

<span class="cm"># CORRECTO: tipos m√°s espec√≠ficos/peque√±os primero</span>
<span class="dec">@vectorize</span>([
    int32(int32, int32),     <span class="cm"># ‚úÖ M√°s espec√≠fico primero</span>
    int64(int64, int64),
    float32(float32, float32),
    float64(float64, float64),
])
<span class="kw">def</span> <span class="fn">bien_orden</span>(a, b):
    <span class="kw">return</span> a + b

<span class="cm"># ERROR 3: Usar target='parallel' con datos muy peque√±os</span>
<span class="dec">@vectorize</span>([float64(float64)], target=<span class="st">'parallel'</span>)
<span class="kw">def</span> <span class="fn">doble</span>(x):
    <span class="kw">return</span> x * <span class="num">2</span>

doble(np.array([<span class="num">1.0</span>, <span class="num">2.0</span>, <span class="num">3.0</span>]))  <span class="cm"># ‚ùå 3 elementos: el overhead de threading</span>
                                      <span class="cm">#    es mayor que el c√°lculo en s√≠</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @guvectorize ================= -->
    <!-- ============================================ -->
    <h3 style="color: #d2a8ff; border-color: #d2a8ff;">3. <code>@guvectorize</code> ‚Äî Ufuncs sobre sub-arrays</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Como <code>@vectorize</code>, pero tu funci√≥n recibe <strong>porciones de arrays</strong> (filas, vectores, submatrices) en lugar de escalares. T√∫ defines un "layout" simb√≥lico que dice qu√© dimensiones tiene cada argumento. NumPy se encarga del broadcasting y de iterar sobre las dimensiones externas.</p>

    <h4>‚úÖ Cu√°ndo usarlo (en vez de @njit o @vectorize)</h4>
    <ul>
      <li>Tu operaci√≥n necesita ver <strong>una fila/vector/subarray completo</strong> para producir un resultado</li>
      <li>Quieres aplicar la misma operaci√≥n a cada fila de una matriz, cada frame de un video, etc.</li>
      <li>Necesitas broadcasting autom√°tico entre inputs de diferentes dimensiones</li>
      <li>Est√°s implementando algo tipo: media por fila, normalizaci√≥n por vector, convoluci√≥n 1D por canal</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Tu operaci√≥n es escalar ‚Üí escalar (usa @vectorize, es m√°s simple)</li>
      <li>Tienes un solo array y un loop secuencial con dependencias (usa @njit)</li>
      <li>No necesitas broadcasting ‚Äî un @njit simple es m√°s legible</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Normalizaci√≥n por filas (muy com√∫n en ML)</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> guvectorize, float64

<span class="dec">@guvectorize</span>(
    [(float64[:], float64[:])],
    <span class="st">'(n)->(n)'</span>  <span class="cm"># recibe vector de n elementos, devuelve vector de n</span>
)
<span class="kw">def</span> <span class="fn">normalize_row</span>(row, result):
    <span class="cm">"""Normaliza un vector a norma unitaria (L2)."""</span>
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(row.shape[<span class="num">0</span>]):
        total += row[i] ** <span class="num">2</span>
    norm = np.sqrt(total)
    <span class="kw">if</span> norm > <span class="num">0</span>:
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(row.shape[<span class="num">0</span>]):
            result[i] = row[i] / norm
    <span class="kw">else</span>:
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(row.shape[<span class="num">0</span>]):
            result[i] = <span class="num">0.0</span>

<span class="cm"># Aplica autom√°ticamente a CADA FILA de una matriz</span>
embeddings = np.random.rand(<span class="num">10000</span>, <span class="num">768</span>)  <span class="cm"># 10K vectores de 768 dimensiones</span>
normalized = normalize_row(embeddings)    <span class="cm"># shape (10000, 768), cada fila normalizada</span></code></pre>

    <p><strong>Ejemplo 2: Media m√≥vil por serie temporal (datos financieros)</strong></p>
<pre><code><span class="dec">@guvectorize</span>(
    [(float64[:], float64[:], float64[:])],
    <span class="st">'(n),(m)->(n)'</span>  <span class="cm"># se√±al de n puntos, ventana de m pesos ‚Üí n resultados</span>
)
<span class="kw">def</span> <span class="fn">weighted_moving_avg</span>(signal, weights, result):
    <span class="cm">"""Media m√≥vil ponderada. weights define la ventana."""</span>
    m = weights.shape[<span class="num">0</span>]
    w_sum = <span class="num">0.0</span>
    <span class="kw">for</span> k <span class="kw">in</span> <span class="fn">range</span>(m):
        w_sum += weights[k]

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(signal.shape[<span class="num">0</span>]):
        <span class="kw">if</span> i < m - <span class="num">1</span>:
            result[i] = np.nan  <span class="cm"># no hay suficientes datos</span>
        <span class="kw">else</span>:
            total = <span class="num">0.0</span>
            <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(m):
                total += signal[i - m + <span class="num">1</span> + j] * weights[j]
            result[i] = total / w_sum

<span class="cm"># 500 acciones, 252 d√≠as de cotizaci√≥n cada una</span>
cotizaciones = np.random.rand(<span class="num">500</span>, <span class="num">252</span>) * <span class="num">100</span> + <span class="num">50</span>
pesos = np.array([<span class="num">1.0</span>, <span class="num">2.0</span>, <span class="num">3.0</span>, <span class="num">4.0</span>, <span class="num">5.0</span>])  <span class="cm"># ventana de 5 d√≠as, peso creciente</span>
medias = weighted_moving_avg(cotizaciones, pesos)  <span class="cm"># shape (500, 252)</span>
<span class="cm"># ¬°Broadcasting autom√°tico! La misma ventana se aplica a las 500 series</span></code></pre>

    <p><strong>Ejemplo 3: Distancia euclidiana entre dos conjuntos de vectores</strong></p>
<pre><code><span class="dec">@guvectorize</span>(
    [(float64[:], float64[:], float64[:])],
    <span class="st">'(d),(d)->()'</span>  <span class="cm"># dos vectores de d dimensiones ‚Üí un escalar</span>
)
<span class="kw">def</span> <span class="fn">euclidean_dist</span>(a, b, result):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(a.shape[<span class="num">0</span>]):
        diff = a[i] - b[i]
        total += diff * diff
    result[<span class="num">0</span>] = np.sqrt(total)

<span class="cm"># Distancia entre 1000 pares de vectores 3D</span>
puntos_a = np.random.rand(<span class="num">1000</span>, <span class="num">3</span>)
puntos_b = np.random.rand(<span class="num">1000</span>, <span class="num">3</span>)
distancias = euclidean_dist(puntos_a, puntos_b)  <span class="cm"># shape (1000,)</span>

<span class="cm"># Broadcasting: distancia de UN punto a TODOS los dem√°s</span>
query = np.array([<span class="num">0.5</span>, <span class="num">0.5</span>, <span class="num">0.5</span>])
todas_dist = euclidean_dist(puntos_a, query)  <span class="cm"># shape (1000,) ‚Äî autom√°tico</span></code></pre>

    <h4>üö´ Errores comunes con @guvectorize</h4>
<pre><code><span class="cm"># ERROR 1: Intentar retornar un valor (como en @vectorize)</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'(n)->()'</span>)
<span class="kw">def</span> <span class="fn">malo</span>(arr, result):
    <span class="kw">return</span> np.sum(arr)  <span class="cm"># ‚ùå El return se IGNORA</span>
    <span class="cm"># ‚úÖ Debes escribir en result[0] = np.sum(arr)</span>

<span class="cm"># ERROR 2: Layout incorrecto ‚Äî olvidar los par√©ntesis del escalar</span>
<span class="cm"># Escalar en el layout se escribe como () no como (1)</span>
<span class="st">'(n)->(1)'</span>   <span class="cm"># ‚ùå Esto crea un array de 1 elemento</span>
<span class="st">'(n)->()'</span>    <span class="cm"># ‚úÖ Esto crea un escalar</span>

<span class="cm"># ERROR 3: Modificar un array de entrada pensando que se guarda</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'()->()'</span>)
<span class="kw">def</span> <span class="fn">malo_overwrite</span>(inval, outval):
    inval[<span class="num">0</span>] = <span class="num">99.0</span>   <span class="cm"># ‚ùå Puede no guardarse si NumPy hizo un cast</span>
    outval[<span class="num">0</span>] = <span class="num">42.0</span>
<span class="cm"># SOLUCI√ìN: usa writable_args=('inval',) si necesitas modificar inputs</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @stencil ===================== -->
    <!-- ============================================ -->
    <h3 style="color: #f0883e; border-color: #f0883e;">4. <code>@stencil</code> ‚Äî Patrones de vecindario</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Defines c√≥mo calcular UN elemento del resultado en funci√≥n de sus <strong>vecinos</strong> en el array de entrada (usando √≠ndices relativos). Numba genera todo el loop y maneja los bordes autom√°ticamente. Cuando se combina con <code>parallel=True</code>, se paraleliza.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Procesamiento de im√°genes (blur, detecci√≥n de bordes, sharpening)</li>
      <li>Simulaciones de ecuaciones diferenciales parciales (difusi√≥n de calor, Laplace)</li>
      <li>Aut√≥matas celulares (Game of Life, etc.)</li>
      <li>Cualquier c√°lculo donde el resultado en (i,j) depende de valores vecinos en el input</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Operaciones que no dependen de vecinos (usa @vectorize o @njit)</li>
      <li>El "vecindario" es variable o depende de los datos (usa @njit con loop manual)</li>
      <li>Necesitas acceder a posiciones absolutas, no relativas (o usa <code>standard_indexing</code>)</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Detecci√≥n de bordes (Sobel) en una imagen</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> stencil, njit, prange
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">sobel_x</span>(img):
    <span class="cm">"""Filtro Sobel horizontal: detecta bordes verticales."""</span>
    <span class="kw">return</span> (-<span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">-1</span>] + <span class="num">0</span> * img[<span class="num">-1</span>, <span class="num">0</span>] + <span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">1</span>] +
            -<span class="num">2</span> * img[ <span class="num">0</span>, <span class="num">-1</span>] + <span class="num">0</span> * img[ <span class="num">0</span>, <span class="num">0</span>] + <span class="num">2</span> * img[ <span class="num">0</span>, <span class="num">1</span>] +
            -<span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">-1</span>] + <span class="num">0</span> * img[ <span class="num">1</span>, <span class="num">0</span>] + <span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">1</span>])

<span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">sobel_y</span>(img):
    <span class="cm">"""Filtro Sobel vertical: detecta bordes horizontales."""</span>
    <span class="kw">return</span> (-<span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">-1</span>] - <span class="num">2</span> * img[<span class="num">-1</span>, <span class="num">0</span>] - <span class="num">1</span> * img[<span class="num">-1</span>, <span class="num">1</span>] +
             <span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">-1</span>] + <span class="num">2</span> * img[ <span class="num">1</span>, <span class="num">0</span>] + <span class="num">1</span> * img[ <span class="num">1</span>, <span class="num">1</span>])

<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">edge_detection</span>(imagen):
    <span class="cm">"""Magnitud del gradiente = sqrt(Gx¬≤ + Gy¬≤)"""</span>
    gx = sobel_x(imagen)
    gy = sobel_y(imagen)
    <span class="cm"># np.sqrt se paraleliza autom√°ticamente</span>
    <span class="kw">return</span> np.sqrt(gx ** <span class="num">2</span> + gy ** <span class="num">2</span>)

imagen = np.random.rand(<span class="num">1920</span>, <span class="num">1080</span>)  <span class="cm"># imagen Full HD</span>
bordes = edge_detection(imagen)</code></pre>

    <p><strong>Ejemplo 2: Simulaci√≥n de difusi√≥n de calor</strong></p>
<pre><code><span class="dec">@stencil</span>
<span class="kw">def</span> <span class="fn">heat_step</span>(T):
    <span class="cm">"""Un paso de la ecuaci√≥n de calor 2D discretizada."""</span>
    <span class="kw">return</span> <span class="num">0.25</span> * (T[<span class="num">-1</span>, <span class="num">0</span>] + T[<span class="num">1</span>, <span class="num">0</span>] + T[<span class="num">0</span>, <span class="num">-1</span>] + T[<span class="num">0</span>, <span class="num">1</span>])

<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">simulate_heat</span>(grid, n_steps):
    <span class="cm">"""Simula difusi√≥n de calor por n pasos."""</span>
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_steps):
        grid = heat_step(grid)
    <span class="kw">return</span> grid

<span class="cm"># Placa de metal 200x200, caliente en el centro</span>
placa = np.zeros((<span class="num">200</span>, <span class="num">200</span>))
placa[<span class="num">90</span>:<span class="num">110</span>, <span class="num">90</span>:<span class="num">110</span>] = <span class="num">100.0</span>  <span class="cm"># fuente de calor</span>
resultado = simulate_heat(placa, <span class="num">500</span>)</code></pre>

    <p><strong>Ejemplo 3: Suavizado de datos de sensor (1D)</strong></p>
<pre><code><span class="dec">@stencil</span>(neighborhood=((<span class="num">-2</span>, <span class="num">2</span>),))
<span class="kw">def</span> <span class="fn">smooth_sensor</span>(data):
    <span class="cm">"""Suavizado con ventana de 5 puntos (media simple)."""</span>
    <span class="kw">return</span> (data[<span class="num">-2</span>] + data[<span class="num">-1</span>] + data[<span class="num">0</span>] + data[<span class="num">1</span>] + data[<span class="num">2</span>]) / <span class="num">5.0</span>

sensor_ruidoso = np.sin(np.linspace(<span class="num">0</span>, <span class="num">10</span>, <span class="num">10000</span>)) + np.random.randn(<span class="num">10000</span>) * <span class="num">0.3</span>
sensor_limpio = smooth_sensor(sensor_ruidoso)</code></pre>

    <!-- ============================================ -->
    <!-- ============ @jitclass ==================== -->
    <!-- ============================================ -->
    <h3 style="color: #f85149; border-color: #f85149;">5. <code>@jitclass</code> ‚Äî Clases con estado compilado</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Compila una clase entera: los datos se almacenan como una estructura C en memoria (sin overhead de objetos Python) y todos los m√©todos se compilan a nopython. Ideal cuando necesitas <strong>encapsular estado mutable</strong> y operarlo eficientemente.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Simulaciones donde entidades tienen estado (part√≠culas, agentes, celdas)</li>
      <li>Estructuras de datos num√©ricas personalizadas (matrices sparse, √°rboles KD simplificados)</li>
      <li>Algoritmos iterativos con estado (optimizadores, filtros Kalman)</li>
    </ul>

    <h4>‚ùå Cu√°ndo NO usarlo</h4>
    <ul>
      <li>Solo necesitas funciones sin estado ‚Üí usa @njit</li>
      <li>Tu clase tiene herencia compleja, propiedades din√°micas, o m√©todos con strings ‚Üí no es compatible</li>
      <li>Solo necesitas pasar datos entre funciones ‚Üí un namedtuple o un array es m√°s simple</li>
    </ul>

    <h4>Ejemplos realistas</h4>

    <p><strong>Ejemplo 1: Filtro Kalman 1D para tracking de sensor</strong></p>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> float64
<span class="kw">from</span> numba.experimental <span class="kw">import</span> jitclass

<span class="dec">@jitclass</span>([
    (<span class="st">'x'</span>, float64),         <span class="cm"># estimaci√≥n actual</span>
    (<span class="st">'P'</span>, float64),         <span class="cm"># incertidumbre</span>
    (<span class="st">'Q'</span>, float64),         <span class="cm"># ruido del proceso</span>
    (<span class="st">'R'</span>, float64),         <span class="cm"># ruido de medici√≥n</span>
])
<span class="kw">class</span> <span class="fn">KalmanFilter1D</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, x0, P0, Q, R):
        self.x = x0
        self.P = P0
        self.Q = Q
        self.R = R

    <span class="kw">def</span> <span class="fn">predict</span>(self):
        <span class="cm"># En modelo simple: predicci√≥n = estado anterior</span>
        self.P += self.Q

    <span class="kw">def</span> <span class="fn">update</span>(self, measurement):
        K = self.P / (self.P + self.R)   <span class="cm"># Kalman gain</span>
        self.x += K * (measurement - self.x)
        self.P *= (<span class="num">1.0</span> - K)

    <span class="kw">def</span> <span class="fn">filter_signal</span>(self, measurements, output):
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(measurements)):
            self.predict()
            self.update(measurements[i])
            output[i] = self.x

<span class="cm"># Uso</span>
kf = KalmanFilter1D(x0=<span class="num">0.0</span>, P0=<span class="num">1.0</span>, Q=<span class="num">0.01</span>, R=<span class="num">0.5</span>)
mediciones = np.sin(np.linspace(<span class="num">0</span>, <span class="num">10</span>, <span class="num">1000</span>)) + np.random.randn(<span class="num">1000</span>) * <span class="num">0.5</span>
salida = np.empty(<span class="num">1000</span>)
kf.filter_signal(mediciones, salida)</code></pre>

    <p><strong>Ejemplo 2: Acumulador de estad√≠sticas online (Welford)</strong></p>
<pre><code><span class="dec">@jitclass</span>([
    (<span class="st">'n'</span>, float64),
    (<span class="st">'mean'</span>, float64),
    (<span class="st">'M2'</span>, float64),
])
<span class="kw">class</span> <span class="fn">OnlineStats</span>:
    <span class="cm">"""Calcula media y varianza incrementalmente (Welford's algorithm).
    √ötil cuando los datos llegan en streaming y no caben en memoria."""</span>

    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.n = <span class="num">0.0</span>
        self.mean = <span class="num">0.0</span>
        self.M2 = <span class="num">0.0</span>

    <span class="kw">def</span> <span class="fn">add</span>(self, x):
        self.n += <span class="num">1</span>
        delta = x - self.mean
        self.mean += delta / self.n
        delta2 = x - self.mean
        self.M2 += delta * delta2

    <span class="kw">def</span> <span class="fn">variance</span>(self):
        <span class="kw">if</span> self.n < <span class="num">2</span>:
            <span class="kw">return</span> <span class="num">0.0</span>
        <span class="kw">return</span> self.M2 / (self.n - <span class="num">1</span>)

<span class="cm"># Procesar un stream de datos enorme</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar_stream</span>(data):
    stats = OnlineStats()
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(data)):
        stats.add(data[i])
    <span class="kw">return</span> stats.mean, stats.variance()

data = np.random.randn(<span class="num">10_000_000</span>)
media, var = procesar_stream(data)</code></pre>

    <h4>üö´ Errores comunes con @jitclass</h4>
<pre><code><span class="cm"># ERROR 1: No inicializar contenedores en __init__</span>
<span class="dec">@jitclass</span>([(<span class="st">'d'</span>, types.DictType(types.int64, types.float64))])
<span class="kw">class</span> <span class="fn">Malo</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.d[<span class="num">1</span>] = <span class="num">10.0</span>  <span class="cm"># ‚ùå SEGFAULT: d no fue inicializado</span>

<span class="cm"># CORRECTO:</span>
<span class="kw">def</span> <span class="fn">__init__</span>(self):
    self.d = typed.Dict.empty(types.int64, types.float64)  <span class="cm"># ‚úÖ primero inicializar</span>
    self.d[<span class="num">1</span>] = <span class="num">10.0</span>

<span class="cm"># ERROR 2: NumPy arrays necesitan spec expl√≠cito (no se infieren de annotations)</span>
<span class="dec">@jitclass</span>
<span class="kw">class</span> <span class="fn">Malo2</span>:
    data: np.ndarray  <span class="cm"># ‚ùå Numba no sabe el dtype ni dimensiones</span>

<span class="cm"># CORRECTO: spec expl√≠cito para arrays</span>
<span class="dec">@jitclass</span>([(<span class="st">'data'</span>, float64[:])])
<span class="kw">class</span> <span class="fn">Bueno</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, n):
        self.data = np.zeros(n)</code></pre>

    <!-- ============================================ -->
    <!-- ============ @cfunc ======================= -->
    <!-- ============================================ -->
    <h3 style="color: #8b949e; border-color: #8b949e;">6. <code>@cfunc</code> ‚Äî Callbacks para C/Fortran</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Compila una funci√≥n Python en un callback con convenci√≥n de llamada C. Es la forma de pasar funciones Numba a librer√≠as C o a <code>scipy.integrate</code>, <code>scipy.optimize</code>, etc.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Quieres pasar una funci√≥n como callback a <code>scipy.integrate.quad</code></li>
      <li>Interact√∫as con c√≥digo C/Fortran v√≠a ctypes</li>
      <li>Necesitas una funci√≥n llamable desde fuera de Python</li>
    </ul>

    <h4>Ejemplo realista</h4>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> cfunc, float64
<span class="kw">from</span> scipy <span class="kw">import</span> integrate

<span class="cm"># Integraci√≥n num√©rica con SciPy, pero el integrando es Numba-compilado</span>
<span class="dec">@cfunc</span>(float64(float64))
<span class="kw">def</span> <span class="fn">integrando</span>(x):
    <span class="kw">return</span> np.exp(-x ** <span class="num">2</span>) * np.cos(<span class="num">2</span> * x)

<span class="cm"># .ctypes pasa el puntero C a SciPy</span>
resultado, error = integrate.quad(integrando.ctypes, <span class="num">0</span>, <span class="num">10</span>)
<span class="cm"># Mucho m√°s r√°pido que pasar una funci√≥n Python normal a quad()</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ @overload ==================== -->
    <!-- ============================================ -->
    <h3 style="color: #58a6ff; border-color: #58a6ff;">7. <code>@overload</code> ‚Äî Extender el ecosistema</h3>

    <h4>¬øQu√© hace realmente?</h4>
    <p>Te permite ense√±arle a Numba c√≥mo ejecutar funciones que no soporta nativamente. Defines una implementaci√≥n alternativa que Numba usar√° en nopython mode. Se ejecuta en <strong>compile time</strong> (no runtime) ‚Äî Numba llama a tu overload con los TIPOS de los argumentos, y t√∫ devuelves una funci√≥n que se compilar√°.</p>

    <h4>‚úÖ Cu√°ndo usarlo</h4>
    <ul>
      <li>Usas una funci√≥n de terceros dentro de @njit que Numba no soporta</li>
      <li>Quieres que funciones de tu librer√≠a sean usables en c√≥digo Numba</li>
      <li>Necesitas comportamiento polim√≥rfico (diferente implementaci√≥n seg√∫n el tipo)</li>
    </ul>

    <h4>Ejemplo realista</h4>
<pre><code><span class="kw">from</span> numba <span class="kw">import</span> types
<span class="kw">from</span> numba.extending <span class="kw">import</span> overload

<span class="cm"># Sup√≥n que tienes esta funci√≥n Python que usas en tu proyecto</span>
<span class="kw">def</span> <span class="fn">clip</span>(value, low, high):
    <span class="kw">return</span> min(max(value, low), high)

<span class="cm"># La haces funcionar dentro de @njit</span>
<span class="dec">@overload</span>(clip)
<span class="kw">def</span> <span class="fn">clip_overload</span>(value, low, high):
    <span class="cm"># Este c√≥digo se ejecuta en COMPILE TIME</span>
    <span class="cm"># value, low, high son TIPOS, no valores</span>
    <span class="kw">if</span> <span class="fn">isinstance</span>(value, types.Float):
        <span class="kw">def</span> <span class="fn">impl</span>(value, low, high):
            <span class="cm"># Este c√≥digo se ejecuta en RUNTIME</span>
            <span class="kw">if</span> value < low:
                <span class="kw">return</span> low
            <span class="kw">elif</span> value > high:
                <span class="kw">return</span> high
            <span class="kw">return</span> value
        <span class="kw">return</span> impl
    <span class="cm"># Si retorna None, Numba prueba otros overloads</span>

<span class="cm"># Ahora clip() funciona dentro de c√≥digo @njit</span>
<span class="dec">@njit</span>
<span class="kw">def</span> <span class="fn">procesar</span>(arr):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(arr)):
        arr[i] = clip(arr[i], <span class="num">0.0</span>, <span class="num">1.0</span>)  <span class="cm"># ‚úÖ Funciona gracias al overload</span>
    <span class="kw">return</span> arr</code></pre>

    <!-- ============================================ -->
    <!-- ============ COMPARACI√ìN DIRECTA ========== -->
    <!-- ============================================ -->
    <h3>üìä Comparaci√≥n directa: el mismo problema, diferentes decoradores</h3>
    <p>Para que quede claro cu√°ndo elegir cada uno, veamos c√≥mo resolver√≠as <strong>"calcular la norma L2 de vectores"</strong> con cada decorador y por qu√© elegir√≠as uno u otro.</p>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> numba <span class="kw">import</span> njit, vectorize, guvectorize, stencil, float64, prange

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN A: @njit ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Cuando tienes UNA matriz y quieres las normas de cada fila</span>
<span class="cm"># M√°s control, escribes el loop t√∫ mismo</span>
<span class="dec">@njit</span>(parallel=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">normas_njit</span>(matrix):
    n_rows = matrix.shape[<span class="num">0</span>]
    result = np.empty(n_rows)
    <span class="kw">for</span> i <span class="kw">in</span> prange(n_rows):
        total = <span class="num">0.0</span>
        <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(matrix.shape[<span class="num">1</span>]):
            total += matrix[i, j] ** <span class="num">2</span>
        result[i] = np.sqrt(total)
    <span class="kw">return</span> result
<span class="cm"># ‚úÖ Mejor cuando: necesitas control total del loop, o la l√≥gica es compleja</span>
<span class="cm"># ‚ùå Evitar cuando: necesitas broadcasting autom√°tico</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN B: @guvectorize ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># Cuando quieres que funcione con arrays de CUALQUIER forma via broadcasting</span>
<span class="dec">@guvectorize</span>([(float64[:], float64[:])], <span class="st">'(n)->()'</span>)
<span class="kw">def</span> <span class="fn">norma_guv</span>(vec, result):
    total = <span class="num">0.0</span>
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(vec.shape[<span class="num">0</span>]):
        total += vec[i] ** <span class="num">2</span>
    result[<span class="num">0</span>] = np.sqrt(total)
<span class="cm"># ‚úÖ Mejor cuando: quieres broadcasting autom√°tico</span>
<span class="cm">#    norma_guv(vector_1d) ‚Üí escalar</span>
<span class="cm">#    norma_guv(matrix_2d) ‚Üí vector (norma por fila)</span>
<span class="cm">#    norma_guv(tensor_3d) ‚Üí matrix (norma por √∫ltima dimensi√≥n)</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN C: @vectorize ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># NO es adecuado aqu√≠ porque vectorize es para operaciones escalar‚Üíescalar</span>
<span class="cm"># La norma necesita ver el vector completo, no elemento por elemento</span>

<span class="cm"># ‚îÄ‚îÄ‚îÄ OPCI√ìN D: @stencil ‚îÄ‚îÄ‚îÄ</span>
<span class="cm"># NO es adecuado aqu√≠ porque la norma no es un patr√≥n de vecindario</span>
<span class="cm"># Stencil es para cuando el resultado en posici√≥n [i] depende de [i-1], [i+1], etc.</span></code></pre>

    <!-- ============================================ -->
    <!-- ============ RESUMEN FINAL ================ -->
    <!-- ============================================ -->
    <h3>üéØ Resumen final: √°rbol de decisi√≥n</h3>
<pre><code><span class="cm">¬øQu√© tipo de operaci√≥n tienes?
‚îÇ
‚îú‚îÄ Escalar ‚Üí Escalar (sin ver vecinos ni el array completo)
‚îÇ  ‚îî‚îÄ ¬øNecesitas broadcasting/reduce/accumulate?
‚îÇ     ‚îú‚îÄ S√ç ‚Üí <strong style="color:#3fb950">@vectorize</strong>
‚îÇ     ‚îî‚îÄ NO ‚Üí <strong style="color:#58a6ff">@njit</strong> (m√°s simple)
‚îÇ
‚îú‚îÄ Sub-array ‚Üí Escalar o Sub-array (operas sobre filas, vectores, etc.)
‚îÇ  ‚îî‚îÄ ¬øNecesitas broadcasting entre inputs de distintas dimensiones?
‚îÇ     ‚îú‚îÄ S√ç ‚Üí <strong style="color:#d2a8ff">@guvectorize</strong>
‚îÇ     ‚îî‚îÄ NO ‚Üí <strong style="color:#58a6ff">@njit</strong> con loop manual
‚îÇ
‚îú‚îÄ Cada resultado depende de VECINOS en un patr√≥n fijo
‚îÇ  ‚îî‚îÄ <strong style="color:#f0883e">@stencil</strong> (+ parallel=True en la funci√≥n que lo llama)
‚îÇ
‚îú‚îÄ Necesitas una clase con estado y m√©todos r√°pidos
‚îÇ  ‚îî‚îÄ <strong style="color:#f85149">@jitclass</strong>
‚îÇ
‚îú‚îÄ Necesitas pasar una funci√≥n como callback a C/SciPy
‚îÇ  ‚îî‚îÄ <strong style="color:#8b949e">@cfunc</strong>
‚îÇ
‚îî‚îÄ Necesitas que una funci√≥n externa funcione dentro de @njit
   ‚îî‚îÄ <strong style="color:#58a6ff">@overload</strong></span></code></pre>

    <div class="tip">
      <strong>Regla de oro:</strong> Empieza siempre con <code>@njit</code>. Solo cambia a otro decorador cuando necesites una capacidad espec√≠fica que <code>@njit</code> no te da (broadcasting ‚Üí @vectorize/@guvectorize, vecinos ‚Üí @stencil, estado ‚Üí @jitclass).
    </div>

  </div>
</div>




<!-- ==================== M√ìDULO 10 ==================== -->
<div class="module" id="m10">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">10</span> Cheatsheet y Patrones Comunes</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <h3>Referencia r√°pida de decoradores</h3>
    <table>
      <tr><th>Decorador</th><th>Para qu√©</th><th>Ejemplo clave</th></tr>
      <tr><td><code>@jit / @njit</code></td><td>Compilar funciones generales</td><td><code>@njit(parallel=True, fastmath=True, cache=True)</code></td></tr>
      <tr><td><code>@vectorize</code></td><td>Crear ufuncs (escalar‚Üíescalar)</td><td><code>@vectorize([float64(float64, float64)])</code></td></tr>
      <tr><td><code>@guvectorize</code></td><td>Ufuncs generalizados (array‚Üíarray)</td><td><code>@guvectorize([(f64[:], f64[:])], '(n)->()')</code></td></tr>
      <tr><td><code>@stencil</code></td><td>Operaciones sobre vecindarios</td><td><code>@stencil(neighborhood=((-1,1),(-1,1)))</code></td></tr>
      <tr><td><code>@jitclass</code></td><td>Clases compiladas con estado</td><td><code>@jitclass([('x', float64)])</code></td></tr>
      <tr><td><code>@cfunc</code></td><td>Crear callbacks C</td><td><code>@cfunc("float64(float64)")</code></td></tr>
      <tr><td><code>@overload</code></td><td>Extender funciones para nopython</td><td><code>@overload(mi_funcion)</code></td></tr>
      <tr><td><code>@intrinsic</code></td><td>Generar LLVM IR directamente</td><td>Para expertos en LLVM</td></tr>
    </table>

    <h3>Opciones de @jit combinadas</h3>
<pre><code><span class="cm"># El "full power" Numba decorator</span>
<span class="dec">@njit</span>(
    parallel=<span class="num">True</span>,     <span class="cm"># Auto-paralelizar operaciones</span>
    fastmath=<span class="num">True</span>,     <span class="cm"># Relajar IEEE 754</span>
    cache=<span class="num">True</span>,        <span class="cm"># Guardar compilaci√≥n en disco</span>
    nogil=<span class="num">True</span>,        <span class="cm"># Liberar GIL para multithreading</span>
    boundscheck=<span class="num">False</span>, <span class="cm"># No verificar l√≠mites de array (default)</span>
    error_model=<span class="st">'numpy'</span>,  <span class="cm"># Seguir sem√°ntica de errores de NumPy</span>
)
<span class="kw">def</span> <span class="fn">ultimate_function</span>(data):
    ...</code></pre>

    <h3>Patr√≥n: Procesar datos en paralelo con threading</h3>
<pre><code><span class="kw">import</span> threading
<span class="kw">from</span> numba <span class="kw">import</span> njit

<span class="dec">@njit</span>(nogil=<span class="num">True</span>)
<span class="kw">def</span> <span class="fn">process_chunk</span>(data, result, start, end):
    <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(start, end):
        result[i] = np.sqrt(data[i]) * np.log(data[i] + <span class="num">1</span>)

<span class="cm"># Dividir trabajo en threads</span>
data = np.random.rand(<span class="num">10_000_000</span>)
result = np.empty_like(data)
n_threads = <span class="num">4</span>
chunk = <span class="fn">len</span>(data) // n_threads

threads = []
<span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_threads):
    start = t * chunk
    end = start + chunk <span class="kw">if</span> t < n_threads - <span class="num">1</span> <span class="kw">else</span> <span class="fn">len</span>(data)
    thread = threading.Thread(target=process_chunk, args=(data, result, start, end))
    threads.append(thread)
    thread.start()
<span class="kw">for</span> t <span class="kw">in</span> threads:
    t.join()</code></pre>

    <h3>Patr√≥n: Ahead-of-Time compilation</h3>
<pre><code><span class="kw">from</span> numba.pycc <span class="kw">import</span> CC

cc = CC(<span class="st">'my_module'</span>)

<span class="dec">@cc.export</span>(<span class="st">'multf'</span>, <span class="st">'f8(f8, f8)'</span>)
<span class="kw">def</span> <span class="fn">mult</span>(a, b):
    <span class="kw">return</span> a * b

<span class="dec">@cc.export</span>(<span class="st">'square'</span>, <span class="st">'f8(f8)'</span>)
<span class="kw">def</span> <span class="fn">square</span>(a):
    <span class="kw">return</span> a ** <span class="num">2</span>

cc.compile()  <span class="cm"># Genera my_module.so / my_module.pyd</span>
<span class="cm"># Luego: import my_module; my_module.multf(3.0, 4.0)</span></code></pre>

    <h3>Variables de entorno √∫tiles</h3>
    <table>
      <tr><th>Variable</th><th>Efecto</th></tr>
      <tr><td><code>NUMBA_NUM_THREADS=N</code></td><td>N√∫mero de threads para paralelismo</td></tr>
      <tr><td><code>NUMBA_PARALLEL_DIAGNOSTICS=4</code></td><td>Diagn√≥sticos de auto-paralelizaci√≥n</td></tr>
      <tr><td><code>NUMBA_DEVELOPER_MODE=1</code></td><td>Mensajes de error detallados</td></tr>
      <tr><td><code>NUMBA_DISABLE_JIT=1</code></td><td>Desactiva JIT (para debugging)</td></tr>
      <tr><td><code>NUMBA_CACHE_DIR=path</code></td><td>Directorio para el cache de compilaci√≥n</td></tr>
      <tr><td><code>NUMBA_THREADING_LAYER=tbb</code></td><td>Seleccionar backend de threading (tbb/omp/workqueue)</td></tr>
    </table>

    <h3>Lo que Numba NO soporta (trampas comunes)</h3>
    <ul>
      <li>Listas de Python est√°ndar (usa <code>numba.typed.List</code>)</li>
      <li>Diccionarios Python est√°ndar (usa <code>numba.typed.Dict</code>)</li>
      <li>Excepciones personalizadas con argumentos</li>
      <li>Clases regulares de Python (usa <code>@jitclass</code>)</li>
      <li><code>try/except</code> limitado (solo excepciones b√°sicas)</li>
      <li>Strings con operaciones complejas</li>
      <li>Generadores con <code>yield</code> (soporte limitado)</li>
      <li>Recursi√≥n con tipos diferentes en cada nivel</li>
      <li><code>**kwargs</code> (solo <code>*args</code> limitado)</li>
    </ul>

    <div class="tip">
      <strong>Regla de oro:</strong> Si tu c√≥digo es principalmente num√©rico con arrays NumPy y loops, Numba lo acelerar√°. Si depende mucho de objetos Python, strings, o I/O, busca otras herramientas (Cython, C extensions).
    </div>
  </div>
</div>

<!-- ==================== M√ìDULO 11 ==================== -->
<!-- √ÅRBOL DE DECISI√ìN INTERACTIVO NUMBA -->
 

<div class="module" id="m11">
  <div class="module-header" onclick="toggle(this)">
    <h2><span class="num">11</span> üå≥ √Årbol de Decisi√≥n Interactivo ‚Äî ¬øQu√© decorador usar?</h2>
    <span class="arrow">‚ñ∂</span>
  </div>
  <div class="module-body">

    <p>Este √°rbol de decisi√≥n interactivo te gu√≠a paso a paso para elegir el decorador y las opciones correctas de Numba seg√∫n tu caso de uso. Responde las preguntas y obt√©n una recomendaci√≥n fundamentada con c√≥digo listo para usar.</p>

    <div class="info">
      <strong>Basado en el consenso:</strong> Este √°rbol fue construido contrastando la documentaci√≥n oficial de Numba (Dec 2025), conocimiento emp√≠rico de la comunidad, y pr√°cticas de ingenier√≠a a escala. Cada recomendaci√≥n incluye el <em>por qu√©</em>, no solo el <em>qu√©</em>.
    </div>

    <!-- ============================================ -->
    <!-- ======= ESTILOS DEL √ÅRBOL DE DECISI√ìN ====== -->
    <!-- ============================================ -->
    <style>
      /* ‚îÄ‚îÄ Motor del √°rbol ‚îÄ‚îÄ */
      .dt-engine {
        position: relative;
        margin: 1.5rem 0;
      }

      /* ‚îÄ‚îÄ Breadcrumb / ruta ‚îÄ‚îÄ */
      .dt-breadcrumb {
        display: flex;
        flex-wrap: wrap;
        gap: 0.4rem;
        align-items: center;
        margin-bottom: 1.2rem;
        padding: 0.6rem 1rem;
        background: rgba(88,166,255,0.04);
        border: 1px solid var(--border);
        border-radius: 8px;
        font-size: 0.8rem;
        color: var(--muted);
        min-height: 2.4rem;
      }
      .dt-breadcrumb:empty::before {
        content: "üå≥ Comienza respondiendo la primera pregunta...";
        color: var(--muted);
        font-style: italic;
      }
      .dt-crumb {
        display: inline-flex;
        align-items: center;
        gap: 0.3rem;
        padding: 2px 8px;
        background: rgba(88,166,255,0.1);
        border-radius: 4px;
        color: var(--accent);
        cursor: pointer;
        transition: background 0.15s;
      }
      .dt-crumb:hover {
        background: rgba(88,166,255,0.2);
      }
      .dt-crumb-sep {
        color: var(--border);
        font-size: 0.7rem;
      }

      /* ‚îÄ‚îÄ Tarjeta de pregunta ‚îÄ‚îÄ */
      .dt-card {
        border: 1px solid var(--border);
        border-radius: 12px;
        overflow: hidden;
        background: var(--surface);
        animation: dtSlideIn 0.3s ease-out;
        margin-bottom: 1rem;
      }
      @keyframes dtSlideIn {
        from { opacity: 0; transform: translateY(12px); }
        to { opacity: 1; transform: translateY(0); }
      }
      .dt-card-header {
        padding: 1rem 1.2rem;
        border-bottom: 1px solid var(--border);
        display: flex;
        align-items: center;
        gap: 0.6rem;
      }
      .dt-step-badge {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        width: 28px;
        height: 28px;
        border-radius: 50%;
        font-size: 0.75rem;
        font-weight: 700;
        flex-shrink: 0;
      }
      .dt-step-badge.q { background: rgba(88,166,255,0.15); color: var(--accent); }
      .dt-step-badge.r { background: rgba(63,185,80,0.15); color: var(--accent2); }
      .dt-question-text {
        font-size: 1rem;
        font-weight: 600;
        line-height: 1.4;
      }
      .dt-card-body {
        padding: 1rem 1.2rem;
      }
      .dt-context {
        font-size: 0.85rem;
        color: var(--muted);
        margin-bottom: 0.8rem;
        line-height: 1.5;
        padding: 0.6rem 0.8rem;
        background: rgba(255,255,255,0.02);
        border-radius: 6px;
        border-left: 3px solid var(--border);
      }

      /* ‚îÄ‚îÄ Opciones ‚îÄ‚îÄ */
      .dt-options {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
      }
      .dt-option {
        display: flex;
        align-items: flex-start;
        gap: 0.8rem;
        padding: 0.8rem 1rem;
        border: 1px solid var(--border);
        border-radius: 8px;
        cursor: pointer;
        transition: all 0.15s;
        background: transparent;
        text-align: left;
        color: var(--text);
        font-size: 0.9rem;
        line-height: 1.5;
        width: 100%;
        font-family: inherit;
      }
      .dt-option:hover {
        border-color: var(--accent);
        background: rgba(88,166,255,0.05);
      }
      .dt-option.selected {
        border-color: var(--accent);
        background: rgba(88,166,255,0.08);
      }
      .dt-option-icon {
        font-size: 1.1rem;
        flex-shrink: 0;
        margin-top: 1px;
      }
      .dt-option-content {
        flex: 1;
      }
      .dt-option-label {
        font-weight: 600;
        display: block;
        margin-bottom: 2px;
      }
      .dt-option-desc {
        font-size: 0.82rem;
        color: var(--muted);
        display: block;
      }

      /* ‚îÄ‚îÄ Resultado final ‚îÄ‚îÄ */
      .dt-result {
        border: 2px solid var(--accent2);
        border-radius: 12px;
        overflow: hidden;
        animation: dtSlideIn 0.35s ease-out;
        margin-bottom: 1rem;
      }
      .dt-result-header {
        background: rgba(63,185,80,0.1);
        padding: 1rem 1.2rem;
        border-bottom: 1px solid rgba(63,185,80,0.2);
        display: flex;
        align-items: center;
        gap: 0.6rem;
      }
      .dt-result-title {
        font-size: 1.1rem;
        font-weight: 700;
        color: var(--accent2);
      }
      .dt-result-body {
        padding: 1.2rem;
        background: var(--surface);
      }
      .dt-result-decorator {
        display: inline-block;
        padding: 4px 14px;
        border-radius: 6px;
        font-family: 'SF Mono', 'Fira Code', monospace;
        font-size: 1.05rem;
        font-weight: 700;
        margin-bottom: 0.8rem;
      }
      .dt-result-decorator.njit { background: rgba(88,166,255,0.15); color: var(--accent); }
      .dt-result-decorator.vectorize { background: rgba(63,185,80,0.15); color: var(--accent2); }
      .dt-result-decorator.guvectorize { background: rgba(210,168,255,0.15); color: var(--accent3); }
      .dt-result-decorator.stencil { background: rgba(240,136,62,0.15); color: var(--accent4); }
      .dt-result-decorator.jitclass { background: rgba(248,81,73,0.15); color: var(--danger); }
      .dt-result-decorator.cfunc { background: rgba(139,148,158,0.2); color: var(--muted); }
      .dt-result-decorator.overload { background: rgba(88,166,255,0.15); color: var(--accent); }
      .dt-result-decorator.objmode { background: rgba(240,136,62,0.15); color: var(--accent4); }
      .dt-result-decorator.jit_module { background: rgba(210,168,255,0.15); color: var(--accent3); }

      .dt-result-why {
        margin: 0.8rem 0;
        font-size: 0.9rem;
        line-height: 1.6;
      }
      .dt-result-why strong {
        color: var(--accent);
      }

      .dt-result-code {
        background: var(--code-bg);
        border: 1px solid var(--border);
        border-radius: 8px;
        padding: 1rem 1.2rem;
        overflow-x: auto;
        font-size: 0.85rem;
        line-height: 1.6;
        font-family: 'SF Mono', 'Fira Code', monospace;
        margin: 0.8rem 0;
        white-space: pre;
      }

      .dt-result-options {
        margin-top: 0.8rem;
        padding: 0.8rem;
        background: rgba(240,136,62,0.05);
        border-radius: 8px;
        border: 1px solid rgba(240,136,62,0.15);
      }
      .dt-result-options-title {
        font-weight: 700;
        font-size: 0.85rem;
        color: var(--accent4);
        margin-bottom: 0.5rem;
      }
      .dt-result-option-row {
        display: flex;
        align-items: flex-start;
        gap: 0.4rem;
        margin-bottom: 0.4rem;
        font-size: 0.82rem;
        line-height: 1.4;
      }
      .dt-result-option-row code {
        background: rgba(88,166,255,0.1);
        padding: 1px 5px;
        border-radius: 3px;
        font-size: 0.8em;
        color: var(--accent);
        white-space: nowrap;
      }

      .dt-pitfall {
        margin-top: 0.8rem;
        padding: 0.8rem;
        background: rgba(248,81,73,0.05);
        border-radius: 8px;
        border: 1px solid rgba(248,81,73,0.15);
        font-size: 0.85rem;
        line-height: 1.5;
      }
      .dt-pitfall-title {
        font-weight: 700;
        color: var(--danger);
        font-size: 0.85rem;
        margin-bottom: 0.4rem;
      }

      .dt-diagnostic {
        margin-top: 0.8rem;
        padding: 0.8rem;
        background: rgba(88,166,255,0.05);
        border-radius: 8px;
        border: 1px solid rgba(88,166,255,0.15);
        font-size: 0.85rem;
        line-height: 1.5;
      }
      .dt-diagnostic-title {
        font-weight: 700;
        color: var(--accent);
        font-size: 0.85rem;
        margin-bottom: 0.4rem;
      }

      /* ‚îÄ‚îÄ Botones de acci√≥n ‚îÄ‚îÄ */
      .dt-actions {
        display: flex;
        gap: 0.6rem;
        margin-top: 1.2rem;
        flex-wrap: wrap;
      }
      .dt-btn {
        padding: 0.5rem 1rem;
        border-radius: 8px;
        border: 1px solid var(--border);
        background: var(--surface);
        color: var(--text);
        font-size: 0.85rem;
        cursor: pointer;
        transition: all 0.15s;
        font-family: inherit;
        display: inline-flex;
        align-items: center;
        gap: 0.4rem;
      }
      .dt-btn:hover {
        border-color: var(--accent);
        background: rgba(88,166,255,0.05);
      }
      .dt-btn.primary {
        background: rgba(88,166,255,0.15);
        border-color: rgba(88,166,255,0.3);
        color: var(--accent);
      }
      .dt-btn.primary:hover {
        background: rgba(88,166,255,0.25);
      }

      /* ‚îÄ‚îÄ Historia / respuestas anteriores (colapsadas) ‚îÄ‚îÄ */
      .dt-answered {
        border: 1px solid var(--border);
        border-radius: 8px;
        padding: 0.6rem 1rem;
        margin-bottom: 0.5rem;
        background: rgba(255,255,255,0.01);
        display: flex;
        align-items: center;
        gap: 0.6rem;
        font-size: 0.85rem;
        opacity: 0.6;
        transition: opacity 0.15s;
      }
      .dt-answered:hover {
        opacity: 0.9;
      }
      .dt-answered-q {
        color: var(--muted);
        flex-shrink: 0;
      }
      .dt-answered-a {
        color: var(--accent);
        font-weight: 600;
      }

      /* ‚îÄ‚îÄ Secci√≥n de referencia r√°pida ‚îÄ‚îÄ */
      .dt-quickref {
        margin-top: 2rem;
        border-top: 1px solid var(--border);
        padding-top: 1.5rem;
      }
      .dt-quickref h4 {
        color: var(--accent);
        margin-bottom: 1rem;
      }
      .dt-ref-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(260px, 1fr));
        gap: 0.8rem;
      }
      .dt-ref-card {
        padding: 0.8rem 1rem;
        border: 1px solid var(--border);
        border-radius: 8px;
        background: var(--surface);
        cursor: pointer;
        transition: all 0.15s;
      }
      .dt-ref-card:hover {
        border-color: var(--accent);
        background: rgba(88,166,255,0.03);
      }
      .dt-ref-card-title {
        font-weight: 700;
        font-size: 0.9rem;
        margin-bottom: 0.3rem;
        font-family: 'SF Mono', 'Fira Code', monospace;
      }
      .dt-ref-card-desc {
        font-size: 0.8rem;
        color: var(--muted);
        line-height: 1.4;
      }

      /* ‚îÄ‚îÄ Counter / stats ‚îÄ‚îÄ */
      .dt-stats {
        display: flex;
        gap: 1rem;
        margin-bottom: 1rem;
        flex-wrap: wrap;
      }
      .dt-stat {
        padding: 0.4rem 0.8rem;
        background: var(--surface);
        border: 1px solid var(--border);
        border-radius: 6px;
        font-size: 0.78rem;
        color: var(--muted);
      }
      .dt-stat strong {
        color: var(--accent);
      }

      /* ‚îÄ‚îÄ Responsive ‚îÄ‚îÄ */
      @media (max-width: 600px) {
        .dt-ref-grid { grid-template-columns: 1fr; }
        .dt-option { flex-direction: column; gap: 0.3rem; }
        .dt-actions { flex-direction: column; }
        .dt-btn { justify-content: center; }
      }
    </style>

    <!-- ============================================ -->
    <!-- ========= CONTENEDOR DEL √ÅRBOL ============= -->
    <!-- ============================================ -->

    <div class="dt-stats">
      <div class="dt-stat">üå≥ <strong>8 decoradores</strong> cubiertos</div>
      <div class="dt-stat">üîÄ <strong>35+ rutas</strong> de decisi√≥n</div>
      <div class="dt-stat">‚ö° <strong>15+ optimizaciones</strong> transversales</div>
    </div>

    <div class="dt-breadcrumb" id="dt-breadcrumb"></div>
    <div id="dt-history"></div>
    <div id="dt-current"></div>

    <!-- ============================================ -->
    <!-- ============ REFERENCIA R√ÅPIDA ============= -->
    <!-- ============================================ -->
    <div class="dt-quickref">
      <h4>‚ö° Acceso directo ‚Äî Ir a un decorador espec√≠fico</h4>
      <div class="dt-ref-grid">
        <div class="dt-ref-card" onclick="dtJumpTo('result_njit_basic')">
          <div class="dt-ref-card-title" style="color:var(--accent)">@njit</div>
          <div class="dt-ref-card-desc">Compilar funciones con loops num√©ricos. El punto de partida para todo.</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_njit_parallel')">
          <div class="dt-ref-card-title" style="color:var(--accent)">@njit(parallel=True)</div>
          <div class="dt-ref-card-desc">Auto-paralelizaci√≥n con prange para iteraciones independientes.</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_vectorize')">
          <div class="dt-ref-card-title" style="color:var(--accent2)">@vectorize</div>
          <div class="dt-ref-card-desc">Crear ufuncs: escalar‚Üíescalar aplicado a arrays con broadcasting.</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_guvectorize')">
          <div class="dt-ref-card-title" style="color:var(--accent3)">@guvectorize</div>
          <div class="dt-ref-card-desc">Ufuncs generalizados: operar sobre sub-arrays (filas, ventanas).</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_stencil')">
          <div class="dt-ref-card-title" style="color:var(--accent4)">@stencil</div>
          <div class="dt-ref-card-desc">Operaciones sobre vecindarios fijos: blur, difusi√≥n, convolutions.</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_jitclass')">
          <div class="dt-ref-card-title" style="color:var(--danger)">@jitclass</div>
          <div class="dt-ref-card-desc">Clases compiladas con estado mutable y m√©todos r√°pidos.</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_cfunc')">
          <div class="dt-ref-card-title" style="color:var(--muted)">@cfunc</div>
          <div class="dt-ref-card-desc">Callbacks C para SciPy, ctypes, y librer√≠as externas.</div>
        </div>
        <div class="dt-ref-card" onclick="dtJumpTo('result_overload')">
          <div class="dt-ref-card-title" style="color:var(--accent)">@overload</div>
          <div class="dt-ref-card-desc">Hacer funciones externas compatibles con nopython mode.</div>
        </div>
      </div>
    </div>

    <!-- ============================================ -->
    <!-- ======== JAVASCRIPT DEL √ÅRBOL ============== -->
    <!-- ============================================ -->
    <script>
    (function() {
      'use strict';

      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
      // ‚ñà‚ñà BASE DE CONOCIMIENTO COMPLETA DE NUMBA ‚ñà‚ñà
      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

      const TREE = {

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // NODO RA√çZ
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        root: {
          type: 'question',
          text: '¬øQu√© tipo de problema est√°s intentando resolver?',
          context: 'Esta es la pregunta fundamental. El decorador correcto depende de la NATURALEZA de tu operaci√≥n, no de su complejidad.',
          options: [
            { id: 'loop_numerico', icon: 'üîÅ', label: 'Tengo un loop num√©rico lento', desc: 'Un for en Python que recorre datos y hace c√°lculos aritm√©ticos/matem√°ticos.' },
            { id: 'escalar_a_array', icon: 'üìê', label: 'Operaci√≥n escalar aplicada a arrays', desc: 'Una funci√≥n que toma UN n√∫mero y devuelve UN n√∫mero, y quiero aplicarla a todo un array.' },
            { id: 'subarray_ops', icon: 'üìä', label: 'Operaci√≥n sobre sub-arrays (filas, ventanas)', desc: 'Necesito procesar filas de una matriz, ventanas de series temporales, o secciones de datos.' },
            { id: 'vecindario', icon: 'üî≤', label: 'Cada resultado depende de sus vecinos', desc: 'Procesamiento de im√°genes, filtros de convoluci√≥n, difusi√≥n de calor, cellular automata.' },
            { id: 'estado', icon: 'üèóÔ∏è', label: 'Necesito un objeto con estado mutable', desc: 'Filtro Kalman, acumulador online, simulador de part√≠culas ‚Äî algo que mantiene estado entre llamadas.' },
            { id: 'interop', icon: 'üîó', label: 'Interoperabilidad (callbacks, extensiones)', desc: 'Pasar funciones a SciPy/C, hacer funciones externas compatibles con Numba, o jittear un m√≥dulo entero.' },
            { id: 'no_se', icon: 'ü§î', label: 'No estoy seguro / mi caso es diferente', desc: 'Quiero entender mejor antes de decidir, o mi problema no encaja en las categor√≠as anteriores.' }
          ]
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 1: LOOP NUM√âRICO
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        loop_numerico: {
          type: 'question',
          text: '¬øLas iteraciones de tu loop son independientes entre s√≠?',
          context: 'Si cada iteraci√≥n NO lee resultados de iteraciones anteriores, el loop es paralelizable. Ejemplo independiente: calcular sqrt(x[i]) para cada i. Ejemplo dependiente: x[i] = x[i-1] * factor (cada paso necesita el anterior).',
          options: [
            { id: 'loop_paralelo', icon: '‚úÖ', label: 'S√≠, cada iteraci√≥n es independiente', desc: 'Puedo calcular cualquier iteraci√≥n sin conocer el resultado de las dem√°s.' },
            { id: 'loop_secuencial', icon: 'üîó', label: 'No, hay dependencia entre iteraciones', desc: 'Cada paso usa el resultado del paso anterior (EMA, filtro Kalman, series recursivas).' },
            { id: 'loop_mixto', icon: '‚ÜîÔ∏è', label: 'Hay un loop externo independiente y uno interno dependiente', desc: 'Ejemplo: para cada activo (independiente), calcular una EMA (dependiente).' }
          ]
        },

        loop_paralelo: {
          type: 'question',
          text: '¬øCu√°ntas iteraciones tiene tu loop aproximadamente?',
          context: 'El paralelismo tiene un overhead de scheduling (~microsegundos para distribuir trabajo entre threads). Con pocas iteraciones, este overhead puede superar el beneficio.',
          options: [
            { id: 'loop_par_grande', icon: 'üöÄ', label: 'M√°s de 10,000 iteraciones', desc: 'El overhead de paralelismo es despreciable. Vale la pena paralelizar.' },
            { id: 'loop_par_medio', icon: 'üìè', label: 'Entre 100 y 10,000 iteraciones', desc: 'Zona gris. Depende de cu√°nto trabajo hace cada iteraci√≥n.' },
            { id: 'loop_par_chico', icon: 'üî¨', label: 'Menos de 100 iteraciones', desc: 'Probablemente no vale la pena paralelizar. El overhead puede ser mayor que el beneficio.' }
          ]
        },

        loop_par_grande: {
          type: 'question',
          text: '¬øLa precisi√≥n exacta IEEE 754 es cr√≠tica?',
          context: 'fastmath=True permite reordenar operaciones de punto flotante (error ~4 ULP). Esto habilita auto-vectorizaci√≥n SIMD y reduce el tiempo ~2x. En finanzas donde un centavo importa: NO. En ML, procesamiento de se√±ales, simulaciones: S√ç es aceptable.',
          options: [
            { id: 'result_njit_parallel_fast', icon: '‚ö°', label: 'No es cr√≠tica (ML, se√±ales, simulaciones)', desc: 'Puedo tolerar un error de ~4 ULP a cambio de ~2x m√°s velocidad.' },
            { id: 'result_njit_parallel_strict', icon: 'üéØ', label: 'S√≠ es cr√≠tica (finanzas, contabilidad, legal)', desc: 'Necesito resultados exactos seg√∫n IEEE 754. Ni un centavo de error.' }
          ]
        },

        loop_par_medio: {
          type: 'question',
          text: '¬øCada iteraci√≥n individual hace mucho trabajo?',
          context: 'Si cada iteraci√≥n hace trabajo pesado (operaciones matriciales, muchos c√°lculos internos), el paralelismo vale la pena incluso con pocas iteraciones. Si cada iteraci√≥n es trivial (una suma, una multiplicaci√≥n), el overhead de scheduling domina.',
          options: [
            { id: 'loop_par_grande', icon: 'üí™', label: 'S√≠, cada iteraci√≥n es pesada', desc: 'Ejemplo: cada iteraci√≥n resuelve un sistema lineal, calcula una FFT, o hace un loop interno largo.' },
            { id: 'result_njit_basic', icon: 'ü™∂', label: 'No, cada iteraci√≥n es liviana', desc: 'Ejemplo: cada iteraci√≥n hace una suma/multiplicaci√≥n simple.' }
          ]
        },

        loop_par_chico: {
          type: 'result',
          redirect: 'result_njit_basic'
        },

        loop_secuencial: {
          type: 'question',
          text: '¬øEsta funci√≥n se llama muchas veces desde Python?',
          context: 'El overhead de llamar una funci√≥n Numba desde Python es ~1Œºs. Si llamas la funci√≥n 100,000 veces en un loop de Python, pierdes ~0.1 segundos solo en overhead de dispatch. La soluci√≥n: mover el loop DENTRO de la funci√≥n Numba.',
          options: [
            { id: 'result_njit_batch', icon: 'üîÑ', label: 'S√≠, se llama >1,000 veces en un loop Python', desc: 'Ejemplo: llamo calculate_ema(data) para cada activo en un loop Python.' },
            { id: 'loop_seq_cache', icon: '1Ô∏è‚É£', label: 'No, se llama pocas veces con datos grandes', desc: 'Ejemplo: llamo una vez con 10 millones de datos.' }
          ]
        },

        loop_seq_cache: {
          type: 'question',
          text: '¬øLa funci√≥n se ejecuta repetidamente al reiniciar el script?',
          context: 'cache=True guarda la funci√≥n compilada en disco. En la siguiente ejecuci√≥n del script, Numba carga desde disco en lugar de recompilar (~0.5s‚Üí~0.05s de startup). No lo uses si cambias frecuentemente funciones importadas de otros m√≥dulos, porque el cache no detecta esos cambios.',
          options: [
            { id: 'result_njit_cached', icon: 'üíæ', label: 'S√≠, es un script/servicio que se ejecuta repetidamente', desc: 'Ejemplo: un pipeline ETL diario, un servicio web, un backtest que corro muchas veces.' },
            { id: 'result_njit_basic', icon: 'üß™', label: 'No, estoy en desarrollo/experimentaci√≥n activa', desc: 'Cambio la funci√≥n frecuentemente y no me importa la compilaci√≥n inicial.' }
          ]
        },

        loop_mixto: {
          type: 'result',
          redirect: 'result_njit_parallel_inner'
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 2: ESCALAR A ARRAY
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        escalar_a_array: {
          type: 'question',
          text: '¬øTu funci√≥n toma UN n√∫mero y devuelve UN n√∫mero?',
          context: 'Esto es clave. @vectorize solo funciona con funciones puramente escalares: recibe escalares, devuelve un escalar. Si necesitas ver m√°s de un elemento a la vez (calcular norma, media, varianza), @vectorize NO sirve.',
          options: [
            { id: 'escalar_puro', icon: '‚úÖ', label: 'S√≠, es puramente escalar ‚Üí escalar', desc: 'Ejemplos: calcular impuesto progresivo, funci√≥n de activaci√≥n custom, convertir temperatura.' },
            { id: 'subarray_ops', icon: '‚ùå', label: 'No, necesita ver varios elementos a la vez', desc: 'Necesita calcular media, norma, varianza, o comparar con otros elementos del array.' }
          ]
        },

        escalar_puro: {
          type: 'question',
          text: '¬øNecesitas las capacidades avanzadas de ufunc?',
          context: '@vectorize te da gratis: broadcasting autom√°tico (funciona con arrays de cualquier shape), .reduce() y .accumulate(), y la integraci√≥n nativa con NumPy. Si solo necesitas aplicar la funci√≥n a un array 1D, @njit con un loop es m√°s simple y sin overhead de dispatch de ufunc.',
          options: [
            { id: 'vectorize_avanzado', icon: 'üéØ', label: 'S√≠: broadcasting, .reduce(), .accumulate()', desc: 'Quiero que funcione con arrays de cualquier shape, o necesito reduce/accumulate.' },
            { id: 'vectorize_simple', icon: 'üìè', label: 'Solo necesito aplicarlo a un array 1D', desc: 'No necesito broadcasting ni operaciones de reducci√≥n.' }
          ]
        },

        vectorize_avanzado: {
          type: 'question',
          text: '¬øEl array tiene m√°s de ~1 mill√≥n de elementos?',
          context: '@vectorize soporta target="parallel" para ejecutar en m√∫ltiples cores. El overhead de paralelismo solo vale la pena con arrays grandes.',
          options: [
            { id: 'result_vectorize_parallel', icon: 'üöÄ', label: 'S√≠, m√°s de 1M elementos', desc: 'El paralelismo amortiza su overhead con esta cantidad de datos.' },
            { id: 'result_vectorize', icon: 'üìê', label: 'No, o no estoy seguro', desc: 'Usar la versi√≥n serial es m√°s predecible.' }
          ]
        },

        vectorize_simple: {
          type: 'question',
          text: '¬øLa funci√≥n ser√° parte de una API p√∫blica o librer√≠a?',
          context: 'Si est√°s creando una librer√≠a que otros usar√°n, @vectorize te da una interfaz NumPy est√°ndar (broadcasting, .reduce(), etc.) que los usuarios esperan. Si es c√≥digo interno, @njit es m√°s simple.',
          options: [
            { id: 'result_vectorize', icon: 'üì¶', label: 'S√≠, es para una librer√≠a/API', desc: 'Quiero que los usuarios la usen como cualquier ufunc de NumPy.' },
            { id: 'result_njit_scalar_loop', icon: 'üîß', label: 'No, es c√≥digo interno', desc: 'Solo yo (o mi equipo) lo usa. Prefiero simplicidad.' }
          ]
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 3: SUB-ARRAY OPS
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        subarray_ops: {
          type: 'question',
          text: '¬øNecesitas broadcasting autom√°tico entre inputs de distintas dimensiones?',
          context: '@guvectorize te permite definir la operaci√≥n sobre una sub-dimensi√≥n (ej: "(n)->()" = recibe vector, produce escalar) y Numba la aplica autom√°ticamente sobre todas las dimensiones superiores. Si no necesitas broadcasting y solo vas a iterar filas con un loop, @njit es m√°s simple y puede ser m√°s r√°pido.',
          options: [
            { id: 'guvec_check', icon: 'üìê', label: 'S√≠, necesito broadcasting autom√°tico', desc: 'Quiero que funcione autom√°ticamente con arrays de distintas dimensiones.' },
            { id: 'subarray_manual', icon: 'üîÅ', label: 'No, voy a iterar las filas/ventanas con un loop', desc: 'Controlo la iteraci√≥n yo mismo. Prefiero explicitar el loop.' }
          ]
        },

        guvec_check: {
          type: 'question',
          text: '¬øLa funci√≥n ser√° llamada millones de veces con sub-arrays peque√±os?',
          context: '@guvectorize genera un ufunc NumPy con overhead de dispatch por cada invocaci√≥n (verificaci√≥n de tipos, resoluci√≥n de broadcasting, etc.). Para millones de llamadas con sub-arrays peque√±os (<100 elementos), este overhead se acumula. En ese caso, @njit con loop manual y output pre-allocado puede ser m√°s eficiente.',
          options: [
            { id: 'result_njit_manual_subarray', icon: '‚ö°', label: 'S√≠, millones de veces con datos peque√±os', desc: 'Priorizo rendimiento puro. Acepto escribir m√°s c√≥digo.' },
            { id: 'result_guvectorize', icon: 'üìä', label: 'No, pocas llamadas o sub-arrays grandes', desc: 'El overhead de dispatch es despreciable con sub-arrays grandes.' }
          ]
        },

        subarray_manual: {
          type: 'question',
          text: '¬øLas filas/ventanas son procesables en paralelo?',
          context: 'Si cada fila/ventana se puede procesar independientemente de las dem√°s, puedes usar prange sobre el loop externo para paralelizar. Esto es especialmente efectivo con matrices grandes (>10K filas).',
          options: [
            { id: 'result_njit_parallel_subarray', icon: 'üöÄ', label: 'S√≠, cada fila/ventana es independiente', desc: 'Ejemplo: normalizar cada fila de una matriz 10K√ó768.' },
            { id: 'result_njit_sequential_subarray', icon: 'üîó', label: 'No, hay dependencia entre filas/ventanas', desc: 'Ejemplo: el c√°lculo de la fila i depende del resultado de la fila i-1.' }
          ]
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 4: VECINDARIO / STENCIL
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        vecindario: {
          type: 'question',
          text: '¬øEl patr√≥n de vecindario es fijo (siempre los mismos offsets)?',
          context: '@stencil requiere que los √≠ndices de vecindario sean constantes enteras literales en tiempo de compilaci√≥n. Ejemplo v√°lido: a[-1,0] + a[1,0] (offsets fijos). Ejemplo inv√°lido: a[offset] donde offset es variable.',
          options: [
            { id: 'stencil_fijo', icon: 'üìç', label: 'S√≠, siempre los mismos vecinos', desc: 'Ejemplo: blur 3√ó3, Laplaciano, Game of Life ‚Äî siempre los mismos offsets.' },
            { id: 'result_njit_variable_stencil', icon: 'üîÄ', label: 'No, el patr√≥n cambia seg√∫n condiciones', desc: 'Los vecinos que accedo dependen de los datos o de un par√°metro variable.' }
          ]
        },

        stencil_fijo: {
          type: 'question',
          text: '¬øNecesitas ejecutar el stencil desde dentro de una funci√≥n paralela?',
          context: '@stencil por s√≠ solo ejecuta de forma serial. Para paralelizar, debes llamar al stencil desde una funci√≥n decorada con @njit(parallel=True). Numba puede auto-paralelizar la ejecuci√≥n del stencil cuando se llama desde contexto paralelo.',
          options: [
            { id: 'result_stencil_parallel', icon: 'üöÄ', label: 'S√≠, quiero m√°ximo rendimiento paralelo', desc: 'Tengo datos grandes y quiero usar todos los cores.' },
            { id: 'result_stencil', icon: 'üìç', label: 'No, serial est√° bien', desc: 'Los datos son moderados o solo necesito una ejecuci√≥n.' }
          ]
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 5: ESTADO MUTABLE
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        estado: {
          type: 'question',
          text: '¬øNecesitas mantener estado ENTRE llamadas a la funci√≥n?',
          context: 'La diferencia clave: si pasas todos los datos y recibes un resultado sin estado residual, usa @njit con funciones puras. Si necesitas un objeto que acumula estado (ej: filtro Kalman que actualiza su covarianza entre cada tick), necesitas @jitclass.',
          options: [
            { id: 'estado_persistente', icon: 'üèóÔ∏è', label: 'S√≠, el objeto vive entre llamadas', desc: 'Ejemplo: filtro.update(measurement) que modifica el estado interno del filtro.' },
            { id: 'result_njit_functional', icon: 'üì¶', label: 'No, solo proceso datos y devuelvo resultado', desc: 'Puedo pasar todo como argumentos y recibir el resultado sin estado residual.' }
          ]
        },

        estado_persistente: {
          type: 'question',
          text: '¬øEl objeto necesita ser usado DENTRO de otras funciones @njit?',
          context: '@jitclass crea una clase cuyas instancias pueden ser usadas dentro de funciones @njit con acceso a atributos tan r√°pido como leer un campo de una struct en C. Si solo necesitas el objeto en c√≥digo Python normal, una clase regular podr√≠a bastar.',
          options: [
            { id: 'result_jitclass', icon: '‚ö°', label: 'S√≠, se usa dentro de funciones @njit', desc: 'Ejemplo: paso el filtro como argumento a una funci√≥n @njit que lo actualiza.' },
            { id: 'jitclass_python', icon: 'üêç', label: 'No, solo se usa desde Python normal', desc: 'El objeto vive en Python y sus m√©todos no se llaman desde @njit.' }
          ]
        },

        jitclass_python: {
          type: 'question',
          text: '¬øLos m√©todos del objeto hacen c√°lculos num√©ricos pesados?',
          context: 'Si los m√©todos son computacionalmente intensivos, @jitclass los compila a c√≥digo m√°quina. Si son ligeros (getters, setters, l√≥gica simple), el overhead de definir tipos expl√≠citos para @jitclass puede no valer la pena.',
          options: [
            { id: 'result_jitclass', icon: 'üí™', label: 'S√≠, los m√©todos son computacionalmente pesados', desc: 'Cada llamada a un m√©todo hace miles de operaciones matem√°ticas.' },
            { id: 'result_njit_functional', icon: 'ü™∂', label: 'No, son m√©todos ligeros', desc: 'Mejor usar una clase Python normal y @njit solo para las funciones pesadas.' }
          ]
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 6: INTEROPERABILIDAD
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        interop: {
          type: 'question',
          text: '¬øQu√© tipo de interoperabilidad necesitas?',
          context: 'Numba ofrece varias herramientas para integrarse con c√≥digo externo, SciPy, y para escalar la compilaci√≥n a m√≥dulos enteros.',
          options: [
            { id: 'interop_callback', icon: 'üîó', label: 'Pasar una funci√≥n como callback a SciPy o C', desc: 'scipy.integrate.quad, scipy.optimize.minimize, o una librer√≠a C que acepta function pointers.' },
            { id: 'interop_overload', icon: 'üß©', label: 'Hacer una funci√≥n externa compatible con @njit', desc: 'Tengo una funci√≥n propia o de librer√≠a que quiero usar dentro de c√≥digo @njit.' },
            { id: 'interop_module', icon: 'üì¶', label: 'Jittear un m√≥dulo entero autom√°ticamente', desc: 'Tengo un m√≥dulo con muchas funciones y quiero compilar todas sin decorar una por una.' },
            { id: 'interop_objmode', icon: 'üåâ', label: 'Ejecutar c√≥digo Python normal dentro de @njit', desc: 'Necesito llamar una funci√≥n que no es compatible con Numba desde dentro de c√≥digo @njit.' }
          ]
        },

        interop_callback: {
          type: 'question',
          text: '¬øSciPy llama tu funci√≥n muchas veces internamente?',
          context: 'La diferencia entre pasar una funci√≥n Python normal y un @cfunc a SciPy es enorme. scipy.integrate.quad puede llamar tu funci√≥n miles de veces. Cada llamada sin @cfunc pasa por el int√©rprete Python. Con @cfunc, SciPy llama directamente al c√≥digo m√°quina sin overhead.',
          options: [
            { id: 'result_cfunc', icon: '‚ö°', label: 'S√≠, se llama miles/millones de veces', desc: 'Ejemplo: integrando que quad eval√∫a miles de veces.' },
            { id: 'result_cfunc_simple', icon: '1Ô∏è‚É£', label: 'No, se llama pocas veces', desc: 'Aun as√≠ @cfunc es beneficioso, pero la diferencia es menor.' }
          ]
        },

        interop_overload: {
          type: 'question',
          text: '¬øEs una funci√≥n que ya existe o una que est√°s creando?',
          context: '@overload permite definir una implementaci√≥n alternativa de una funci√≥n que Numba puede compilar. Numba la usa en tiempo de compilaci√≥n, y tu c√≥digo original sigue funcionando fuera de Numba.',
          options: [
            { id: 'result_overload', icon: 'üîß', label: 'Ya existe (m√≠a o de una librer√≠a)', desc: 'Quiero que mi_funcion() funcione tanto en Python normal como dentro de @njit.' },
            { id: 'result_register_jitable', icon: 'üÜï', label: 'La estoy creando desde cero', desc: 'Estoy escribiendo una utilidad nueva que ser√° usada dentro de @njit.' }
          ]
        },

        interop_module: {
          type: 'result',
          redirect: 'result_jit_module'
        },

        interop_objmode: {
          type: 'result',
          redirect: 'result_objmode'
        },

        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // RAMA 7: NO ESTOY SEGURO
        // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        no_se: {
          type: 'question',
          text: '¬øTu c√≥digo actual es principalmente...?',
          context: 'Vamos a identificar tu caso por eliminaci√≥n. La naturaleza del c√≥digo te dir√° qu√© camino seguir.',
          options: [
            { id: 'no_se_loops', icon: 'üîÅ', label: 'Loops con operaciones num√©ricas', desc: 'For/while con sumas, multiplicaciones, acceso a arrays.' },
            { id: 'no_se_numpy', icon: 'üìä', label: 'Operaciones vectorizadas de NumPy', desc: 'np.sum(), np.mean(), operaciones element-wise, broadcasting.' },
            { id: 'no_se_objetos', icon: 'üèóÔ∏è', label: 'Clases y objetos con m√©todos', desc: 'Orientado a objetos con estado mutable.' },
            { id: 'no_se_io', icon: 'üìÅ', label: 'Lectura/escritura de archivos o red', desc: 'I/O, bases de datos, HTTP, lectura de CSV.' }
          ]
        },

        no_se_loops: { type: 'result', redirect: 'loop_numerico' },
        no_se_numpy: {
          type: 'question',
          text: '¬øLas operaciones de NumPy son el cuello de botella?',
          context: 'NumPy ya ejecuta operaciones en C. Si tu cuello de botella son operaciones NumPy individuales, Numba puede ayudar al fusionar m√∫ltiples operaciones en un solo paso (evitando arrays temporales). Si el cuello de botella es la cantidad de llamadas a NumPy, Numba puede fusionar todo en un solo kernel.',
          options: [
            { id: 'result_njit_fuse', icon: 'üîó', label: 'S√≠, hago muchas operaciones encadenadas', desc: 'Ejemplo: result = np.sqrt(a**2 + b**2) * np.log(c + 1) ‚Äî cada paso crea un array temporal.' },
            { id: 'no_se_numpy_ok', icon: '‚úÖ', label: 'No realmente, NumPy ya es r√°pido para mi caso', desc: 'Mi cuello de botella est√° en otro lado (I/O, l√≥gica de negocio, etc.).' }
          ]
        },
        no_se_numpy_ok: {
          type: 'result',
          redirect: 'result_no_numba_needed'
        },
        no_se_objetos: { type: 'result', redirect: 'estado' },
        no_se_io: {
          type: 'result',
          redirect: 'result_no_numba_io'
        },


        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  RESULTADOS FINALES  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        // ‚îÄ‚îÄ @njit b√°sico ‚îÄ‚îÄ
        result_njit_basic: {
          type: 'result',
          decorator: '@njit',
          decoratorClass: 'njit',
          title: '@njit ‚Äî Compilaci√≥n JIT b√°sica',
          why: `<strong>@njit</strong> (alias de <strong>@jit</strong> desde Numba 0.59) compila tu funci√≥n a c√≥digo m√°quina nativo usando LLVM. Es el punto de partida universal: si tu c√≥digo tiene loops con operaciones num√©ricas y arrays NumPy, @njit te dar√° velocidades cercanas a C sin cambiar tu l√≥gica.`,
          code: `from numba import njit
import numpy as np

@njit
def mi_calculo(data):
    n = len(data)
    result = np.empty(n)
    for i in range(n):
        result[i] = np.sqrt(data[i]) * np.log(data[i] + 1)
    return result

# Primera llamada: compila (~0.5s) + ejecuta
arr = np.random.rand(1_000_000)
resultado = mi_calculo(arr)

# Llamadas siguientes: solo ejecuta (~100x m√°s r√°pido que Python)
resultado = mi_calculo(arr)`,
          options: [
            { code: 'cache=True', desc: 'Guarda compilaci√≥n en disco. Usa si el script se ejecuta repetidamente. No uses si cambias funciones importadas de otros m√≥dulos frecuentemente.' },
            { code: 'nogil=True', desc: 'Libera el GIL. Permite ejecutar en paralelo con threading.Thread manual. No confundir con parallel=True.' },
            { code: 'error_model="numpy"', desc: 'Usa la sem√°ntica de errores de NumPy (ej: divisi√≥n por cero retorna inf en vez de error).' },
            { code: 'boundscheck=True', desc: 'Activa verificaci√≥n de l√≠mites de array. √ötil para debugging, desact√≠valo en producci√≥n (est√° desactivado por defecto).' }
          ],
          pitfall: `<strong>Warm-up:</strong> La primera llamada siempre es lenta (compilaci√≥n JIT). Si tu aplicaci√≥n necesita respuesta inmediata, haz warm-up con datos dummy del tama√±o real al inicio:<br><code>mi_calculo(np.zeros(1))  # warm-up</code><br><br><strong>Tipos consistentes:</strong> Cada combinaci√≥n nueva de tipos de entrada genera una recompilaci√≥n. Si llamas con int64 y luego con float64, se compilan dos versiones distintas. Mant√©n los tipos consistentes o especif√≠calos con eager compilation.`,
          diagnostic: `<strong>Verificar que funcion√≥:</strong><br><code>mi_calculo.inspect_types()</code> ‚Äî muestra los tipos inferidos (aseg√∫rate de que no hay "pyobject").<br><code>mi_calculo.signatures</code> ‚Äî lista las especializaciones compiladas.`
        },

        // ‚îÄ‚îÄ @njit con cache ‚îÄ‚îÄ
        result_njit_cached: {
          type: 'result',
          decorator: '@njit(cache=True)',
          decoratorClass: 'njit',
          title: '@njit(cache=True) ‚Äî Compilaci√≥n cacheada en disco',
          why: `<strong>cache=True</strong> persiste la funci√≥n compilada en archivos <code>__pycache__</code>. La pr√≥xima vez que el script se ejecute, Numba carga desde disco en ~0.05s en vez de recompilar en ~0.5s. Ideal para scripts de producci√≥n, pipelines ETL, y servicios que reinician.`,
          code: `from numba import njit
import numpy as np

@njit(cache=True)
def ema(data, alpha):
    """Exponential Moving Average ‚Äî dependencia secuencial"""
    n = len(data)
    result = np.empty(n)
    result[0] = data[0]
    for i in range(1, n):
        result[i] = alpha * data[i] + (1 - alpha) * result[i-1]
    return result

# Primera ejecuci√≥n del script: compila y cachea
# Siguientes ejecuciones: carga desde disco (casi instant√°neo)
precios = np.random.rand(10_000_000)
ema_result = ema(precios, 0.1)`,
          options: [
            { code: 'cache=True', desc: '‚ö†Ô∏è Limitaciones conocidas: (1) No detecta cambios en funciones importadas de OTROS m√≥dulos. (2) Las variables globales se tratan como constantes ‚Äî el cache recuerda el valor al momento de compilar.' }
          ],
          pitfall: `<strong>Cache invalidation:</strong> Si cambias una funci√≥n auxiliar en otro archivo que tu funci√≥n @njit llama, el cache NO se invalida autom√°ticamente. Borra <code>__pycache__</code> manualmente o usa <code>NUMBA_CACHE_DIR</code> para controlar la ubicaci√≥n.<br><br><strong>Variables globales:</strong> Si tienes <code>N = 100</code> como global y lo cambias a <code>N = 200</code>, la versi√≥n cacheada seguir√° usando <code>N = 100</code> hasta que borres el cache.`,
          diagnostic: `<strong>Verificar que el cache funciona:</strong><br>Ejecuta el script dos veces y mide el tiempo de import/primera llamada. Deber√≠a ser ~10x m√°s r√°pido la segunda vez.`
        },

        // ‚îÄ‚îÄ @njit parallel+fastmath ‚îÄ‚îÄ
        result_njit_parallel_fast: {
          type: 'result',
          decorator: '@njit(parallel=True, fastmath=True)',
          decoratorClass: 'njit',
          title: '@njit(parallel=True, fastmath=True) ‚Äî M√°ximo rendimiento',
          why: `<strong>parallel=True</strong> auto-paraleliza loops con <code>prange</code> usando threads nativos (sin GIL). <strong>fastmath=True</strong> permite reordenar operaciones de punto flotante, habilitando vectorizaci√≥n SIMD. La combinaci√≥n puede dar <strong>10-50x</strong> sobre Python puro. Si adem√°s instalas Intel SVML (<code>conda install intel-cmplr-lib-rt</code>), las funciones trascendentales (sin, cos, exp, log) se ejecutan hasta <strong>4x m√°s r√°pido</strong>.`,
          code: `from numba import njit, prange
import numpy as np

@njit(parallel=True, fastmath=True, cache=True)
def procesar_masivo(data):
    n = len(data)
    result = np.empty(n)
    for i in prange(n):  # prange = range paralelo
        # Cada iteraci√≥n se ejecuta en un thread separado
        result[i] = np.sqrt(data[i]) * np.exp(-data[i]) + np.log(data[i] + 1)
    return result

# Reducci√≥n paralela (Numba la detecta autom√°ticamente)
@njit(parallel=True, fastmath=True)
def suma_paralela(data):
    n = len(data)
    total = 0.0
    for i in prange(n):
        total += np.sqrt(data[i])  # reducci√≥n auto-detectada
    return total

arr = np.random.rand(10_000_000)
resultado = procesar_masivo(arr)
total = suma_paralela(arr)`,
          options: [
            { code: 'parallel=True', desc: 'Usa prange en lugar de range para marcar loops como paralelos. Numba tambi√©n auto-paraleliza operaciones NumPy (np.sum, np.dot, etc.) sin prange.' },
            { code: 'fastmath=True', desc: 'Permite reassociation, no-signed-zeros, y otras relajaciones. Error ~4 ULP. Puedes ser selectivo: fastmath={"reassoc", "nsz"} para solo algunas optimizaciones.' },
            { code: 'prange', desc: '√ösalo SOLO para loops donde las iteraciones son independientes. Para reducciones (sum, min, max), Numba las detecta autom√°ticamente dentro de prange.' }
          ],
          pitfall: `<strong>prange NO es m√°gico:</strong> Si las iteraciones no son realmente independientes, obtendr√°s resultados incorrectos sin error. Numba NO verifica esto por ti.<br><br><strong>SVML importa mucho:</strong> Sin Intel SVML, funciones como np.sin, np.cos, np.exp son ~2-4x m√°s lentas. Instala <code>intel-cmplr-lib-rt</code> siempre que uses funciones trascendentales.<br><br><strong>Overhead de scheduling:</strong> Con menos de ~10K iteraciones, el overhead de distribuir trabajo entre threads puede superar el beneficio. Mide siempre.`,
          diagnostic: `<strong>Verificar paralelismo:</strong><br><code>procesar_masivo.parallel_diagnostics(level=4)</code> ‚Äî muestra qu√© loops fueron paralelizados, qu√© fusiones se hicieron, y qu√© operaciones SIMD se aplicaron.<br><br><strong>Verificar SVML:</strong><br><code>procesar_masivo.inspect_llvm()</code> ‚Äî busca cadenas como <code>__svml_sqrt</code> o <code>__svml_exp</code>. Si aparecen, SVML est√° activo.`
        },

        // ‚îÄ‚îÄ @njit parallel strict ‚îÄ‚îÄ
        result_njit_parallel_strict: {
          type: 'result',
          decorator: '@njit(parallel=True)',
          decoratorClass: 'njit',
          title: '@njit(parallel=True) ‚Äî Paralelo con precisi√≥n estricta',
          why: `<strong>parallel=True sin fastmath</strong> te da paralelizaci√≥n multi-core manteniendo la sem√°ntica exacta IEEE 754. Ideal para <strong>c√°lculos financieros</strong> donde necesitas reproducibilidad bit-a-bit y no puedes tolerar errores de reordenamiento.`,
          code: `from numba import njit, prange
import numpy as np

@njit(parallel=True, cache=True)  # SIN fastmath
def calcular_pnl(precios, posiciones, comisiones):
    """P&L preciso para un portafolio financiero"""
    n = len(precios)
    pnl = np.empty(n)
    for i in prange(n):
        pnl[i] = posiciones[i] * precios[i] - comisiones[i]
    return pnl

@njit(parallel=True)
def valor_en_riesgo(returns, confidence_level):
    """VaR param√©trico con precisi√≥n IEEE 754"""
    n = len(returns)
    mean = 0.0
    for i in prange(n):
        mean += returns[i]
    mean /= n

    variance = 0.0
    for i in prange(n):
        variance += (returns[i] - mean) ** 2
    variance /= (n - 1)

    # Aproximaci√≥n normal del VaR
    z_score = 2.326  # 99% confidence
    return mean - z_score * np.sqrt(variance)`,
          options: [
            { code: 'error_model="numpy"', desc: 'Usa sem√°ntica NumPy para errores: divisi√≥n por cero ‚Üí inf, no excepci√≥n. √ötil para datos financieros que pueden tener valores edge-case.' }
          ],
          pitfall: `<strong>Reproducibilidad:</strong> Incluso con parallel=True sin fastmath, el ORDEN de acumulaci√≥n en reducciones paralelas puede variar entre ejecuciones (cada thread acumula un subtotal diferente). Para reproducibilidad perfecta, usa sumas secuenciales con @njit simple (sin parallel).`,
          diagnostic: `<strong>Verificar precisi√≥n:</strong> Compara resultados de la versi√≥n paralela vs. secuencial con <code>np.allclose(result_par, result_seq, rtol=0, atol=0)</code>. Deber√≠an ser id√©nticos bit a bit (a diferencia de con fastmath).`
        },

        // ‚îÄ‚îÄ @njit batch (mover loop adentro) ‚îÄ‚îÄ
        result_njit_batch: {
          type: 'result',
          decorator: '@njit (con batch processing)',
          decoratorClass: 'njit',
          title: '@njit con batch processing ‚Äî Eliminar overhead de dispatch',
          why: `El overhead de llamar una funci√≥n Numba desde Python es ~1Œºs por llamada. Si llamas 100,000 veces desde un loop Python, pierdes ~0.1 segundos en overhead puro. La soluci√≥n: <strong>mover el loop externo DENTRO de la funci√≥n @njit</strong>. As√≠ tienes un solo dispatch Python‚ÜíNumba y todo el loop corre en c√≥digo m√°quina.`,
          code: `from numba import njit
import numpy as np

# ‚ùå MAL: loop externo en Python ‚Üí 100K dispatches
@njit
def calcular_ema_una(data, alpha):
    n = len(data)
    result = np.empty(n)
    result[0] = data[0]
    for i in range(1, n):
        result[i] = alpha * data[i] + (1 - alpha) * result[i-1]
    return result

# Esto genera 100,000 llamadas Python ‚Üí Numba
# for activo in range(100_000):
#     resultados[activo] = calcular_ema_una(datos[activo], 0.1)

# ‚úÖ BIEN: todo dentro de @njit ‚Üí 1 solo dispatch
@njit(parallel=True, cache=True)
def calcular_ema_batch(datos_2d, alpha):
    """Procesa TODOS los activos en un solo dispatch"""
    n_activos, n_periodos = datos_2d.shape
    resultados = np.empty_like(datos_2d)
    for a in prange(n_activos):          # paralelo: activos independientes
        resultados[a, 0] = datos_2d[a, 0]
        for t in range(1, n_periodos):   # secuencial: dependencia temporal
            resultados[a, t] = alpha * datos_2d[a, t] + (1 - alpha) * resultados[a, t-1]
    return resultados

from numba import prange
datos = np.random.rand(100_000, 5_000)  # 100K activos √ó 5K periodos
resultados = calcular_ema_batch(datos, 0.1)  # UN solo dispatch`,
          options: [
            { code: 'parallel=True + prange', desc: 'Paraleliza el loop EXTERNO (activos) mientras el interno (temporal) corre secuencial. Lo mejor de ambos mundos.' }
          ],
          pitfall: `<strong>Regla del 1Œºs:</strong> Si tu loop Python llama a una funci√≥n Numba m√°s de ~1,000 veces, SIEMPRE mueve el loop dentro de @njit. El costo es reorganizar tus datos en un array 2D/3D en vez de una lista de arrays separados.<br><br><strong>Memoria:</strong> Un array 2D contiguo (100K √ó 5K √ó 8 bytes = ~4GB) es m√°s cache-friendly que 100K arrays separados. Pero aseg√∫rate de tener suficiente RAM.`,
          diagnostic: `<strong>Medir el overhead:</strong><br><code>%timeit calcular_ema_una(datos_1d, 0.1)</code> vs <code>%timeit calcular_ema_batch(datos_2d, 0.1)</code> dividido por n_activos. La diferencia es el overhead de dispatch.`
        },

        // ‚îÄ‚îÄ @njit parallel inner loop ‚îÄ‚îÄ
        result_njit_parallel_inner: {
          type: 'result',
          decorator: '@njit(parallel=True) con loop mixto',
          decoratorClass: 'njit',
          title: '@njit(parallel=True) ‚Äî Loop externo paralelo, interno secuencial',
          why: `Cuando tienes un loop externo donde cada iteraci√≥n es independiente (ej: por activo, por sensor, por muestra), pero el loop interno tiene dependencia temporal (ej: EMA, filtro, acumulaci√≥n), la estrategia es <strong>prange en el externo y range en el interno</strong>.`,
          code: `from numba import njit, prange
import numpy as np

@njit(parallel=True, cache=True)
def procesar_multi_serie(series, parametros):
    """
    series: (n_series, n_puntos) ‚Äî cada fila es una serie temporal
    parametros: (n_series,) ‚Äî par√°metro diferente por serie
    """
    n_series, n_puntos = series.shape
    output = np.empty_like(series)

    for s in prange(n_series):      # ‚Üê PARALELO: series independientes
        alpha = parametros[s]
        output[s, 0] = series[s, 0]
        for t in range(1, n_puntos): # ‚Üê SECUENCIAL: dependencia temporal
            output[s, t] = alpha * series[s, t] + (1 - alpha) * output[s, t-1]

    return output

datos = np.random.rand(50_000, 10_000)
params = np.random.uniform(0.01, 0.5, 50_000)
resultado = procesar_multi_serie(datos, params)`,
          options: [
            { code: 'prange + range', desc: 'prange SOLO en el loop cuyas iteraciones son independientes. range normal para el loop con dependencias. Numba los distingue correctamente.' },
            { code: 'fastmath=True', desc: 'A√±adir si la precisi√≥n IEEE 754 no es cr√≠tica. El loop interno con acumulaci√≥n se beneficia del reordenamiento.' }
          ],
          pitfall: `<strong>Memory layout importa:</strong> Con series como filas (C-order), el loop interno accede a memoria contigua (√≥ptimo). Si tus series fueran columnas, deber√≠as transponer primero o usar Fortran-order.`,
          diagnostic: `<strong>parallel_diagnostics(level=4)</strong> te dir√° exactamente qu√© loop fue paralelizado. Deber√≠as ver el loop externo marcado como "parallel" y el interno como "serial".`
        },

        // ‚îÄ‚îÄ @njit escalar en loop ‚îÄ‚îÄ
        result_njit_scalar_loop: {
          type: 'result',
          decorator: '@njit',
          decoratorClass: 'njit',
          title: '@njit con loop manual ‚Äî M√°s simple que @vectorize',
          why: `Si tu funci√≥n escalar‚Üíescalar solo necesita aplicarse a un array 1D y no necesitas broadcasting ni .reduce()/.accumulate(), un <strong>loop manual con @njit</strong> es m√°s simple, sin el overhead conceptual ni el dispatch de ufunc.`,
          code: `from numba import njit
import numpy as np

@njit(cache=True)
def aplicar_activacion(data, result):
    """Funci√≥n de activaci√≥n custom aplicada element-wise"""
    for i in range(len(data)):
        x = data[i]
        if x > 0:
            result[i] = x
        else:
            result[i] = 0.01 * (np.exp(x) - 1)  # ELU-like

# Pre-allocar output (evita crear array temporal)
arr = np.random.randn(1_000_000)
out = np.empty_like(arr)
aplicar_activacion(arr, out)`,
          options: [
            { code: 'parallel=True + prange', desc: 'Si el array es grande (>100K) y cada operaci√≥n es independiente, a√±ade parallel=True y cambia range‚Üíprange para paralelizar trivialmente.' }
          ],
          pitfall: `<strong>Pre-allocar output:</strong> Pasar el array de output como argumento (en vez de crearlo dentro) te permite reutilizar el buffer entre llamadas. Esto reduce presi√≥n en el garbage collector y mejora cache locality.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @njit fusionar operaciones ‚îÄ‚îÄ
        result_njit_fuse: {
          type: 'result',
          decorator: '@njit',
          decoratorClass: 'njit',
          title: '@njit ‚Äî Fusionar operaciones NumPy (eliminar temporales)',
          why: `Cuando encadenas operaciones NumPy como <code>result = np.sqrt(a**2 + b**2) * np.log(c + 1)</code>, NumPy crea <strong>arrays temporales</strong> para cada subexpresi√≥n. Con 10M de elementos, eso es 5 arrays √ó 80MB = 400MB de memoria temporal que adem√°s destruye el cache de CPU. <strong>@njit fusiona todo en un solo loop</strong>: una lectura, un c√°lculo, una escritura.`,
          code: `import numpy as np
from numba import njit, prange

# ‚ùå NumPy: 5 arrays temporales de 80MB cada uno
def numpy_chain(a, b, c):
    return np.sqrt(a**2 + b**2) * np.log(c + 1)

# ‚úÖ Numba: cero temporales, todo en un solo pass
@njit(parallel=True, fastmath=True, cache=True)
def fused_chain(a, b, c):
    n = len(a)
    result = np.empty(n)
    for i in prange(n):
        result[i] = np.sqrt(a[i]**2 + b[i]**2) * np.log(c[i] + 1)
    return result

n = 10_000_000
a, b, c = np.random.rand(n), np.random.rand(n), np.random.rand(n)
# fused_chain es ~3-10x m√°s r√°pido que numpy_chain
# (el factor depende de cu√°nto pesa el overhead de memoria)`,
          options: [
            { code: 'parallel=True', desc: 'El loop fusionado es trivialmente paralelizable ‚Äî cada i es independiente.' },
            { code: 'fastmath=True', desc: 'Habilita SIMD vectorizaci√≥n del loop fusionado. Especialmente efectivo con SVML.' }
          ],
          pitfall: `<strong>¬øCu√°ndo vale la pena?:</strong> Si tus arrays son peque√±os (<10K elementos), el overhead de NumPy por temporales es insignificante y @njit no mejora mucho. La fusi√≥n brilla con datos que NO caben en L3 cache (>~10MB).`,
          diagnostic: `<strong>Medir memoria:</strong> Usa <code>tracemalloc</code> para comparar el peak memory de la versi√≥n NumPy vs Numba. La diferencia deber√≠a ser proporcional al n√∫mero de operaciones encadenadas.`
        },

        // ‚îÄ‚îÄ @njit para funcional sin estado ‚îÄ‚îÄ
        result_njit_functional: {
          type: 'result',
          decorator: '@njit (funciones puras)',
          decoratorClass: 'njit',
          title: '@njit con funciones puras ‚Äî Sin necesidad de @jitclass',
          why: `Si puedes expresar tu l√≥gica como <strong>funciones puras</strong> que reciben datos y devuelven resultados sin mantener estado, @njit es m√°s simple, m√°s r√°pido de compilar, y m√°s f√°cil de testear que @jitclass. Pasa el "estado" como arrays expl√≠citos.`,
          code: `from numba import njit
import numpy as np

# En lugar de una clase con estado...
# class KalmanFilter:
#     def __init__(self): self.x = 0; self.P = 1
#     def update(self, z): ...

# ...usa funciones puras con estado expl√≠cito:
@njit(cache=True)
def kalman_update(x, P, z, R, Q):
    """Un paso de Kalman ‚Äî sin estado mutable"""
    # Predicci√≥n
    x_pred = x
    P_pred = P + Q
    # Correcci√≥n
    K = P_pred / (P_pred + R)
    x_new = x_pred + K * (z - x_pred)
    P_new = (1 - K) * P_pred
    return x_new, P_new

@njit(cache=True)
def kalman_filter_full(measurements, R, Q):
    """Filtra toda la serie ‚Äî loop secuencial"""
    n = len(measurements)
    filtered = np.empty(n)
    x, P = 0.0, 1.0
    for i in range(n):
        x, P = kalman_update(x, P, measurements[i], R, Q)
        filtered[i] = x
    return filtered`,
          options: [],
          pitfall: `<strong>¬øCu√°ndo S√ç necesitas @jitclass?:</strong> Cuando el estado es complejo (matrices, arrays con dimensiones din√°micas), cuando necesitas pasar el objeto a otras funciones @njit como argumento, o cuando la API orientada a objetos es fundamental para tu dise√±o.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @njit subarray paralelo ‚îÄ‚îÄ
        result_njit_parallel_subarray: {
          type: 'result',
          decorator: '@njit(parallel=True)',
          decoratorClass: 'njit',
          title: '@njit(parallel=True) + prange ‚Äî Procesar sub-arrays en paralelo',
          why: `Para procesar filas, ventanas, o secciones de un array grande donde cada secci√≥n es independiente, un <strong>loop con prange sobre las filas</strong> es simple y eficiente. Es m√°s directo que @guvectorize cuando no necesitas broadcasting autom√°tico.`,
          code: `from numba import njit, prange
import numpy as np

@njit(parallel=True, cache=True)
def normalizar_filas(matrix):
    """Normalizar cada fila a norma unitaria"""
    n_filas, n_cols = matrix.shape
    result = np.empty_like(matrix)
    for i in prange(n_filas):          # cada fila es independiente
        norma = 0.0
        for j in range(n_cols):
            norma += matrix[i, j] ** 2
        norma = np.sqrt(norma)
        if norma > 0:
            for j in range(n_cols):
                result[i, j] = matrix[i, j] / norma
        else:
            for j in range(n_cols):
                result[i, j] = 0.0
    return result

data = np.random.rand(100_000, 768)
normalizado = normalizar_filas(data)`,
          options: [
            { code: 'fastmath=True', desc: 'Seguro para normalizaci√≥n y operaciones similares donde la precisi√≥n exacta no es cr√≠tica.' }
          ],
          pitfall: `<strong>vs @guvectorize:</strong> Para este patr√≥n espec√≠fico, @njit con prange es generalmente m√°s r√°pido porque evita el overhead de dispatch de ufunc. @guvectorize brilla cuando necesitas la interfaz de ufunc (broadcasting, .reduce(), etc.).`,
          diagnostic: `<code>parallel_diagnostics(level=4)</code> deber√≠a mostrar el loop externo como paralelo. Si no lo paraleliza, aseg√∫rate de que NO hay escritura compartida entre iteraciones.`
        },

        // ‚îÄ‚îÄ @njit subarray secuencial ‚îÄ‚îÄ
        result_njit_sequential_subarray: {
          type: 'result',
          decorator: '@njit',
          decoratorClass: 'njit',
          title: '@njit ‚Äî Procesar sub-arrays con dependencia',
          why: `Cuando hay dependencia entre filas/ventanas (ej: el c√°lculo de la fila i depende del resultado de la fila i-1), no puedes paralelizar el loop externo. Pero @njit a√∫n te da el beneficio de compilar a c√≥digo m√°quina.`,
          code: `from numba import njit
import numpy as np

@njit(cache=True)
def propagacion_secuencial(matrix, decay):
    """Cada fila depende de la anterior"""
    n_filas, n_cols = matrix.shape
    result = np.empty_like(matrix)
    # Primera fila: copia directa
    for j in range(n_cols):
        result[0, j] = matrix[0, j]
    # Resto: cada fila incorpora informaci√≥n de la anterior
    for i in range(1, n_filas):
        for j in range(n_cols):
            result[i, j] = matrix[i, j] + decay * result[i-1, j]
    return result`,
          options: [
            { code: 'fastmath=True', desc: 'A√±ade si la precisi√≥n exacta no es cr√≠tica. El loop interno (sobre columnas) puede vectorizarse con SIMD.' }
          ],
          pitfall: `<strong>El loop sobre columnas S√ç es paralelizable:</strong> Aunque las filas son secuenciales, el loop interno sobre columnas podr√≠a paralelizarse. Numba con parallel=True puede detectar esto autom√°ticamente.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @njit para sub-arrays con rendimiento cr√≠tico ‚îÄ‚îÄ
        result_njit_manual_subarray: {
          type: 'result',
          decorator: '@njit (loop manual + pre-allocated output)',
          decoratorClass: 'njit',
          title: '@njit ‚Äî Loop manual optimizado vs @guvectorize',
          why: `Cuando procesas <strong>millones de sub-arrays peque√±os</strong>, el overhead de dispatch de ufunc de @guvectorize se acumula. Un loop manual con @njit + output pre-allocado elimina ese overhead. Es m√°s c√≥digo, pero puede ser significativamente m√°s r√°pido para este patr√≥n espec√≠fico.`,
          code: `from numba import njit, prange
import numpy as np

# Pre-allocar output una sola vez
@njit(parallel=True, cache=True)
def norma_filas_manual(matrix, output):
    """Calcula norma de cada fila ‚Äî sin overhead de ufunc dispatch"""
    n_filas, n_cols = matrix.shape
    for i in prange(n_filas):
        acc = 0.0
        for j in range(n_cols):
            acc += matrix[i, j] ** 2
        output[i] = np.sqrt(acc)

# Uso: pre-allocar y reutilizar
data = np.random.rand(1_000_000, 64)
output = np.empty(data.shape[0])
norma_filas_manual(data, output)  # sin allocaci√≥n interna`,
          options: [
            { code: 'Pre-allocar output', desc: 'Pasar el array de output como argumento evita crear un nuevo array en cada llamada. Crucial cuando la funci√≥n se llama repetidamente en un pipeline.' }
          ],
          pitfall: `<strong>Trade-off claridad vs rendimiento:</strong> @guvectorize con "(n)->()" es m√°s expresivo y autodocumentado. El loop manual es m√°s r√°pido pero menos legible. Usa benchmarks con TUS datos antes de decidir.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @njit stencil variable ‚îÄ‚îÄ
        result_njit_variable_stencil: {
          type: 'result',
          decorator: '@njit',
          decoratorClass: 'njit',
          title: '@njit ‚Äî Stencil con vecindario variable',
          why: `Cuando el patr√≥n de vecindario cambia seg√∫n condiciones (ej: adaptive smoothing, vecindario variable seg√∫n la densidad local), <strong>@stencil no aplica</strong> porque requiere offsets literales constantes. Usa @njit con loops manuales y boundary checking expl√≠cito.`,
          code: `from numba import njit, prange
import numpy as np

@njit(parallel=True, cache=True)
def adaptive_smooth(data, radii):
    """Suavizado con radio variable por posici√≥n"""
    n = len(data)
    result = np.empty(n)
    for i in prange(n):
        r = radii[i]  # radio diferente por posici√≥n
        total = 0.0
        count = 0
        for j in range(max(0, i - r), min(n, i + r + 1)):
            total += data[j]
            count += 1
        result[i] = total / count if count > 0 else 0.0
    return result`,
          options: [],
          pitfall: `<strong>Cuidado con prange:</strong> Si el radio variable hace que iteraciones adyacentes lean regiones solapadas, prange sigue siendo seguro para lecturas concurrentes. Solo falla si hay ESCRITURAS solapadas.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @vectorize ‚îÄ‚îÄ
        result_vectorize: {
          type: 'result',
          decorator: '@vectorize',
          decoratorClass: 'vectorize',
          title: '@vectorize ‚Äî Crear ufuncs NumPy',
          why: `<strong>@vectorize</strong> convierte una funci√≥n escalar‚Üíescalar en un <strong>ufunc NumPy real</strong>. Obtienes gratis: broadcasting autom√°tico (funciona con cualquier shape), <code>.reduce()</code>, <code>.accumulate()</code>, <code>.outer()</code>, y la integraci√≥n completa con el ecosistema NumPy. Ideal cuando quieres una interfaz limpia y est√°ndar.`,
          code: `from numba import vectorize, float64, int64

# Modo eager: especificas tipos ‚Üí compila inmediatamente
@vectorize([float64(float64)])
def impuesto_progresivo(ingreso):
    """Calcula impuesto con tramos progresivos"""
    if ingreso <= 12000:
        return ingreso * 0.0
    elif ingreso <= 40000:
        return (ingreso - 12000) * 0.15
    elif ingreso <= 80000:
        return 4200 + (ingreso - 40000) * 0.25
    else:
        return 14200 + (ingreso - 80000) * 0.35

# Modo lazy: sin tipos ‚Üí compila en primera llamada
@vectorize
def activacion_custom(x):
    if x > 0:
        return x
    else:
        return 0.01 * x  # Leaky ReLU

import numpy as np
ingresos = np.array([10000, 30000, 60000, 100000])
impuestos = impuesto_progresivo(ingresos)

# Broadcasting gratis:
matrix = np.random.rand(100, 200)
result = activacion_custom(matrix)  # funciona sin cambios

# .reduce() gratis:
# activacion_custom.reduce(array)  # reduce a escalar
# activacion_custom.accumulate(array)  # scan acumulativo`,
          options: [
            { code: 'Eager vs Lazy', desc: 'Eager: @vectorize([float64(float64)]) ‚Äî compila al decorar, m√°s control, permite m√∫ltiples firmas. Lazy: @vectorize ‚Äî compila en primera llamada, m√°s flexible pero genera DUFunc.' },
            { code: 'M√∫ltiples firmas', desc: '@vectorize([int32(int32), float64(float64)]) ‚Äî soporta m√∫ltiples tipos de entrada. Numba elige la firma correcta autom√°ticamente.' }
          ],
          pitfall: `<strong>Solo escalares:</strong> @vectorize SOLO ve un elemento a la vez. No puedes acceder a x[i-1], no puedes calcular la media, no puedes ver el array completo. Si necesitas eso, usa @guvectorize o @njit.<br><br><strong>Modo lazy (DUFunc):</strong> Sin firmas expl√≠citas, genera un DUFunc que recompila con cada nuevo tipo. Peligroso si le pasas tipos inconsistentes accidentalmente.`,
          diagnostic: `<strong>Ver firmas compiladas:</strong><br><code>impuesto_progresivo.types</code> ‚Äî lista todas las especializaciones compiladas.`
        },

        // ‚îÄ‚îÄ @vectorize parallel ‚îÄ‚îÄ
        result_vectorize_parallel: {
          type: 'result',
          decorator: '@vectorize(target="parallel")',
          decoratorClass: 'vectorize',
          title: '@vectorize(target="parallel") ‚Äî Ufunc multi-core',
          why: `Con <code>target="parallel"</code>, el ufunc ejecuta en m√∫ltiples cores autom√°ticamente. Solo vale la pena con <strong>arrays grandes (>1M elementos)</strong> porque el overhead de distribuci√≥n de trabajo entre threads es significativo.`,
          code: `from numba import vectorize, float64

@vectorize([float64(float64)], target='parallel')
def sigmoid_parallel(x):
    return 1.0 / (1.0 + np.exp(-x))

import numpy as np
# Efectivo con arrays grandes:
huge_array = np.random.randn(10_000_000)
result = sigmoid_parallel(huge_array)  # usa todos los cores`,
          options: [
            { code: 'target="parallel"', desc: 'Targets disponibles: "cpu" (serial, default), "parallel" (multi-core). Para CUDA GPU se usa numba.cuda en lugar de @vectorize.' },
            { code: 'Requiere firmas expl√≠citas', desc: 'target="parallel" REQUIERE modo eager (firmas expl√≠citas). No funciona con modo lazy.' }
          ],
          pitfall: `<strong>No asumas que paralelo = m√°s r√°pido:</strong> Con arrays peque√±os (<100K), la versi√≥n serial "cpu" es m√°s r√°pida. Mide siempre. El target="parallel" de @vectorize usa un mecanismo diferente al parallel=True de @njit.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @guvectorize ‚îÄ‚îÄ
        result_guvectorize: {
          type: 'result',
          decorator: '@guvectorize',
          decoratorClass: 'guvectorize',
          title: '@guvectorize ‚Äî Ufuncs generalizados',
          why: `<strong>@guvectorize</strong> extiende @vectorize a operaciones sobre <strong>sub-arrays</strong>. Defines la firma dimensional (ej: "(n)->()" = recibe vector de n, produce escalar) y Numba aplica autom√°ticamente la operaci√≥n sobre las dimensiones superiores con broadcasting. El resultado es un <strong>gufunc</strong> compatible con el ecosistema NumPy.`,
          code: `from numba import guvectorize, float64

# "(n)->()" significa: recibe array de n elementos, produce un escalar
@guvectorize([(float64[:], float64[:])], '(n)->()')
def norma_l2(vec, result):
    """Norma L2 de un vector"""
    acc = 0.0
    for i in range(vec.shape[0]):
        acc += vec[i] ** 2
    result[0] = np.sqrt(acc)  # output via argumento, no return

# "(n),(n)->(n)" = dos inputs de n, un output de n
@guvectorize([(float64[:], float64[:], float64[:])], '(n),(n)->(n)')
def weighted_add(a, b, result):
    for i in range(a.shape[0]):
        result[i] = 0.7 * a[i] + 0.3 * b[i]

import numpy as np
# Broadcasting autom√°tico:
matrix = np.random.rand(10_000, 768)
normas = norma_l2(matrix)  # (10_000,) ‚Äî aplic√≥ a cada fila

# Funciona con cualquier dimensi√≥n superior:
tensor = np.random.rand(100, 50, 768)
normas_3d = norma_l2(tensor)  # (100, 50) ‚Äî broadcasting gratis`,
          options: [
            { code: 'Firmas dimensionales', desc: '"(n)->()" = vector‚Üíescalar. "(m,n)->(m)" = matriz‚Üívector. "(n),(n)->(n)" = dos vectores‚Üívector. El output siempre es un argumento, no un return.' },
            { code: 'target="parallel"', desc: 'Similar a @vectorize, puedes usar target="parallel" para ejecuci√≥n multi-core. Requiere firmas expl√≠citas.' },
            { code: 'Retornar escalar', desc: 'Para retornar un escalar usa firma "(n)->()" y escribe en result[0]. No uses return.' }
          ],
          pitfall: `<strong>Output via argumento:</strong> @guvectorize NO usa return. El resultado se escribe en el √∫ltimo argumento (que NumPy pre-alloca). Esto confunde a muchos principiantes.<br><br><strong>Overhead de dispatch:</strong> Cada invocaci√≥n del gufunc tiene overhead de tipo-checking y broadcasting resolution. Para millones de llamadas con sub-arrays peque√±os, considera @njit con loop manual.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @stencil ‚îÄ‚îÄ
        result_stencil: {
          type: 'result',
          decorator: '@stencil',
          decoratorClass: 'stencil',
          title: '@stencil ‚Äî Operaciones sobre vecindarios fijos',
          why: `<strong>@stencil</strong> es az√∫car sint√°ctica para operaciones donde cada elemento del output depende de sus vecinos en un patr√≥n fijo. Escribes <code>a[-1, 0] + a[1, 0]</code> para referirte a "un paso arriba" y "un paso abajo", y Numba genera el loop, maneja los bordes, y puede paralelizar autom√°ticamente.`,
          code: `from numba import stencil
import numpy as np

@stencil
def blur_3x3(a):
    """Promedio de los 8 vecinos + el centro"""
    return (a[-1, -1] + a[-1, 0] + a[-1, 1] +
            a[ 0, -1] + a[ 0, 0] + a[ 0, 1] +
            a[ 1, -1] + a[ 1, 0] + a[ 1, 1]) / 9.0

@stencil
def laplaciano_2d(a):
    """Operador Laplaciano discreto"""
    return a[-1, 0] + a[1, 0] + a[0, -1] + a[0, 1] - 4 * a[0, 0]

@stencil
def game_of_life_step(grid):
    """Un paso del Game of Life de Conway"""
    vecinos = (grid[-1, -1] + grid[-1, 0] + grid[-1, 1] +
               grid[ 0, -1]              + grid[ 0, 1] +
               grid[ 1, -1] + grid[ 1, 0] + grid[ 1, 1])
    if grid[0, 0] == 1:  # c√©lula viva
        if vecinos < 2 or vecinos > 3:
            return 0  # muere
        return 1      # sobrevive
    else:               # c√©lula muerta
        if vecinos == 3:
            return 1  # nace
        return 0      # sigue muerta

imagen = np.random.rand(1000, 1000)
borrosa = blur_3x3(imagen)  # bordes se ponen a 0 por defecto`,
          options: [
            { code: 'Bordes', desc: 'Por defecto, los bordes del output se ponen a 0 (no hay vecinos fuera del array). Puedes usar func_or_mode="constant" con cval para cambiar el valor del borde.' },
            { code: 'Con par√°metros extra', desc: '@stencil puede recibir argumentos escalares adicionales: @stencil def kernel(a, factor): return factor * (a[-1,0] + a[1,0])' }
          ],
          pitfall: `<strong>Solo offsets literales:</strong> Los √≠ndices deben ser constantes enteras. <code>a[variable, 0]</code> NO funciona. Para vecindarios din√°micos, usa @njit con loop manual.<br><br><strong>Output type:</strong> Numba infiere el tipo del output. En el ejemplo del blur, aunque la entrada es int64, el output ser√° float64 porque la divisi√≥n produce un float.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @stencil paralelo ‚îÄ‚îÄ
        result_stencil_parallel: {
          type: 'result',
          decorator: '@stencil + @njit(parallel=True)',
          decoratorClass: 'stencil',
          title: '@stencil con paralelismo ‚Äî Llamar desde funci√≥n paralela',
          why: `@stencil por s√≠ solo ejecuta de forma serial. Para paralelizar, <strong>llama al stencil desde dentro de una funci√≥n @njit(parallel=True)</strong>. Numba detecta que el stencil puede paralelizarse y distribuye el trabajo entre cores autom√°ticamente.`,
          code: `from numba import stencil, njit
import numpy as np

@stencil
def difusion_kernel(T, coef):
    """Paso de difusi√≥n de calor 2D"""
    return T[0, 0] + coef * (
        T[-1, 0] + T[1, 0] + T[0, -1] + T[0, 1] - 4 * T[0, 0]
    )

@njit(parallel=True)  # ‚Üê esto paraleliza la ejecuci√≥n del stencil
def simular_calor(T_initial, coef, n_pasos):
    T = T_initial.copy()
    for step in range(n_pasos):
        T = difusion_kernel(T, coef)  # paralelizado autom√°ticamente
    return T

# Simulaci√≥n
grid = np.zeros((500, 500))
grid[240:260, 240:260] = 100.0  # fuente de calor
resultado = simular_calor(grid, 0.25, 1000)`,
          options: [],
          pitfall: `<strong>El loop temporal NO se paraleliza:</strong> Solo la ejecuci√≥n interna del stencil (recorrer la grilla) se paraleliza. El loop <code>for step in range(n_pasos)</code> sigue siendo secuencial porque cada paso depende del anterior.`,
          diagnostic: `<code>simular_calor.parallel_diagnostics(level=4)</code> deber√≠a mostrar la ejecuci√≥n del stencil como paralela.`
        },

        // ‚îÄ‚îÄ @jitclass ‚îÄ‚îÄ
        result_jitclass: {
          type: 'result',
          decorator: '@jitclass',
          decoratorClass: 'jitclass',
          title: '@jitclass ‚Äî Clases compiladas con estado',
          why: `<strong>@jitclass</strong> compila una clase completa: atributos tipados (acceso tan r√°pido como struct en C) y m√©todos compilados. Ideal cuando necesitas un <strong>objeto con estado mutable</strong> que se usa dentro de funciones @njit. Debes declarar los tipos de TODOS los atributos.`,
          code: `from numba import njit, int64, float64
from numba.experimental import jitclass

# Definir tipos de todos los atributos
spec = [
    ('n', int64),
    ('mean', float64),
    ('M2', float64),
]

@jitclass(spec)
class WelfordAccumulator:
    """Algoritmo de Welford para media y varianza online"""
    def __init__(self):
        self.n = 0
        self.mean = 0.0
        self.M2 = 0.0

    def update(self, x):
        self.n += 1
        delta = x - self.mean
        self.mean += delta / self.n
        delta2 = x - self.mean
        self.M2 += delta * delta2

    def variance(self):
        if self.n < 2:
            return 0.0
        return self.M2 / (self.n - 1)

    def std(self):
        return np.sqrt(self.variance())

# Usar desde Python
import numpy as np
acc = WelfordAccumulator()
for x in np.random.randn(1_000_000):
    acc.update(x)
print(f"Media: {acc.mean:.6f}, Std: {acc.std():.6f}")

# Usar DENTRO de @njit
@njit
def procesar_stream(data):
    acc = WelfordAccumulator()
    for i in range(len(data)):
        acc.update(data[i])
    return acc.mean, acc.std()`,
          options: [
            { code: 'spec', desc: 'Los tipos se declaran como lista de tuplas: [("nombre", tipo)]. Tipos comunes: int64, float64, float64[:], float64[:,:], boolean.' },
            { code: 'numba.typed containers', desc: 'Para listas y diccionarios dentro de @jitclass, usa numba.typed.List y numba.typed.Dict en vez de list/dict de Python.' }
          ],
          pitfall: `<strong>Importar de experimental:</strong> <code>from numba.experimental import jitclass</code> ‚Äî sigue siendo experimental. La API puede cambiar entre versiones.<br><br><strong>Todos los atributos deben ser tipados:</strong> No puedes tener atributos opcionales o de tipo variable. Si un atributo puede ser None, necesitas manejarlo manualmente (ej: usar un valor sentinela como -1.0 o NaN).<br><br><strong>No soporta herencia:</strong> @jitclass no soporta herencia de clases. Usa composici√≥n en su lugar.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @cfunc ‚îÄ‚îÄ
        result_cfunc: {
          type: 'result',
          decorator: '@cfunc',
          decoratorClass: 'cfunc',
          title: '@cfunc ‚Äî Callbacks C para SciPy y librer√≠as externas',
          why: `<strong>@cfunc</strong> produce un function pointer compatible con C que SciPy o ctypes pueden llamar directamente <strong>sin pasar por el int√©rprete Python</strong>. Cuando SciPy llama tu funci√≥n miles de veces internamente (ej: quad para integraci√≥n, minimize para optimizaci√≥n), la diferencia es dram√°tica: ~18x m√°s r√°pido seg√∫n benchmarks de la documentaci√≥n oficial.`,
          code: `import numpy as np
from numba import cfunc
from scipy import integrate

# Definir el integrando como @cfunc
@cfunc("float64(float64)")
def integrando(t):
    return np.exp(-t) / t**2

# Pasar el .ctypes al integrador de SciPy
result, error = integrate.quad(integrando.ctypes, 1, np.inf)
# ~18x m√°s r√°pido que pasar una funci√≥n Python normal

# Ejemplo con optimize
from scipy.optimize import minimize_scalar

@cfunc("float64(float64)")
def funcion_objetivo(x):
    return (x - 3.14159) ** 2 + np.sin(x * 10) * 0.1

# minimize_scalar llama la funci√≥n cientos de veces
result = minimize_scalar(funcion_objetivo.ctypes, bounds=(0, 10), method='bounded')`,
          options: [
            { code: 'Firma string', desc: '@cfunc("float64(float64, float64)") ‚Äî la firma usa strings de tipos Numba. El formato es "retorno(arg1, arg2, ...)".' },
            { code: '.ctypes', desc: 'Accede al callback C con .ctypes. P√°salo donde SciPy o ctypes esperen un function pointer.' },
            { code: 'Punteros y arrays', desc: 'Para callbacks que reciben punteros a datos (ej: doble *input, int n), usa numba.carray() para crear vistas de arrays desde punteros C.' }
          ],
          pitfall: `<strong>Sin objetos Python:</strong> @cfunc compila en nopython mode. No puedes usar listas, diccionarios, o llamar funciones Python arbitrarias dentro del callback.<br><br><strong>Firma fija:</strong> A diferencia de @njit, @cfunc requiere exactamente UNA firma. No hay inferencia de tipos ni m√∫ltiples especializaciones.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @cfunc simple ‚îÄ‚îÄ
        result_cfunc_simple: {
          type: 'result',
          decorator: '@cfunc',
          decoratorClass: 'cfunc',
          title: '@cfunc ‚Äî Callback C (pocas llamadas)',
          why: `Incluso con pocas llamadas, @cfunc elimina el overhead del int√©rprete Python en cada invocaci√≥n. La compilaci√≥n es un costo √∫nico. √ösalo siempre que pases funciones a SciPy.`,
          code: `from numba import cfunc
from scipy.optimize import brentq

@cfunc("float64(float64)")
def ecuacion(x):
    return x**3 - 2*x - 5  # buscar ra√≠z

raiz = brentq(ecuacion.ctypes, 2, 3)`,
          options: [],
          pitfall: `Aunque la diferencia de rendimiento sea menor con pocas llamadas, el costo de @cfunc es despreciable (compilaci√≥n una sola vez). No hay raz√≥n para NO usarlo.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ @overload ‚îÄ‚îÄ
        result_overload: {
          type: 'result',
          decorator: '@overload',
          decoratorClass: 'overload',
          title: '@overload ‚Äî Hacer funciones compatibles con nopython mode',
          why: `<strong>@overload</strong> te permite definir una implementaci√≥n alternativa de una funci√≥n que Numba usa en tiempo de compilaci√≥n. La funci√≥n original sigue funcionando en Python normal. Es el puente entre el mundo Python y el mundo Numba.`,
          code: `from numba import njit
from numba.extending import overload
import numpy as np

# Tu funci√≥n que NO funciona dentro de @njit
def mi_clip(x, lo, hi):
    """Clip personalizado con l√≥gica custom"""
    return min(max(x, lo), hi)

# Implementaci√≥n para Numba
@overload(mi_clip)
def mi_clip_overload(x, lo, hi):
    # Esta funci√≥n retorna la IMPLEMENTACI√ìN, no el resultado
    def impl(x, lo, hi):
        if x < lo:
            return lo
        elif x > hi:
            return hi
        return x
    return impl

# Ahora funciona en ambos mundos:
print(mi_clip(5.0, 0.0, 3.0))  # Python normal: 3.0

@njit
def procesar(data, lo, hi):
    result = np.empty_like(data)
    for i in range(len(data)):
        result[i] = mi_clip(data[i], lo, hi)  # usa la versi√≥n overloaded
    return result

resultado = procesar(np.array([1.0, 5.0, -2.0]), 0.0, 3.0)`,
          options: [
            { code: 'Estructura', desc: '@overload(funcion_original) decora una funci√≥n que RETORNA la implementaci√≥n (no el resultado directo). El patr√≥n es: def overload_fn(args): def impl(args): ... return impl' },
            { code: 'Type dispatch', desc: 'Dentro de @overload puedes inspeccionar los tipos de los argumentos para retornar diferentes implementaciones seg√∫n el tipo.' }
          ],
          pitfall: `<strong>Retorna la implementaci√≥n, no el resultado:</strong> El error m√°s com√∫n es escribir l√≥gica directa dentro de @overload en vez de retornar una funci√≥n interna que contiene la l√≥gica.<br><br><strong>Solo nopython:</strong> La implementaci√≥n que retornas debe ser compatible con nopython mode (sin objetos Python, sin I/O, sin excepciones complejas).`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ register_jitable ‚îÄ‚îÄ
        result_register_jitable: {
          type: 'result',
          decorator: '@njit (register_jitable o simplemente @njit)',
          decoratorClass: 'njit',
          title: 'Funciones nuevas para usar dentro de @njit',
          why: `Si est√°s creando una funci√≥n nueva desde cero que ser√° usada dentro de @njit, la soluci√≥n m√°s simple es <strong>decorarla con @njit</strong>. Numba puede llamar funciones @njit desde dentro de otras funciones @njit sin overhead. No necesitas @overload para funciones nuevas.`,
          code: `from numba import njit

# Funci√≥n auxiliar ‚Äî tambi√©n compilada
@njit
def safe_divide(a, b):
    if b == 0:
        return 0.0
    return a / b

@njit
def safe_log(x):
    if x <= 0:
        return -999.0  # sentinel value
    return np.log(x)

# Usarlas dentro de otra @njit: cero overhead
@njit
def pipeline(data):
    result = np.empty(len(data))
    for i in range(len(data)):
        result[i] = safe_log(safe_divide(data[i], data[i] + 1))
    return result`,
          options: [
            { code: '@njit para todo', desc: 'Cuando @njit llama a otra @njit, Numba puede incluso INLINEAR la funci√≥n auxiliar, eliminando totalmente el overhead de llamada.' }
          ],
          pitfall: `<strong>¬øCu√°ndo usar @overload en vez de @njit?:</strong> Solo cuando necesitas que una funci√≥n EXISTENTE (que no puedes o no quieres modificar) funcione dentro de @njit. Si creas algo nuevo, @njit es suficiente.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ jit_module ‚îÄ‚îÄ
        result_jit_module: {
          type: 'result',
          decorator: 'jit_module()',
          decoratorClass: 'jit_module',
          title: 'jit_module() ‚Äî Jittear un m√≥dulo entero autom√°ticamente',
          why: `<strong>jit_module()</strong> aplica @jit autom√°ticamente a TODAS las funciones definidas en un m√≥dulo Python, sin decorar cada una manualmente. Ideal cuando tienes un m√≥dulo con muchas funciones utilitarias num√©ricas.`,
          code: `# mi_modulo.py
from numba import jit_module
import numpy as np

def funcion_a(x):
    return np.sqrt(x) + 1

def funcion_b(x, y):
    return x ** 2 + y ** 2

def funcion_c(data):
    total = 0.0
    for val in data:
        total += val
    return total

# ‚ö†Ô∏è LLAMAR AL FINAL del m√≥dulo
# Todas las funciones ARRIBA ser√°n jitteadas
# Las de ABAJO no
jit_module(nopython=True, error_model="numpy")

# Esta NO ser√° jitteada (definida despu√©s de jit_module)
def funcion_no_jit(x):
    return str(x)`,
          options: [
            { code: 'nopython=True', desc: 'Pasa cualquier kwarg que le dar√≠as a @jit. Todas las funciones recibir√°n esas opciones.' },
            { code: 'Posici√≥n importa', desc: 'Solo jittea funciones definidas ANTES de la llamada a jit_module() y DENTRO del mismo m√≥dulo (no importadas).' }
          ],
          pitfall: `<strong>Experimental:</strong> La API puede cambiar entre versiones de Numba.<br><br><strong>No es selectivo:</strong> Jittea TODAS las funciones. Si alguna no es compatible con nopython mode, fallar√°. Considera decorar manualmente las funciones incompatibles con <code>@jit(forceobj=True)</code> antes de llamar a jit_module().`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ objmode ‚îÄ‚îÄ
        result_objmode: {
          type: 'result',
          decorator: 'with objmode()',
          decoratorClass: 'objmode',
          title: 'objmode() ‚Äî C√≥digo Python dentro de @njit',
          why: `<strong>objmode()</strong> es un context manager que te permite ejecutar c√≥digo Python normal (con el int√©rprete) dentro de una funci√≥n @njit. Es un "escape hatch" para cuando necesitas llamar una funci√≥n incompatible con Numba sin salir de la funci√≥n compilada. <strong>√ösalo con cautela</strong> ‚Äî el c√≥digo dentro de objmode NO es r√°pido (corre en el int√©rprete).`,
          code: `from numba import njit, objmode
import numpy as np

@njit(cache=True)
def pipeline_hibrido(data):
    # Parte r√°pida: c√≥digo compilado
    n = len(data)
    procesado = np.empty(n)
    for i in range(n):
        procesado[i] = np.sqrt(data[i]) * 2.0

    # Necesito llamar algo incompatible con Numba
    with objmode(resultado='float64[:]'):
        # Este bloque corre en el int√©rprete Python
        # Puedo usar pandas, scipy, lo que sea
        from scipy.signal import medfilt
        resultado = medfilt(procesado, kernel_size=5).astype(np.float64)

    # De vuelta en c√≥digo compilado
    total = 0.0
    for i in range(len(resultado)):
        total += resultado[i]

    return total`,
          options: [
            { code: 'Anotar tipos de salida', desc: 'DEBES declarar el tipo de las variables que salen del bloque objmode: objmode(var1="float64[:]", var2="int64")' },
            { code: 'Rendimiento', desc: 'El bloque objmode tiene overhead significativo (adquirir GIL, boxing/unboxing). Minimiza su uso. Idealmente, haz TODO el trabajo pesado en nopython y solo usa objmode para operaciones puntuales incompatibles.' }
          ],
          pitfall: `<strong>‚ö†Ô∏è Advertencia oficial:</strong> "This feature can be easily mis-used." √ösalo SOLO cuando no hay alternativa. Primero intenta: (1) reescribir la l√≥gica en Numba puro, (2) usar @overload, (3) separar en funciones distintas.<br><br><strong>Limitaciones:</strong> No puede usar yield, break, return, o raise que salga del with. No puede contener otros with statements. Los estados de random number generator no se sincronizan entre nopython y objmode.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ No necesitas Numba ‚îÄ‚îÄ
        result_no_numba_needed: {
          type: 'result',
          decorator: 'NumPy puro',
          decoratorClass: 'njit',
          title: 'Probablemente no necesitas Numba aqu√≠',
          why: `Si tus operaciones NumPy individuales ya son r√°pidas y tu cuello de botella no es computaci√≥n num√©rica, Numba no te va a ayudar mucho. NumPy ya ejecuta en C internamente. Numba brilla cuando: (1) tienes loops Python que NumPy no puede vectorizar, (2) encadenas muchas operaciones NumPy creando temporales, o (3) necesitas paralelismo a nivel de loop.`,
          code: `# Si NumPy ya es r√°pido para tu caso, estas son mejores opciones:

# 1. Profiling primero ‚Äî ¬ød√≥nde est√° el cuello de botella?
# python -m cProfile mi_script.py

# 2. Si es I/O ‚Üí Polars, asyncio, multiprocessing
# 3. Si es l√≥gica de negocio ‚Üí refactorizar algoritmo
# 4. Si es memoria ‚Üí chunking con np.memmap o Dask

# Solo si despu√©s del profiling el cuello de botella es
# un loop num√©rico, vuelve a este √°rbol de decisi√≥n.`,
          options: [],
          pitfall: `<strong>Profiling primero, siempre:</strong> La optimizaci√≥n prematura es la ra√≠z de todo mal. Usa <code>cProfile</code> o <code>line_profiler</code> para identificar d√≥nde pasa realmente el tiempo tu c√≥digo.`,
          diagnostic: ``
        },

        // ‚îÄ‚îÄ No Numba para I/O ‚îÄ‚îÄ
        result_no_numba_io: {
          type: 'result',
          decorator: 'No usar Numba',
          decoratorClass: 'njit',
          title: 'Numba no es para I/O',
          why: `Numba compila c√≥digo num√©rico a c√≥digo m√°quina. <strong>No puede acelerar I/O</strong> (lectura de archivos, red, bases de datos). Si tu cuello de botella es I/O, las herramientas correctas son otras.`,
          code: `# Para I/O intensivo, considera:

# Lectura de CSV/Parquet ‚Üí Polars (m√°s r√°pido que pandas)
# import polars as pl
# df = pl.read_parquet("datos.parquet")

# Procesamiento paralelo de archivos ‚Üí concurrent.futures
# from concurrent.futures import ThreadPoolExecutor
# with ThreadPoolExecutor(max_workers=8) as pool:
#     results = pool.map(procesar_archivo, lista_archivos)

# Datos que no caben en RAM ‚Üí Dask
# import dask.dataframe as dd
# df = dd.read_parquet("datos_enormes/")

# Base de datos ‚Üí conexiones pooled + queries optimizados`,
          options: [],
          pitfall: `<strong>Patr√≥n com√∫n:</strong> Muchos pipelines son I/O-bound en la carga y CPU-bound en el procesamiento. Usa Polars/Dask para cargar, luego Numba para procesar los arrays num√©ricos resultantes.`,
          diagnostic: ``
        }
      };

      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
      // ‚ñà‚ñà MOTOR DE RENDERIZADO DEL √ÅRBOL ‚ñà‚ñà
      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

      let dtState = {
        history: [],     // [{nodeId, optionId, optionLabel}]
        currentNode: 'root'
      };

      function dtRender() {
        const node = TREE[dtState.currentNode];
        if (!node) return;

        // Handle redirects
        if (node.type === 'result' && node.redirect) {
          dtState.currentNode = node.redirect;
          dtRender();
          return;
        }

        renderBreadcrumb();
        renderHistory();
        renderCurrent(node);
      }

      function renderBreadcrumb() {
        const el = document.getElementById('dt-breadcrumb');
        if (dtState.history.length === 0) {
          el.innerHTML = '';
          return;
        }
        let html = '';
        dtState.history.forEach((h, idx) => {
          if (idx > 0) html += '<span class="dt-crumb-sep">‚Üí</span>';
          html += `<span class="dt-crumb" onclick="dtGoBack(${idx})" title="Volver aqu√≠">${h.optionLabel}</span>`;
        });
        el.innerHTML = html;
      }

      function renderHistory() {
        const el = document.getElementById('dt-history');
        let html = '';
        dtState.history.forEach((h, idx) => {
          const node = TREE[h.nodeId];
          if (node && node.type === 'question') {
            html += `<div class="dt-answered" onclick="dtGoBack(${idx})">
              <span class="dt-step-badge q">${idx + 1}</span>
              <span class="dt-answered-q">${truncate(node.text, 50)}</span>
              <span style="color:var(--border)">‚Üí</span>
              <span class="dt-answered-a">${h.optionLabel}</span>
            </div>`;
          }
        });
        el.innerHTML = html;
      }

      function renderCurrent(node) {
        const el = document.getElementById('dt-current');
        if (node.type === 'question') {
          el.innerHTML = renderQuestion(node);
        } else if (node.type === 'result') {
          el.innerHTML = renderResult(node);
        }
      }

      function renderQuestion(node) {
        const stepNum = dtState.history.length + 1;
        let optionsHtml = '';
        node.options.forEach(opt => {
          optionsHtml += `
            <button class="dt-option" onclick="dtChoose('${opt.id}', '${escHtml(opt.label)}')">
              <span class="dt-option-icon">${opt.icon}</span>
              <span class="dt-option-content">
                <span class="dt-option-label">${opt.label}</span>
                <span class="dt-option-desc">${opt.desc}</span>
              </span>
            </button>`;
        });

        let contextHtml = '';
        if (node.context) {
          contextHtml = `<div class="dt-context">${node.context}</div>`;
        }

        return `
          <div class="dt-card">
            <div class="dt-card-header">
              <span class="dt-step-badge q">${stepNum}</span>
              <span class="dt-question-text">${node.text}</span>
            </div>
            <div class="dt-card-body">
              ${contextHtml}
              <div class="dt-options">${optionsHtml}</div>
            </div>
          </div>`;
      }

      function renderResult(node) {
        let optionsHtml = '';
        if (node.options && node.options.length > 0) {
          let rows = '';
          node.options.forEach(opt => {
            rows += `<div class="dt-result-option-row">‚ñ∏ <code>${opt.code}</code> ‚Äî ${opt.desc}</div>`;
          });
          optionsHtml = `
            <div class="dt-result-options">
              <div class="dt-result-options-title">‚ö° Opciones de rendimiento</div>
              ${rows}
            </div>`;
        }

        let pitfallHtml = '';
        if (node.pitfall) {
          pitfallHtml = `
            <div class="dt-pitfall">
              <div class="dt-pitfall-title">‚ö†Ô∏è Trampas y advertencias</div>
              ${node.pitfall}
            </div>`;
        }

        let diagnosticHtml = '';
        if (node.diagnostic) {
          diagnosticHtml = `
            <div class="dt-diagnostic">
              <div class="dt-diagnostic-title">üîç C√≥mo verificar que funciona</div>
              ${node.diagnostic}
            </div>`;
        }

        return `
          <div class="dt-result">
            <div class="dt-result-header">
              <span class="dt-step-badge r">‚úì</span>
              <span class="dt-result-title">${node.title}</span>
            </div>
            <div class="dt-result-body">
              <span class="dt-result-decorator ${node.decoratorClass}">${node.decorator}</span>
              <div class="dt-result-why">${node.why}</div>
              <div class="dt-result-code">${node.code}</div>
              ${optionsHtml}
              ${pitfallHtml}
              ${diagnosticHtml}
              <div class="dt-actions">
                <button class="dt-btn primary" onclick="dtReset()">üîÑ Empezar de nuevo</button>
                <button class="dt-btn" onclick="dtCopyCode('${escAttr(node.code || '')}')">üìã Copiar c√≥digo</button>
              </div>
            </div>
          </div>`;
      }

      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
      // ‚ñà‚ñà FUNCIONES DE NAVEGACI√ìN Y CONTROL ‚ñà‚ñà
      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

      function dtChoose(optionId, optionLabel) {
        // Guardar la elecci√≥n en el historial
        dtState.history.push({
          nodeId: dtState.currentNode,
          optionId: optionId,
          optionLabel: optionLabel
        });

        // Avanzar al siguiente nodo
        dtState.currentNode = optionId;

        // Renderizar
        dtRender();

        // Scroll suave al contenido actual
        setTimeout(() => {
          const el = document.getElementById('dt-current');
          if (el) {
            el.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
          }
        }, 50);
      }

      function dtGoBack(stepIndex) {
        // Volver a un punto anterior del historial
        const target = dtState.history[stepIndex];
        dtState.currentNode = target.nodeId;
        dtState.history = dtState.history.slice(0, stepIndex);
        dtRender();
      }

      function dtReset() {
        dtState = { history: [], currentNode: 'root' };
        dtRender();
        // Scroll al inicio del √°rbol
        setTimeout(() => {
          const el = document.getElementById('dt-breadcrumb');
          if (el) {
            el.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }
        }, 50);
      }

      function dtJumpTo(resultId) {
        // Acceso directo a un resultado espec√≠fico
        // Limpia el historial y muestra directamente el resultado
        dtState = {
          history: [{ nodeId: 'root', optionId: resultId, optionLabel: 'Acceso directo' }],
          currentNode: resultId
        };
        dtRender();
        setTimeout(() => {
          const el = document.getElementById('dt-current');
          if (el) {
            el.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
          }
        }, 50);
      }

      function dtCopyCode(code) {
        // Decodificar entidades HTML y copiar al portapapeles
        const textarea = document.createElement('textarea');
        textarea.value = code.replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>').replace(/&#39;/g, "'").replace(/&quot;/g, '"');
        document.body.appendChild(textarea);
        textarea.select();
        try {
          document.execCommand('copy');
          // Feedback visual breve
          const btns = document.querySelectorAll('.dt-btn');
          btns.forEach(btn => {
            if (btn.textContent.includes('Copiar')) {
              const original = btn.innerHTML;
              btn.innerHTML = '‚úÖ ¬°Copiado!';
              btn.style.borderColor = 'var(--accent2)';
              btn.style.color = 'var(--accent2)';
              setTimeout(() => {
                btn.innerHTML = original;
                btn.style.borderColor = '';
                btn.style.color = '';
              }, 1500);
            }
          });
        } catch (e) {
          // Fallback: intentar con API moderna
          if (navigator.clipboard) {
            navigator.clipboard.writeText(textarea.value);
          }
        }
        document.body.removeChild(textarea);
      }

      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
      // ‚ñà‚ñà UTILIDADES ‚ñà‚ñà
      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

      function escHtml(str) {
        if (!str) return '';
        return str.replace(/'/g, "\\'").replace(/"/g, '&quot;');
      }

      function escAttr(str) {
        if (!str) return '';
        return str
          .replace(/&/g, '&amp;')
          .replace(/'/g, '&#39;')
          .replace(/"/g, '&quot;')
          .replace(/</g, '&lt;')
          .replace(/>/g, '&gt;')
          .replace(/\n/g, '\\n')
          .replace(/`/g, '\\`');
      }

      function truncate(str, maxLen) {
        if (!str) return '';
        if (str.length <= maxLen) return str;
        return str.substring(0, maxLen) + '...';
      }

      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
      // ‚ñà‚ñà INICIALIZACI√ìN ‚ñà‚ñà
      // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

      // Renderizar el estado inicial cuando el DOM est√© listo
      if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', dtRender);
      } else {
        dtRender();
      }

      // Exponer funciones al scope global para los onclick
      window.dtChoose = dtChoose;
      window.dtGoBack = dtGoBack;
      window.dtReset = dtReset;
      window.dtJumpTo = dtJumpTo;
      window.dtCopyCode = dtCopyCode;

    })(); // Fin del IIFE
    </script>

    <!-- ============================================ -->
    <!-- === MAPA EST√ÅTICO DE REFERENCIA R√ÅPIDA ===== -->
    <!-- ============================================ -->
    <div class="dt-quickref" style="margin-top:2.5rem; border-top:1px solid var(--border); padding-top:1.5rem;">
      <h3>üìã Mapa est√°tico completo (para referencia sin interactividad)</h3>
      <p style="color:var(--muted); font-size:0.9rem; margin-bottom:1rem;">Si prefieres ver todo de un vistazo sin navegar el √°rbol interactivo:</p>

<pre><code><span class="cm">¬øQu√© tipo de problema tienes?</span>
‚îÇ
‚îú‚îÄ <span class="kw">LOOP NUM√âRICO LENTO</span>
‚îÇ  ‚îú‚îÄ ¬øIteraciones independientes?
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç + &gt;10K iteraciones
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Precisi√≥n IEEE no cr√≠tica ‚Üí <span class="dec">@njit(parallel=True, fastmath=True)</span> + prange
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Precisi√≥n IEEE cr√≠tica    ‚Üí <span class="dec">@njit(parallel=True)</span> + prange
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç + &lt;10K iteraciones       ‚Üí <span class="dec">@njit</span> (paralelo no vale la pena)
‚îÇ  ‚îÇ  ‚îî‚îÄ NO (dependencia secuencial)
‚îÇ  ‚îÇ     ‚îú‚îÄ Llamada &gt;1K veces desde Python ‚Üí <span class="dec">@njit</span> con batch (loop DENTRO)
‚îÇ  ‚îÇ     ‚îî‚îÄ Pocas llamadas
‚îÇ  ‚îÇ        ‚îú‚îÄ Script repetitivo ‚Üí <span class="dec">@njit(cache=True)</span>
‚îÇ  ‚îÇ        ‚îî‚îÄ Desarrollo activo ‚Üí <span class="dec">@njit</span>
‚îÇ  ‚îî‚îÄ Loop externo independiente + interno dependiente
‚îÇ     ‚îî‚îÄ <span class="dec">@njit(parallel=True)</span>: prange externo + range interno
‚îÇ
‚îú‚îÄ <span class="kw">OPERACI√ìN ESCALAR ‚Üí ARRAY</span>
‚îÇ  ‚îú‚îÄ ¬øFunci√≥n puramente escalar‚Üíescalar?
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç + necesita broadcasting/.reduce()/.accumulate()
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Array &gt;1M elementos ‚Üí <span class="dec">@vectorize(target='parallel')</span>
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Array normal         ‚Üí <span class="dec">@vectorize</span>
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç + solo array 1D
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Es para librer√≠a/API ‚Üí <span class="dec">@vectorize</span> (interfaz NumPy est√°ndar)
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Es c√≥digo interno    ‚Üí <span class="dec">@njit</span> con loop manual (m√°s simple)
‚îÇ  ‚îÇ  ‚îî‚îÄ NO (necesita ver varios elementos) ‚Üí Ir a "Sub-array ops"
‚îÇ  ‚îÇ
‚îú‚îÄ <span class="kw">OPERACI√ìN SOBRE SUB-ARRAYS</span> (filas, ventanas, secciones)
‚îÇ  ‚îú‚îÄ ¬øNecesita broadcasting autom√°tico?
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç + millones de llamadas con sub-arrays peque√±os
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ <span class="dec">@njit</span> loop manual + output pre-allocado (evita overhead ufunc)
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç + pocas llamadas o sub-arrays grandes
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ <span class="dec">@guvectorize</span> con firma dimensional: "(n)->()"
‚îÇ  ‚îÇ  ‚îî‚îÄ NO (itero filas con loop)
‚îÇ  ‚îÇ     ‚îú‚îÄ Filas independientes ‚Üí <span class="dec">@njit(parallel=True)</span> + prange
‚îÇ  ‚îÇ     ‚îî‚îÄ Filas dependientes   ‚Üí <span class="dec">@njit</span>
‚îÇ  ‚îÇ
‚îú‚îÄ <span class="kw">CADA RESULTADO DEPENDE DE VECINOS</span> (grilla/imagen)
‚îÇ  ‚îú‚îÄ Patr√≥n de vecindario fijo
‚îÇ  ‚îÇ  ‚îú‚îÄ Quiero paralelismo ‚Üí <span class="dec">@stencil</span> llamado desde <span class="dec">@njit(parallel=True)</span>
‚îÇ  ‚îÇ  ‚îî‚îÄ Serial est√° bien   ‚Üí <span class="dec">@stencil</span>
‚îÇ  ‚îî‚îÄ Patr√≥n variable       ‚Üí <span class="dec">@njit</span> con loop manual
‚îÇ
‚îú‚îÄ <span class="kw">OBJETO CON ESTADO MUTABLE</span>
‚îÇ  ‚îú‚îÄ ¬øSe usa dentro de @njit?
‚îÇ  ‚îÇ  ‚îú‚îÄ S√ç ‚Üí <span class="dec">@jitclass</span>
‚îÇ  ‚îÇ  ‚îî‚îÄ NO + m√©todos pesados ‚Üí <span class="dec">@jitclass</span>
‚îÇ  ‚îÇ  ‚îî‚îÄ NO + m√©todos ligeros ‚Üí Clase Python normal + <span class="dec">@njit</span> para lo pesado
‚îÇ  ‚îî‚îÄ ¬øSolo proceso datos sin estado residual?
‚îÇ     ‚îî‚îÄ <span class="dec">@njit</span> con funciones puras (pasar estado como argumentos)
‚îÇ
‚îú‚îÄ <span class="kw">INTEROPERABILIDAD</span>
‚îÇ  ‚îú‚îÄ Callback para SciPy/C     ‚Üí <span class="dec">@cfunc</span>("float64(float64)")
‚îÇ  ‚îú‚îÄ Funci√≥n externa en @njit   ‚Üí <span class="dec">@overload</span>(mi_funcion)
‚îÇ  ‚îú‚îÄ Jittear m√≥dulo entero      ‚Üí <span class="fn">jit_module</span>(nopython=True) al final del m√≥dulo
‚îÇ  ‚îî‚îÄ C√≥digo Python dentro @njit ‚Üí <span class="kw">with</span> <span class="fn">objmode</span>(var='tipo') <span class="cm"># ‚ö†Ô∏è escape hatch, no es r√°pido</span>
‚îÇ
‚îî‚îÄ <span class="kw">OPTIMIZACIONES TRANSVERSALES</span> (aplican a cualquier decorador)
   ‚îú‚îÄ <span class="dec">cache=True</span>      ‚Üí Evitar recompilaci√≥n al reiniciar script
   ‚îú‚îÄ <span class="dec">fastmath=True</span>   ‚Üí ~2x m√°s r√°pido, error ~4 ULP (no finanzas)
   ‚îú‚îÄ <span class="dec">nogil=True</span>      ‚Üí Liberar GIL para threading.Thread manual
   ‚îú‚îÄ <span class="fn">Intel SVML</span>      ‚Üí conda install intel-cmplr-lib-rt (~2-4x en sin/cos/exp)
   ‚îú‚îÄ <span class="fn">Warm-up</span>         ‚Üí Primera llamada con datos dummy para pre-compilar
   ‚îú‚îÄ <span class="fn">Batch</span>           ‚Üí Mover loops Python DENTRO de @njit (&gt;1K llamadas)
   ‚îú‚îÄ <span class="fn">Pre-allocar</span>     ‚Üí Pasar output como argumento, reutilizar buffers
   ‚îú‚îÄ <span class="fn">float32</span>         ‚Üí Mitad de memoria, doble SIMD, doble Throughput (Rendimiento real) RAM
   ‚îú‚îÄ <span class="fn">C-order</span>         ‚Üí Acceder √∫ltima dimensi√≥n en loop interno (contiguo)
   ‚îî‚îÄ <span class="fn">Verificar</span>       ‚Üí inspect_types(), parallel_diagnostics(4), inspect_llvm()</code></pre>

      <div class="warn">
        <strong>Meta-principio:</strong> Empieza con <code>@njit</code>. Solo cambia de decorador cuando necesites una capacidad espec√≠fica que <code>@njit</code> no ofrece. Optimiza <strong>memoria antes que CPU</strong>, porque la CPU casi nunca es el bottleneck real. Y <strong>mide siempre</strong> ‚Äî sin benchmarks con tus datos reales, est√°s adivinando.
      </div>

      <div class="tip">
        <strong>¬øCu√°ndo Numba NO es suficiente?</strong> Este √°rbol cubre Numba como herramienta. Si despu√©s de optimizar con Numba necesitas m√°s rendimiento: (1) <strong>GPU</strong> ‚Üí CuPy o numba.cuda, (2) <strong>Multi-nodo</strong> ‚Üí Ray/Dask, (3) <strong>I/O bound</strong> ‚Üí Polars/asyncio. Eso ya no es un √°rbol de Numba, es <em>arquitectura de sistemas num√©ricos</em> ‚Äî un tema diferente.
      </div>
    </div>

  </div><!-- fin .module-body -->
</div><!-- fin #m11 -->


</div>

<script>
function toggle(header) {
  header.parentElement.classList.toggle('open');
}
</script>

</body>
</html>
